{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb9f541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Comment\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import display, HTML\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "337a6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_filename(file_path):\n",
    "    #split the file path into directory, name, and extension\n",
    "    directory, filename = os.path.split(file_path)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "\n",
    "    #if the file doesn't exist, return the original file path\n",
    "    if not os.path.exists(file_path):\n",
    "        return file_path\n",
    "\n",
    "    #if the file exists, generate a new file name\n",
    "    i = 1\n",
    "    while True:\n",
    "        #generate a new file name\n",
    "        new_name = f\"{name}_{i}{ext}\"\n",
    "        new_file_path = os.path.join(directory, new_name)\n",
    "\n",
    "        #if the new file name doesnt exist, return it\n",
    "        if not os.path.exists(new_file_path):\n",
    "            return new_file_path\n",
    "\n",
    "        #if the new file name exists, increment the counter and go again\n",
    "        i += 1\n",
    "\n",
    "def extract_text_from_dom(soup, id_name): #, elements=None\n",
    "     #try to find the <main> element\n",
    "    main = soup.find(id=id_name)\n",
    "\n",
    "    #if <main> is not found, fall back to the <body> element\n",
    "    if main is None:\n",
    "        main = soup.find('body')\n",
    "        \n",
    "    #define a function to filter visible tags\n",
    "    def tag_visible(element):\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]','table','figure','video','image']:\n",
    "            return False\n",
    "        if element.name == 'sup':  # Ignore superscript text\n",
    "            return False\n",
    "        return True if isinstance(element, NavigableString) else tag_visible(element.parent)\n",
    "\n",
    "    #extract text from visible tags\n",
    "    texts = main.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    visible_text = u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "    #remove text within square brackets\n",
    "    visible_text = re.sub(r'\\[.*?\\]', '', visible_text)\n",
    "\n",
    "    #split text into sentences using regular expressions\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', visible_text)\n",
    "    return '\\n'.join(sentences)\n",
    "\n",
    "def extract_text_from_website(url, file_path, id_name=None, visited_urls=None, max_depth=1): #, elements=None\n",
    "    # initialize visited_urls as an empty set if its not provided\n",
    "    if visited_urls is None:\n",
    "        visited_urls = set()\n",
    "\n",
    "    # check if the URL has been visited to avoid infinite recursion\n",
    "    if url in visited_urls:\n",
    "        return\n",
    "\n",
    "    #add the current URL to visited URLs\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    try:\n",
    "        #fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        #stop extraction if 'See also' section is found\n",
    "        see_also = soup.find(id='See_also')\n",
    "        if see_also is not None:\n",
    "            soup = BeautifulSoup(str(soup).split(str(see_also))[0], 'html.parser')\n",
    "\n",
    "        #extract text from the current page\n",
    "        extracted_text = extract_text_from_dom(soup, id_name) #, elements\n",
    "\n",
    "        #write the extracted_text to the file, had to use utf-8 for some characters\n",
    "        with open(file_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        #find all links on the page\n",
    "        links = soup.find_all('a', href=True, text=True)\n",
    "\n",
    "        #extract text from linked pages recursively\n",
    "        for link in links:\n",
    "            subpage_url = link['href']\n",
    "            #convert relative links to absolute links\n",
    "            subpage_url = urljoin(url, subpage_url)\n",
    "            if \"en.wikipedia\" in subpage_url and subpage_url not in visited_urls and max_depth > 0: \n",
    "                extract_text_from_website(subpage_url, file_path, id_name, visited_urls, max_depth - 1) #, elements\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL '{url}': {e}\")\n",
    "        \n",
    "def text_to_clear_sentences(file_path):\n",
    "    #open file and read the text\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    #tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    #remove duplicate sentences\n",
    "    sentences = list(dict.fromkeys(sentences))\n",
    "\n",
    "    banned_strings = ['submit a request','follow us on', 'subscribe to', 'about terms', 'privacy help','faq', 'about terms privacy help', '@', 'help desk', 'help support', 'help center', 'homepage', 'navbar']\n",
    "    sentences = [sent for sent in sentences if not re.search(r'\\b(\\d{2} , \\d{4})\\b', sent.lower()) and not any(banned_string in sent.lower() for banned_string in banned_strings)]\n",
    "\n",
    "    #create cleared text file\n",
    "    base_name = os.path.basename(file_path)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    cleared_file_path = os.path.join(os.path.dirname(file_path), f\"{name}_cleared{ext}\")\n",
    "\n",
    "    #write cleared sentences to the file\n",
    "    with open(cleared_file_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + \"\\n\")\n",
    "\n",
    "    return cleared_file_path\n",
    "    \n",
    "def text_analytics(file_path):\n",
    "    #open file and read the text\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    #tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    #filter out non-alphabetic words and stopwords\n",
    "    stop_words = set(stopwords.words('english')) #the, a, and, of...\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(5)\n",
    "    \n",
    "    #compute average word length and round it\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    avg_word_length = round(sum(word_lengths) / len(word_lengths))\n",
    "\n",
    "    #compute sentiment of the text and convert it to percentages\n",
    "    sentiment = TextBlob(text).sentiment\n",
    "    sentiment_dict = dict(sentiment._asdict())  #convert sentiment to dictionary\n",
    "    for key in sentiment_dict:\n",
    "        sentiment_dict[key] = round(sentiment_dict[key] * 100, 1)  #to percentage\n",
    "\n",
    "    #perform Topic Modeling\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_data = count_vectorizer.fit_transform(words)\n",
    "    lda = LDA(n_components=5)\n",
    "    lda.fit(count_data)\n",
    "    topics = lda.components_\n",
    "\n",
    "    #create analytics file\n",
    "    base_name = os.path.basename(file_path)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    analytics_file_path = os.path.join(os.path.dirname(file_path), f\"{name}_analytics{ext}\")\n",
    "\n",
    "    #write analytics to the file\n",
    "    with open(analytics_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"5 most common words in extracted text:\\n\")\n",
    "        for word, frequency in most_common_words:\n",
    "            f.write(f\"{word}: {frequency}\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(f\"Average word length: {avg_word_length}\\n\\n\")\n",
    "        f.write(\"Sentiment:\\n\")\n",
    "        for key, value in sentiment_dict.items():\n",
    "            f.write(f\"{key}={value}%\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"Topics:\\n\")\n",
    "        for i, topic in enumerate(topics):\n",
    "            #for each topic found, print top 5 words (topic.argsort)\n",
    "            f.write(f\"Topic {i+1}: {', '.join([count_vectorizer.get_feature_names()[i] for i in topic.argsort()[-5:]])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b62d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL 'https://en.wikipedia.org/w/index.php?title=Hjorleifshofdi&action=edit&redlink=1': 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Hjorleifshofdi\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Types_of_volcanic_eruptions\"\n",
    "id_name = \"mw-content-text\"  # specify the id name\n",
    "file_path = \"C:\\Users\\lukas\\OneDrive\\Documents\\GitHub\\extractor\\outputWIKI\"\n",
    "file_path = get_unique_filename(file_path)\n",
    "extract_text_from_website(url, file_path, id_name)\n",
    "cleared_file_path = text_to_clear_sentences(file_path)\n",
    "text_analytics(cleared_file_path)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210393b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
