  Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
It works
with your favorite parser to provide idiomatic ways of navigating,
searching, and modifying the parse tree.
It commonly saves programmers
hours or days of work.
These instructions illustrate all major features of Beautiful Soup 4,
with examples.
I show you what the library is good for, how it works,
how to use it, how to make it do what you want, and what to do when it
violates your expectations.
This document covers Beautiful Soup version 4.12.2.
The examples in
this documentation were written for Python 3.8.
You might be looking for the documentation for Beautiful Soup 3 .
If so, you should know that Beautiful Soup 3 is no longer being
developed and that all support for it was dropped on December
31, 2020.
If you want to learn about the differences between Beautiful
Soup 3 and Beautiful Soup 4, see Porting code to BS4 .
This documentation has been translated into other languages by
Beautiful Soup users:   这篇文档当然还有中文版.
このページは日本語で利用できます( 外部リンク )  이 문서는 한국어 번역도 가능합니다.
Este documento também está disponível em Português do Brasil.
Este documento también está disponible en una traducción al español.
Эта документация доступна на русском языке.
Getting help ¶  If you have questions about Beautiful Soup, or run into problems, send mail to the discussion group .
If
your problem involves parsing an HTML document, be sure to mention what the diagnose() function says about
that document.
When reporting an error in this documentation, please mention which
translation you’re reading.
Quick Start ¶  Here’s an HTML document I’ll be using as an example throughout this
document.
It’s part of a story from Alice in Wonderland :  html_doc  =  """<html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """    Running the “three sisters” document through Beautiful Soup gives us a BeautifulSoup object, which represents the document as a nested
data structure:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )  print ( soup .
prettify ())  # <html>  #  <head>  #   <title>  #    The Dormouse's story  #   </title>  #  </head>  #  <body>  #   <p class="title">  #    <b>  #     The Dormouse's story  #    </b>  #   </p>  #   <p class="story">  #    Once upon a time there were three little sisters; and their names were  #    <a class="sister" href="http://example.com/elsie" id="link1">  #     Elsie  #    </a>  #    ,  #    <a class="sister" href="http://example.com/lacie" id="link2">  #     Lacie  #    </a>  #    and  #    <a class="sister" href="http://example.com/tillie" id="link3">  #     Tillie  #    </a>  #    ; and they lived at the bottom of a well.
#   </p>  #   <p class="story">  #    ...
 #   </p>  #  </body>  # </html>    Here are some simple ways to navigate that data structure:  soup .
title  # <title>The Dormouse's story</title>  soup .
title .
name  # u'title'  soup .
string  # u'The Dormouse's story'  soup .
parent .
name  # u'head'  soup .
p  # <p class="title"><b>The Dormouse's story</b></p>  soup .
p [ 'class' ]  # u'title'  soup .
a  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  soup .
find_all ( 'a' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  soup .
find ( id = "link3" )  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>    One common task is extracting all the URLs found within a page’s <a> tags:  for  link  in  soup .
find_all ( 'a' ):  print ( link .
get ( 'href' ))  # http://example.com/elsie  # http://example.com/lacie  # http://example.com/tillie    Another common task is extracting all the text from a page:  print ( soup .
get_text ())  # The Dormouse's story  #  # The Dormouse's story  #  # Once upon a time there were three little sisters; and their names were  # Elsie,  # Lacie and  # Tillie;  # and they lived at the bottom of a well.
#  # ...
Does this look like what you need?
If so, read on.
Installing Beautiful Soup ¶  If you’re using a recent version of Debian or Ubuntu Linux, you can
install Beautiful Soup with the system package manager:  $  apt - get  install  python3 - bs4  Beautiful Soup 4 is published through PyPi, so if you can’t install it
with the system packager, you can install it with easy_install or pip .
The package name is beautifulsoup4 .
Make sure you use the
right version of pip or easy_install for your Python version
(these may be named pip3 and easy_install3 respectively).
$  easy_install  beautifulsoup4  $  pip  install  beautifulsoup4  (The BeautifulSoup package is not what you want.
That’s
the previous major release, Beautiful Soup 3 .
Lots of software uses
BS3, so it’s still available, but if you’re writing new code you
should install beautifulsoup4 .)
If you don’t have easy_install or pip installed, you can download the Beautiful Soup 4 source tarball and
install it with setup.py .
$  python  setup.py  install  If all else fails, the license for Beautiful Soup allows you to
package the entire library with your application.
You can download the
tarball, copy its bs4 directory into your application’s codebase,
and use Beautiful Soup without installing it at all.
I use Python 3.10 to develop Beautiful Soup, but it should work with
other recent versions.
Installing a parser ¶  Beautiful Soup supports the HTML parser included in Python’s standard
library, but it also supports a number of third-party Python parsers.
One is the lxml parser .
Depending on your setup,
you might install lxml with one of these commands:  $  apt - get  install  python - lxml  $  easy_install  lxml  $  pip  install  lxml  Another alternative is the pure-Python html5lib parser , which parses HTML the way a
web browser does.
Depending on your setup, you might install html5lib
with one of these commands:  $  apt - get  install  python3 - html5lib  $  pip  install  html5lib  This table summarizes the advantages and disadvantages of each parser library:   Parser  Typical usage  Advantages  Disadvantages   Python’s html.parser  BeautifulSoup(markup,  "html.parser")   Batteries included  Decent speed     Not as fast as lxml,
less lenient than
html5lib.
lxml’s HTML parser  BeautifulSoup(markup,  "lxml")   Very fast     External C dependency     lxml’s XML parser  BeautifulSoup(markup,  "lxml-xml")  BeautifulSoup(markup,  "xml")   Very fast  The only currently supported
XML parser     External C dependency     html5lib  BeautifulSoup(markup,  "html5lib")   Extremely lenient  Parses pages the same way a
web browser does  Creates valid HTML5     Very slow  External Python
dependency      If you can, I recommend you install and use lxml for speed.
Note that if a document is invalid, different parsers will generate
different Beautiful Soup trees for it.
See Differences
between parsers for details.
Making the soup ¶  To parse a document, pass it into the BeautifulSoup constructor.
You can pass in a string or an open filehandle:  from  bs4  import  BeautifulSoup  with  open ( "index.html" )  as  fp :  soup  =  BeautifulSoup ( fp ,  'html.parser' )  soup  =  BeautifulSoup ( "<html>a web page</html>" ,  'html.parser' )    First, the document is converted to Unicode, and HTML entities are
converted to Unicode characters:  print ( BeautifulSoup ( "<html><head></head><body>Sacr&eacute; bleu!</body></html>" ,  "html.parser" ))  # <html><head></head><body>Sacré bleu!</body></html>    Beautiful Soup then parses the document using the best available
parser.
It will use an HTML parser unless you specifically tell it to
use an XML parser.
(See Parsing XML .)
Kinds of objects ¶  Beautiful Soup transforms a complex HTML document into a complex tree
of Python objects.
But you’ll only ever have to deal with about four kinds of objects: Tag , NavigableString , BeautifulSoup ,
and Comment .
These objects represent the HTML elements that comprise the page.
class  bs4.
Tag ¶  A Tag object corresponds to an XML or HTML tag in the original document.
soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
b  type ( tag )  # <class 'bs4.element.Tag'>    Tags have a lot of attributes and methods, and I’ll cover most of them
in Navigating the tree and Searching the tree .
For now, the most
important methods of a tag are for accessing its name and attributes.
name ¶  Every tag has a name:  tag .
name  # 'b'    If you change a tag’s name, the change will be reflected in any
markup generated by Beautiful Soup down the line:  tag .
name  =  "blockquote"  tag  # <blockquote class="boldest">Extremely bold</blockquote>       attrs ¶  An HTML or XML tag may have any number of attributes.
The tag <b  id="boldest"> has an attribute “id” whose value is
“boldest”.
You can access a tag’s attributes by treating the tag like
a dictionary:  tag  =  BeautifulSoup ( '<b id="boldest">bold</b>' ,  'html.parser' ) .
b  tag [ 'id' ]  # 'boldest'    You can access the dictionary of attributes directly as .attrs :  tag .
attrs  # {'id': 'boldest'}  tag .
attrs .
keys ()  # dict_keys(['id'])    You can add, remove, and modify a tag’s attributes.
Again, this is
done by treating the tag as a dictionary:  tag [ 'id' ]  =  'verybold'  tag [ 'another-attribute' ]  =  1  tag  # <b another-attribute="1" id="verybold"></b>  del  tag [ 'id' ]  del  tag [ 'another-attribute' ]  tag  # <b>bold</b>  tag [ 'id' ]  # KeyError: 'id'  tag .
get ( 'id' )  # None     Multi-valued attributes ¶  HTML 4 defines a few attributes that can have multiple values.
HTML 5
removes a couple of them, but defines a few more.
The most common
multi-valued attribute is class (that is, a tag can have more than
one CSS class).
Others include rel , rev , accept-charset , headers , and accesskey .
By default, Beautiful Soup stores the value(s)
of a multi-valued attribute as a list:  css_soup  =  BeautifulSoup ( '<p class="body"></p>' ,  'html.parser' )  css_soup .
p [ 'class' ]  # ['body']  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'html.parser' )  css_soup .
a [ 'rel' ]  # ['index', 'first']  rel_soup .
a [ 'rel' ]  =  [ 'index' ,  'contents' ]  print ( rel_soup .
p [ 'id' ]  # 'my id'    You can force all attributes to be stored as strings by passing multi_valued_attributes=None as a keyword argument into the BeautifulSoup constructor:  no_list_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'html.parser' ,  multi_valued_attributes = None )  no_list_soup .
p [ 'class' ]  # 'body strikeout'    You can use get_attribute_list to always return the value in a list
container, whether it’s a string or multi-valued attribute value:  id_soup .
p [ 'id' ]  # 'my id'  id_soup .
p .
get_attribute_list ( 'id' )  # ["my id"]    If you parse a document as XML, there are no multi-valued attributes:  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' )  xml_soup .
p [ 'class' ]  # 'body strikeout'    Again, you can configure this using the multi_valued_attributes argument:  class_is_multi =  {  '*'  :  'class' }  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' ,  multi_valued_attributes = class_is_multi )  xml_soup .
p [ 'class' ]  # ['body', 'strikeout']    You probably won’t need to do this, but if you do, use the defaults as
a guide.
They implement the rules described in the HTML specification:  from  bs4.builder  import  builder_registry  builder_registry .
lookup ( 'html' ) .
DEFAULT_CDATA_LIST_ATTRIBUTES         class  bs4.
NavigableString ¶    A tag can contain strings as pieces of text.
Beautiful Soup
uses the NavigableString class to contain these pieces of text:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
b  tag .
string  # 'Extremely bold'  type ( tag .
string )  # <class 'bs4.element.NavigableString'>    A NavigableString is just like a Python Unicode string, except
that it also supports some of the features described in Navigating
the tree and Searching the tree .
You can convert a NavigableString to a Unicode string with str :  unicode_string  =  str ( tag .
string )  unicode_string  # 'Extremely bold'  type ( unicode_string )  # <type 'str'>    You can’t edit a string in place, but you can replace one string with
another, using replace_with() :  tag .
string .
replace_with ( "No longer bold" )  tag  # <b class="boldest">No longer bold</b>    NavigableString supports most of the features described in Navigating the tree and Searching the tree , but not all of
them.
In particular, since a string can’t contain anything (the way a
tag may contain a string or another tag), strings don’t support the .contents or .string attributes, or the find() method.
If you want to use a NavigableString outside of Beautiful Soup,
you should call unicode() on it to turn it into a normal Python
Unicode string.
If you don’t, your string will carry around a
reference to the entire Beautiful Soup parse tree, even when you’re
done using Beautiful Soup.
This is a big waste of memory.
BeautifulSoup ¶    The BeautifulSoup object represents the parsed document as a
whole.
For most purposes, you can treat it as a Tag object.
This means it supports most of the methods described in Navigating the tree and Searching the tree .
You can also pass a BeautifulSoup object into one of the methods
defined in Modifying the tree , just as you would a Tag .
This
lets you do things like combine two parsed documents:  doc  =  BeautifulSoup ( "<document><content/>INSERT FOOTER HERE</document" ,  "xml" )  footer  =  BeautifulSoup ( "<footer>Here's the footer</footer>" ,  "xml" )  doc .
find ( text = "INSERT FOOTER HERE" ) .
replace_with ( footer )  # 'INSERT FOOTER HERE'  print ( doc )  # <?xml version="1.0" encoding="utf-8"?>  # <document><content/><footer>Here's the footer</footer></document>    Since the BeautifulSoup object doesn’t correspond to an actual
HTML or XML tag, it has no name and no attributes.
But sometimes it’s
useful to reference its .name (such as when writing code that works
with both Tag and BeautifulSoup objects),
so it’s been given the special .name “[document]”:  soup .
name  # '[document]'     Special strings ¶  Tag , NavigableString , and BeautifulSoup cover almost everything you’ll see in an
HTML or XML file, but there are a few leftover bits.
The main one
you’ll probably encounter is the Comment .
Comment ¶   markup  =  "<b><!--Hey, buddy.
Want to buy a used parser?--></b>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  comment  =  soup .
b .
string  type ( comment )  # <class 'bs4.element.Comment'>    The Comment object is just a special type of NavigableString :  comment  # 'Hey, buddy.
Want to buy a used parser'    But when it appears as part of an HTML document, a Comment is
displayed with special formatting:  print ( soup .
prettify ())  # <b>  #  <!--Hey, buddy.
Want to buy a used parser?-->  # </b>     For HTML documents ¶  Beautiful Soup defines a few NavigableString subclasses to
contain strings found inside specific HTML tags.
This makes it easier
to pick out the main body of the page, by ignoring strings that
probably represent programming directives found within the
page.
(These classes are new in Beautiful Soup 4.9.0, and the
html5lib parser doesn’t use them.)
Stylesheet ¶   A NavigableString subclass that represents embedded CSS
stylesheets; that is, any strings found inside a <style> tag
during document parsing.
Script ¶   A NavigableString subclass that represents embedded
Javascript; that is, any strings found inside a <script> tag
during document parsing.
Template ¶   A NavigableString subclass that represents embedded HTML
templates; that is, any strings found inside a <template> tag during
document parsing.
For XML documents ¶  Beautiful Soup defines some NavigableString classes for
holding special types of strings that can be found in XML
documents.
Like Comment , these classes are subclasses of NavigableString that add something extra to the string on
output.
Declaration ¶   A NavigableString subclass representing the declaration at the beginning of
an XML document.
Doctype ¶   A NavigableString subclass representing the document type
declaration which may
be found near the beginning of an XML document.
CData ¶   A NavigableString subclass that represents a CData section .
ProcessingInstruction ¶   A NavigableString subclass that represents the contents
of an XML processing instruction .
Navigating the tree ¶  Here’s the “Three sisters” HTML document again:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    I’ll use this as an example to show you how to move from one part of
a document to another.
Going down ¶  Tags may contain strings and more tags.
These elements are the tag’s children .
Beautiful Soup provides a lot of different attributes for
navigating and iterating over a tag’s children.
Note that Beautiful Soup strings don’t support any of these
attributes, because a string can’t have children.
Navigating using tag names ¶  The simplest way to navigate the parse tree is to find a tag by name.
To
do this, you can use the find() method:  soup .
find ( "head" )  # <head><title>The Dormouse's story</title></head>    For convenience, just saying the name of the tag you want is equivalent
to find() (if no built-in attribute has that name).
If you want the
<head> tag, just say soup.head :  soup .
head  # <head><title>The Dormouse's story</title></head>  soup .
title  # <title>The Dormouse's story</title>    You can use this trick again and again to zoom in on a certain part
of the parse tree.
This code gets the first <b> tag beneath the <body> tag:  soup .
body .
b  # <b>The Dormouse's story</b>    find() (and its convenience equivalent) gives you only the first tag
by that name:  soup .
a  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    If you need to get all the <a> tags, you can use find_all() :  soup .
find_all ( 'a' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    For more complicated tasks, such as pattern-matching and filtering, you can
use the methods described in Searching the tree .
.contents and .children ¶  A tag’s children are available in a list called .contents :  head_tag  =  soup .
head  head_tag  # <head><title>The Dormouse's story</title></head>  head_tag .
contents  # [<title>The Dormouse's story</title>]  title_tag  =  head_tag .
contents [ 0 ]  title_tag  # <title>The Dormouse's story</title>  title_tag .
contents  # ['The Dormouse's story']    The BeautifulSoup object itself has children.
In this case, the
<html> tag is the child of the BeautifulSoup object.
:  len ( soup .
contents )  # 1  soup .
contents [ 0 ] .
name  # 'html'    A string does not have .contents , because it can’t contain
anything:  text  =  title_tag .
contents [ 0 ]  text .
contents  # AttributeError: 'NavigableString' object has no attribute 'contents'    Instead of getting them as a list, you can iterate over a tag’s
children using the .children generator:  for  child  in  title_tag .
children :  print ( child )  # The Dormouse's story    If you want to modify a tag’s children, use the methods described in Modifying the tree .
Don’t modify the the .contents list
directly: that can lead to problems that are subtle and difficult to
spot.
.descendants ¶  The .contents and .children attributes consider only a tag’s direct children.
For instance, the <head> tag has a single direct
child–the <title> tag:  head_tag .
contents  # [<title>The Dormouse's story</title>]    But the <title> tag itself has a child: the string “The Dormouse’s
story”.
There’s a sense in which that string is also a child of the
<head> tag.
The .descendants attribute lets you iterate over all of a tag’s children, recursively: its direct children, the children of
its direct children, and so on:  for  child  in  head_tag .
descendants :  print ( child )  # <title>The Dormouse's story</title>  # The Dormouse's story    The <head> tag has only one child, but it has two descendants: the
<title> tag and the <title> tag’s child.
The BeautifulSoup object
only has one direct child (the <html> tag), but it has a whole lot of
descendants:  len ( list ( soup .
children ))  # 1  len ( list ( soup .
descendants ))  # 26      .string ¶  If a tag has only one child, and that child is a NavigableString ,
the child is made available as .string :  title_tag .
string  # 'The Dormouse's story'    If a tag’s only child is another tag, and that tag has a .string , then the parent tag is considered to have the same .string as its child:  head_tag .
contents  # [<title>The Dormouse's story</title>]  head_tag .
string  # 'The Dormouse's story'    If a tag contains more than one thing, then it’s not clear what .string should refer to, so .string is defined to be None :  print ( soup .
html .
string )  # None      .strings and stripped_strings ¶  If there’s more than one thing inside a tag, you can still look at
just the strings.
Use the .strings generator to see all descendant
strings:  for  string  in  soup .
strings :  print ( repr ( string ))  ' \n '  # "The Dormouse's story"  # '\n'  # '\n'  # "The Dormouse's story"  # '\n'  # 'Once upon a time there were three little sisters; and their names were\n'  # 'Elsie'  # ',\n'  # 'Lacie'  # ' and\n'  # 'Tillie'  # ';\nand they lived at the bottom of a well.'
# '\n'  # '...'  # '\n'    Newlines and spaces that separate tags are also strings.
You can remove extra
whitespace by using the .stripped_strings generator instead:  for  string  in  soup .
stripped_strings :  print ( repr ( string ))  # "The Dormouse's story"  # "The Dormouse's story"  # 'Once upon a time there were three little sisters; and their names were'  # 'Elsie'  # ','  # 'Lacie'  # 'and'  # 'Tillie'  # ';\n and they lived at the bottom of a well.'
# '...'    Here, strings consisting entirely of whitespace are ignored, and
whitespace at the beginning and end of strings is removed.
Going up ¶  Continuing the “family tree” analogy, every tag and every string has a parent : the tag that contains it.
.parent ¶  You can access an element’s parent with the .parent attribute.
In
the example “three sisters” document, the <head> tag is the parent
of the <title> tag:  title_tag  =  soup .
title  title_tag  # <title>The Dormouse's story</title>  title_tag .
parent  # <head><title>The Dormouse's story</title></head>    The title string itself has a parent: the <title> tag that contains
it:  title_tag .
parent  # <title>The Dormouse's story</title>    The parent of a top-level tag like <html> is the BeautifulSoup object
itself:  html_tag  =  soup .
html  type ( html_tag .
parent )  # <class 'bs4.BeautifulSoup'>    And the .parent of a BeautifulSoup object is defined as None:  print ( soup .
parent )  # None      .parents ¶  You can iterate over all of an element’s parents with .parents .
This example uses .parents to travel from an <a> tag
buried deep within the document, to the very top of the document:  link  =  soup .
a  link  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  for  parent  in  link .
parents :  print ( parent .
name )  # p  # body  # html  # [document]       Going sideways ¶  Consider a simple document like this:  sibling_soup  =  BeautifulSoup ( "<a><b>text1</b><c>text2</c></a>" ,  'html.parser' )  print ( sibling_soup .
prettify ())  #   <a>  #    <b>  #     text1  #    </b>  #    <c>  #     text2  #    </c>  #   </a>    The <b> tag and the <c> tag are at the same level: they’re both direct
children of the same tag.
We call them siblings .
When a document is
pretty-printed, siblings show up at the same indentation level.
You
can also use this relationship in the code you write.
.next_sibling and .previous_sibling ¶  You can use .next_sibling and .previous_sibling to navigate
between page elements that are on the same level of the parse tree:  sibling_soup .
next_sibling  # <c>text2</c>  sibling_soup .
c .
previous_sibling  # <b>text1</b>    The <b> tag has a .next_sibling , but no .previous_sibling ,
because there’s nothing before the <b> tag on the same level of the
tree .
For the same reason, the <c> tag has a .previous_sibling but no .next_sibling :  print ( sibling_soup .
previous_sibling )  # None  print ( sibling_soup .
next_sibling )  # None    The strings “text1” and “text2” are not siblings, because they don’t
have the same parent:  sibling_soup .
string  # 'text1'  print ( sibling_soup .
next_sibling )  # None    In real documents, the .next_sibling or .previous_sibling of a
tag will usually be a string containing whitespace.
Going back to the
“three sisters” document:  # <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  # <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  # <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;    You might think that the .next_sibling of the first <a> tag would
be the second <a> tag.
But actually, it’s a string: the comma and
newline that separate the first <a> tag from the second:  link  =  soup .
a  link  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  link .
next_sibling  # ',\n '    The second <a> tag is then the .next_sibling of the comma string:  link .
next_sibling .
next_sibling  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>      .next_siblings and .previous_siblings ¶  You can iterate over a tag’s siblings with .next_siblings or .previous_siblings :  for  sibling  in  soup .
a .
next_siblings :  print ( repr ( sibling ))  # ',\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # ' and\n'  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>  # '; and they lived at the bottom of a well.'
for  sibling  in  soup .
find ( id = "link3" ) .
previous_siblings :  print ( repr ( sibling ))  # ' and\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # ',\n'  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  # 'Once upon a time there were three little sisters; and their names were\n'    (If the argument syntax to find tags by their attribute value is unfamiliar,
don’t worry; this is covered later in The keyword arguments .)
Going back and forth ¶  Take a look at the beginning of the “three sisters” document:  # <html><head><title>The Dormouse's story</title></head>  # <p class="title"><b>The Dormouse's story</b></p>    An HTML parser takes this string of characters and turns it into a
series of events: “open an <html> tag”, “open a <head> tag”, “open a
<title> tag”, “add a string”, “close the <title> tag”, “open a <p>
tag”, and so on.
The order in which the opening tags and strings are
encountered is called document order .
Beautiful Soup offers tools for
searching a document’s elements in document order.
.next_element and .previous_element ¶  The .next_element attribute of a string or tag points to whatever
was parsed immediately after the opening of the current tag or after
the current string.
It might be the same as .next_sibling , but it’s
usually drastically different.
Here’s the final <a> tag in the “three sisters” document.
Its .next_sibling is a string: the conclusion of the sentence that was
interrupted by the start of the <a> tag:  last_a_tag  =  soup .
find ( "a" ,  id = "link3" )  last_a_tag  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>  last_a_tag .
next_sibling  # ';\nand they lived at the bottom of a well.'
But the .next_element of that <a> tag, the thing that was parsed
immediately after the <a> tag, is not the rest of that sentence:
it’s the string “Tillie” inside it:  last_a_tag .
next_element  # 'Tillie'    That’s because in the original markup, the word “Tillie” appeared
before that semicolon.
The parser encountered an <a> tag, then the
word “Tillie”, then the closing </a> tag, then the semicolon and rest of
the sentence.
The semicolon is on the same level as the <a> tag, but the
word “Tillie” was encountered first.
The .previous_element attribute is the exact opposite of .next_element .
It points to the opening tag or string that was
parsed immediately before this one:  last_a_tag .
previous_element  # ' and\n'  last_a_tag .
previous_element .
next_element  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>      .next_elements and .previous_elements ¶  You should get the idea by now.
You can use these iterators to move
forward or backward in the document as it was parsed:  for  element  in  last_a_tag .
next_elements :  print ( repr ( element ))  # 'Tillie'  # ';\nand they lived at the bottom of a well.'
# '\n'  # <p class="story">...</p>  # '...'  # '\n'        Searching the tree ¶  Beautiful Soup defines a lot of methods for searching the parse tree,
but they’re all very similar.
I’m going to spend a lot of time explaining
the two most popular methods: find() and find_all() .
The other
methods take almost exactly the same arguments, so I’ll just cover
them briefly.
Once again, I’ll be using the “three sisters” document as an example:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    By passing in a filter to a method like find_all() , you can
zoom in on the parts of the document you’re interested in.
Kinds of filters ¶  Before talking in detail about find_all() and similar methods, I
want to show examples of different filters you can pass into these
methods.
These filters show up again and again, throughout the
search API.
You can use them to filter based on a tag’s name,
on its attributes, on the text of a string, or on some combination of
these.
A string ¶  The simplest filter is a string.
Pass a string to a search method and
Beautiful Soup will perform a tag-name match against that exact string.
This code finds all the <b> tags in the document:  soup .
find_all ( 'b' )  # [<b>The Dormouse's story</b>]    If you pass in a byte string, Beautiful Soup will assume the string is
encoded as UTF-8.
You can avoid this by passing in a Unicode string instead.
A regular expression ¶  If you pass in a regular expression object, Beautiful Soup will filter
against that regular expression using its search() method.
This code
finds all the tags whose names start with the letter “b”; in this
case, the <body> tag and the <b> tag:  import  re  for  tag  in  soup .
find_all ( re .
compile ( "^b" )):  print ( tag .
name )  # body  # b    This code finds all the tags whose names contain the letter ‘t’:  for  tag  in  soup .
compile ( "t" )):  print ( tag .
name )  # html  # title      True ¶  The value True matches every tag it can.
This code finds all the tags in the document, but none of the text strings:  for  tag  in  soup .
find_all ( True ):  print ( tag .
name )  # html  # head  # title  # body  # p  # b  # p  # a  # a  # a  # p      A function ¶  If none of the other matches work for you, define a function that
takes an element as its only argument.
The function should return True if the argument matches, and False otherwise.
Here’s a function that returns True if a tag defines the “class”
attribute but doesn’t define the “id” attribute:  def  has_class_but_no_id ( tag ):  return  tag .
has_attr ( 'class' )  and  not  tag .
has_attr ( 'id' )    Pass this function into find_all() and you’ll pick up all the <p>
tags:  soup .
find_all ( has_class_but_no_id )  # [<p class="title"><b>The Dormouse's story</b></p>,  #  <p class="story">Once upon a time there were…bottom of a well.</p>,  #  <p class="story">...</p>]    This function picks up only the <p> tags.
It doesn’t pick up the <a>
tags, because those tags define both “class” and “id”.
It doesn’t pick
up tags like <html> and <title>, because those tags don’t define
“class”.
The function can be as complicated as you need it to be.
Here’s a
function that returns True if a tag is surrounded by string
objects:  from  bs4  import  NavigableString  def  surrounded_by_strings ( tag ):  return  ( isinstance ( tag .
next_element ,  NavigableString )  and  isinstance ( tag .
previous_element ,  NavigableString ))  for  tag  in  soup .
find_all ( surrounded_by_strings ):  print ( tag .
name )  # body  # p  # a  # a  # a  # p      A list ¶  If you pass in a list, Beautiful Soup will look for a match against any string, regular expression, or function in that list.
This
code finds all the <a> tags and all the <b> tags:  soup .
find_all ([ "a" ,  "b" ])  # [<b>The Dormouse's story</b>,  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Now we’re ready to look at the search methods in detail.
find_all() ¶  Method signature: find_all( name , attrs , recursive , string , limit , **kwargs )  The find_all() method looks through a tag’s descendants and
retrieves all descendants that match your filters.
I gave several
examples in Kinds of filters , but here are a few more:  soup .
find_all ( "title" )  # [<title>The Dormouse's story</title>]  soup .
find_all ( "p" ,  "title" )  # [<p class="title"><b>The Dormouse's story</b></p>]  soup .
find_all ( "a" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  soup .
find_all ( id = "link2" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]  import  re  soup .
find ( string = re .
compile ( "sisters" ))  # 'Once upon a time there were three little sisters; and their names were\n'    Some of these should look familiar, but others are new.
What does it
mean to pass in a value for string , or id ?
Why does find_all("p",  "title") find a <p> tag with the CSS class “title”?
Let’s look at the arguments to find_all() .
The name argument ¶  Pass in a value for name and you’ll tell Beautiful Soup to only
consider tags with certain names.
Text strings will be ignored, as
will tags whose names that don’t match.
This is the simplest usage:  soup .
find_all ( "title" )  # [<title>The Dormouse's story</title>]    Recall from Kinds of filters that the value to name can be a
string , a regular expression , a list , a function , or the value
True .
The keyword arguments ¶  Any keyword argument that’s not recognized will be turned into a filter
that matches tags by their attributes.
If you pass in a value for an argument called id , Beautiful Soup will
filter against each tag’s ‘id’ attribute value:  soup .
find_all ( id = 'link2' )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Just as with tags, you can filter an attribute based on a string , a regular expression , a list , a function , or the value True .
If you pass in a regular expression object for href , Beautiful Soup will
pattern-match against each tag’s ‘href’ attribute value:  soup .
find_all ( href = re .
compile ( "elsie" ))  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    The value True matches every tag that defines the attribute.
This code
finds all tags with an id attribute:   soup.find_all(id=True)
# [<a class=”sister” href=” http://example.com/elsie ” id=”link1”>Elsie</a>,
#  <a class=”sister” href=” http://example.com/lacie ” id=”link2”>Lacie</a>,
#  <a class=”sister” href=” http://example.com/tillie ” id=”link3”>Tillie</a>]   For more complex matches, you can define a function that takes an attribute
value as its only argument.
The function should return True if the value
matches, and False otherwise.
Here’s a function that finds all a tags whose href attribute does not match a regular expression:  import  re  def  not_lacie ( href ):  return  href  and  not  re .
compile ( "lacie" ) .
search ( href )  soup .
find_all ( href = not_lacie )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    If you pass in a list for an argument, Beautiful Soup will look for an
attribute-value match against any string, regular expression, or function in
that list.
This code finds the first and last link:   soup.find_all(id=[“link1”, re.compile(“3$”)])
# [<a class=”sister” href=” http://example.com/elsie ” id=”link1”>Elsie</a>,
#  <a class=”sister” href=” http://example.com/tillie ” id=”link3”>Tillie</a>]   You can filter against multiple attributes at once by passing multiple
keyword arguments:  soup .
compile ( "elsie" ),  id = 'link1' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    Some attributes, like the data-* attributes in HTML 5, have names that
can’t be used as the names of keyword arguments:  data_soup  =  BeautifulSoup ( '<div data-foo="value">foo!</div>' ,  'html.parser' )  data_soup .
find_all ( data - foo = "value" )  # SyntaxError: keyword can't be an expression    You can use these attributes in searches by putting them into a
dictionary and passing the dictionary into find_all() as the attrs argument:  data_soup .
find_all ( attrs = { "data-foo" :  "value" })  # [<div data-foo="value">foo!</div>]    Similarly, you can’t use a keyword argument to search for HTML’s ‘name’ attribute,
because Beautiful Soup uses the name argument to contain the name
of the tag itself.
Instead, you can give a value to ‘name’ in the attrs argument:  name_soup  =  BeautifulSoup ( '<input name="email"/>' ,  'html.parser' )  name_soup .
find_all ( name = "email" )  # []  name_soup .
find_all ( attrs = { "name" :  "email" })  # [<input name="email"/>]      Searching by CSS class ¶  It’s very useful to search for a tag that has a certain CSS class, but
the name of the CSS attribute, “class”, is a reserved word in
Python.
Using class as a keyword argument will give you a syntax
error.
As of Beautiful Soup 4.1.2, you can search by CSS class using
the keyword argument class_ :  soup .
find_all ( "a" ,  class_ = "sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    As with any keyword argument, you can pass class_ a string, a regular
expression, a function, or True :  soup .
find_all ( class_ = re .
compile ( "itl" ))  # [<p class="title"><b>The Dormouse's story</b></p>]  def  has_six_characters ( css_class ):  return  css_class  is  not  None  and  len ( css_class )  ==  6  soup .
find_all ( class_ = has_six_characters )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Remember that a single tag can have multiple
values for its “class” attribute.
When you search for a tag that
matches a certain CSS class, you’re matching against any of its CSS
classes:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'html.parser' )  css_soup .
find_all ( "p" ,  class_ = "strikeout" )  # [<p class="body strikeout"></p>]  css_soup .
find_all ( "p" ,  class_ = "body" )  # [<p class="body strikeout"></p>]    You can also search for the exact string value of the class attribute:  css_soup .
find_all ( "p" ,  class_ = "body strikeout" )  # [<p class="body strikeout"></p>]    But searching for variants of the string value won’t work:  css_soup .
find_all ( "p" ,  class_ = "strikeout body" )  # []    In older versions of Beautiful Soup, which don’t have the class_ shortcut, you can use the attrs argument trick mentioned above.
Create a dictionary whose value for “class” is the string (or regular
expression, or whatever) you want to search for:  soup .
find_all ( "a" ,  attrs = { "class" :  "sister" })  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    To search for tags that match two or more CSS classes at once, use the select() CSS selector method described here :  css_soup .
select ( "p.strikeout.body" )  # [<p class="body strikeout"></p>]      The string argument ¶  With the string argument, you can search for strings instead of tags.
As
with name and attribute keyword arguments, you can pass in a string , a
regular expression , a function , a list , or the value True .
Here are some examples:  soup .
find_all ( string = "Elsie" )  # ['Elsie']  soup .
find_all ( string = [ "Tillie" ,  "Elsie" ,  "Lacie" ])  # ['Elsie', 'Lacie', 'Tillie']  soup .
find_all ( string = re .
compile ( "Dormouse" ))  # ["The Dormouse's story", "The Dormouse's story"]  def  is_the_only_string_within_a_tag ( s ):   """Return True if this string is the only child of its parent tag."""
return  ( s  ==  s .
string )  soup .
find_all ( string = is_the_only_string_within_a_tag )  # ["The Dormouse's story", "The Dormouse's story", 'Elsie', 'Lacie', 'Tillie', '...']    If you use the string argument in a tag search, Beautiful Soup will find
all tags whose .string matches your value for string .
This code finds
the <a> tags whose .string is “Elsie”:  soup .
find_all ( "a" ,  string = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]    The string argument is new in Beautiful Soup 4.4.0.
In earlier
versions it was called text :  soup .
find_all ( "a" ,  text = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]      The limit argument ¶  find_all() returns all the tags and strings that match your
filters.
This can take a while if the document is large.
If you don’t
need all the results, you can pass in a number for limit .
This
works just like the LIMIT keyword in SQL.
It tells Beautiful Soup to
stop gathering results after it’s found a certain number.
There are three links in the “three sisters” document, but this code
only finds the first two:  soup .
find_all ( "a" ,  limit = 2 )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]      The recursive argument ¶  By default, mytag.find_all() will examine all the descendants of mytag :
its children, its children’s children, and so on.
To consider only direct
children, you can pass in recursive=False .
See the difference here:  soup .
find_all ( "title" ,  recursive = False )  # []    Here’s that part of the document:  < html >  < head >  < title >  The  Dormouse 's story  </ title >  </ head >  ...
The <title> tag is beneath the <html> tag, but it’s not directly beneath the <html> tag: the <head> tag is in the way.
Beautiful Soup
finds the <title> tag when it’s allowed to look at all descendants of
the <html> tag, but when recursive=False restricts it to the
<html> tag’s immediate children, it finds nothing.
Beautiful Soup offers a lot of tree-searching methods (covered below),
and they mostly take the same arguments as find_all() : name , attrs , string , limit , and attribute keyword arguments.
But the recursive argument is specific to the find_all() and find() methods.
Passing recursive=False into a method like find_parents() wouldn’t be
very useful.
Calling a tag is like calling find_all() ¶  For convenience, calling a BeautifulSoup object or Tag object as a function is equivalent to calling find_all() (if no built-in method has the name of the tag you’re
looking for).
These two lines of code are equivalent:  soup .
find_all ( "a" )  soup ( "a" )    These two lines are also equivalent:  soup .
find_all ( string = True )  soup .
title ( string = True )      find() ¶  Method signature: find( name , attrs , recursive , string , **kwargs )  The find_all() method scans the entire document looking for
results, but sometimes you only want to find one result.
If you know a
document has only one <body> tag, it’s a waste of time to scan the
entire document looking for more.
Rather than passing in limit=1 every time you call find_all , you can use the find() method.
These two lines of code are nearly equivalent:  soup .
find_all ( 'title' ,  limit = 1 )  # [<title>The Dormouse's story</title>]  soup .
find ( 'title' )  # <title>The Dormouse's story</title>    The only difference is that find_all() returns a list containing
the single result, and find() just returns the result.
If find_all() can’t find anything, it returns an empty list.
If find() can’t find anything, it returns None :  print ( soup .
find ( "nosuchtag" ))  # None    Remember the soup.head.title trick from Navigating using tag
names ?
That trick works by repeatedly calling find() :  soup .
head .
find ( "head" ) .
find ( "title" )  # <title>The Dormouse's story</title>      find_parents() and find_parent() ¶  Method signature: find_parents( name , attrs , string , limit , **kwargs )  Method signature: find_parent( name , attrs , string , **kwargs )  I spent a lot of time above covering find_all() and find() .
The Beautiful Soup API defines ten other methods for
searching the tree, but don’t be afraid.
Five of these methods are
basically the same as find_all() , and the other five are basically
the same as find() .
The only differences are in how they move from
one part of the tree to another.
First let’s consider find_parents() and find_parent() .
Remember that find_all() and find() work
their way down the tree, looking at tag’s descendants.
These methods
do the opposite: they work their way up the tree, looking at a tag’s
(or a string’s) parents.
Let’s try them out, starting from a string
buried deep in the “three daughters” document:  a_string  =  soup .
find ( string = "Lacie" )  a_string  # 'Lacie'  a_string .
find_parents ( "a" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]  a_string .
find_parent ( "p" )  # <p class="story">Once upon a time there were three little sisters; and their names were  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;  #  and they lived at the bottom of a well.</p>  a_string .
find_parents ( "p" ,  class_ = "title" )  # []    One of the three <a> tags is the direct parent of the string in
question, so our search finds it.
One of the three <p> tags is an
indirect parent ( ancestor ) of the string, and our search finds that as
well.
There’s a <p> tag with the CSS class “title” somewhere in the
document, but it’s not one of this string’s parents, so we can’t find
it with find_parents() .
You may have noticed a similarity between find_parent() and find_parents() , and the .parent and .parents attributes
mentioned earlier.
These search methods actually use the .parents attribute to iterate through all parents (unfiltered), checking each one
against the provided filter to see if it matches.
find_next_siblings() and find_next_sibling() ¶  Method signature: find_next_siblings( name , attrs , string , limit , **kwargs )  Method signature: find_next_sibling( name , attrs , string , **kwargs )  These methods use .next_siblings to
iterate over the rest of an element’s siblings in the tree.
The find_next_siblings() method returns all the siblings that match,
and find_next_sibling() returns only the first one:  first_link  =  soup .
a  first_link  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  first_link .
find_next_siblings ( "a" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  first_story_paragraph  =  soup .
find ( "p" ,  "story" )  first_story_paragraph .
find_next_sibling ( "p" )  # <p class="story">...</p>      find_previous_siblings() and find_previous_sibling() ¶  Method signature: find_previous_siblings( name , attrs , string , limit , **kwargs )  Method signature: find_previous_sibling( name , attrs , string , **kwargs )  These methods use .previous_siblings to iterate over an element’s
siblings that precede it in the tree.
The find_previous_siblings() method returns all the siblings that match, and find_previous_sibling() returns only the first one:  last_link  =  soup .
find ( "a" ,  id = "link3" )  last_link  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>  last_link .
find_previous_siblings ( "a" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]  first_story_paragraph  =  soup .
find_previous_sibling ( "p" )  # <p class="title"><b>The Dormouse's story</b></p>      find_all_next() and find_next() ¶  Method signature: find_all_next( name , attrs , string , limit , **kwargs )  Method signature: find_next( name , attrs , string , **kwargs )  These methods use .next_elements to
iterate over whatever tags and strings that come after it in the
document.
The find_all_next() method returns all matches, and find_next() returns only the first match:  first_link  =  soup .
find_all_next ( string = True )  # ['Elsie', ',\n', 'Lacie', ' and\n', 'Tillie',  #  ';\nand they lived at the bottom of a well.
', '\n', '...', '\n']  first_link .
find_next ( "p" )  # <p class="story">...</p>    In the first example, the string “Elsie” showed up, even though it was
contained within the <a> tag we started from.
In the second example,
the last <p> tag in the document showed up, even though it’s not in
the same part of the tree as the <a> tag we started from.
For these
methods, all that matters is that an element matches the filter and
it shows up later in the document in document order .
find_all_previous() and find_previous() ¶  Method signature: find_all_previous( name , attrs , string , limit , **kwargs )  Method signature: find_previous( name , attrs , string , **kwargs )  These methods use .previous_elements to
iterate over the tags and strings that came before it in the
document.
The find_all_previous() method returns all matches, and find_previous() only returns the first match:  first_link  =  soup .
find_all_previous ( "p" )  # [<p class="story">Once upon a time there were three little sisters; ...</p>,  #  <p class="title"><b>The Dormouse's story</b></p>]  first_link .
find_previous ( "title" )  # <title>The Dormouse's story</title>    The call to find_all_previous("p") found the first paragraph in
the document (the one with class=”title”), but it also finds the
second paragraph, the <p> tag that contains the <a> tag we started
with.
This shouldn’t be too surprising: we’re looking at all the tags
that show up earlier in the document in document order than the one we started with.
A
<p> tag that contains an <a> tag must have shown up before the <a>
tag it contains.
CSS selectors through the .css property ¶  BeautifulSoup and Tag objects support CSS selectors through
their .css property.
The actual selector implementation is handled
by the Soup Sieve package, available on PyPI as soupsieve .
If you installed
Beautiful Soup through pip , Soup Sieve was installed at the same
time, so you don’t have to do anything extra.
The Soup Sieve documentation lists all the currently supported CSS
selectors , but
here are some of the basics.
You can find tags by name:  soup .
css .
select ( "title" )  # [<title>The Dormouse's story</title>]  soup .
select ( "p:nth-of-type(3)" )  # [<p class="story">...</p>]    Find tags by ID:  soup .
select ( "#link1" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]  soup .
select ( "a#link2" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Find tags contained anywhere within other tags:  soup .
select ( "body a" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie"  id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  soup .
select ( "html head title" )  # [<title>The Dormouse's story</title>]    Find tags directly within other tags:  soup .
select ( "head > title" )  # [<title>The Dormouse's story</title>]  soup .
select ( "p > a" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie"  id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  soup .
select ( "p > a:nth-of-type(2)" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]  soup .
select ( "body > a" )  # []    Find all matching next siblings of tags:  soup .
select ( "#link1 ~ .sister" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie"  id="link3">Tillie</a>]    Find the next sibling tag (but only if it matches):  soup .
select ( "#link1 + .sister" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Find tags by CSS class:  soup .
select ( ".sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  soup .
select ( "[class~=sister]" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Find tags that match any selector from a list of selectors:  soup .
select ( "#link1,#link2" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Test for the existence of an attribute:  soup .
select ( 'a[href]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Find tags by attribute value:  soup .
select ( 'a[href="http://example.com/elsie"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]  soup .
select ( 'a[href^="http://example.com/"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  soup .
select ( 'a[href$="tillie"]' )  # [<a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]  soup .
select ( 'a[href*=".com/el"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    There’s also a method called select_one() , which finds only the
first tag that matches a selector:  soup .
select_one ( ".sister" )  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    As a convenience, you can call select() and select_one() can
directly on the BeautifulSoup or Tag object, omitting the .css property:  soup .
select_one ( ".sister" )  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    CSS selector support is a convenience for people who already know the
CSS selector syntax.
You can do all of this with the Beautiful Soup
API.
If CSS selectors are all you need, you should skip Beautiful Soup
altogether and parse the document with lxml : it’s a lot
faster.
But Soup Sieve lets you combine CSS selectors with the
Beautiful Soup API.
Advanced Soup Sieve features ¶  Soup Sieve offers a substantial API beyond the select() and select_one() methods, and you can access most of that API through
the .css attribute of Tag or BeautifulSoup .
What follows
is just a list of the supported methods; see the Soup Sieve
documentation for full
documentation.
The iselect() method works the same as select() , but it
returns a generator instead of a list:  [ tag [ 'id' ]  for  tag  in  soup .
iselect ( ".sister" )]  # ['link1', 'link2', 'link3']    The closest() method returns the nearest parent of a given Tag that matches a CSS selector, similar to Beautiful Soup’s find_parent() method:  elsie  =  soup .
select_one ( ".sister" )  elsie .
closest ( "p.story" )  # <p class="story">Once upon a time there were three little sisters; and their names were  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;  #  and they lived at the bottom of a well.</p>    The match() method returns a Boolean depending on whether or not a
specific Tag matches a selector:  # elsie.css.match("#link1")  True  # elsie.css.match("#link2")  False    The filter() method returns the subset of a tag’s direct children
that match a selector:  [ tag .
string  for  tag  in  soup .
find ( 'p' ,  'story' ) .
filter ( 'a' )]  # ['Elsie', 'Lacie', 'Tillie']    The escape() method escapes CSS identifiers that would otherwise
be invalid:  soup .
escape ( "1-strange-identifier" )  # '\\31 -strange-identifier'      Namespaces in CSS selectors ¶  If you’ve parsed XML that defines namespaces, you can use them in CSS
selectors.
:  from  bs4  import  BeautifulSoup  xml  =  """<tag xmlns:ns1="http://namespace1/" xmlns:ns2="http://namespace2/">  <ns1:child>I'm in namespace 1</ns1:child>  <ns2:child>I'm in namespace 2</ns2:child>  </tag> """  namespace_soup  =  BeautifulSoup ( xml ,  "xml" )  namespace_soup .
select ( "child" )  # [<ns1:child>I'm in namespace 1</ns1:child>, <ns2:child>I'm in namespace 2</ns2:child>]  namespace_soup .
select ( "ns1|child" )  # [<ns1:child>I'm in namespace 1</ns1:child>]    Beautiful Soup tries to use namespace prefixes that make sense based
on what it saw while parsing the document, but you can always provide
your own dictionary of abbreviations:  namespaces  =  dict ( first = "http://namespace1/" ,  second = "http://namespace2/" )  namespace_soup .
select ( "second|child" ,  namespaces = namespaces )  # [<ns1:child>I'm in namespace 2</ns1:child>]      History of CSS selector support ¶  The .css property was added in Beautiful Soup 4.12.0.
Prior to this,
only the .select() and .select_one() convenience methods were
supported.
The Soup Sieve integration was added in Beautiful Soup 4.7.0.
Earlier
versions had the .select() method, but only the most commonly-used
CSS selectors were supported.
Modifying the tree ¶  Beautiful Soup’s main strength is in searching the parse tree, but you
can also modify the tree and write your changes as a new HTML or XML
document.
Changing tag names and attributes ¶  I covered this earlier, in Tag.attrs , but it bears repeating.
You
can rename a tag, change the values of its attributes, add new
attributes, and delete attributes:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
name  =  "blockquote"  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>      Modifying .string ¶  If you set a tag’s .string attribute to a new string, the tag’s contents are
replaced with that string:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
a  tag .
string  =  "New link text."
tag  # <a href="http://example.com/">New link text.</a>    Be careful: if the tag contained other tags, they and all their
contents will be destroyed.
append() ¶  You can add to a tag’s contents with Tag.append() .
It works just
like calling .append() on a Python list:  soup  =  BeautifulSoup ( "<a>Foo</a>" ,  'html.parser' )  soup .
append ( "Bar" )  soup  # <a>FooBar</a>  soup .
contents  # ['Foo', 'Bar']      extend() ¶  Starting in Beautiful Soup 4.7.0, Tag also supports a method
called .extend() , which adds every element of a list to a Tag ,
in order:  soup  =  BeautifulSoup ( "<a>Soup</a>" ,  'html.parser' )  soup .
extend ([ "'s" ,  " " ,  "on" ])  soup  # <a>Soup's on</a>  soup .
contents  # ['Soup', ''s', ' ', 'on']      NavigableString() and .new_tag() ¶  If you need to add a string to a document, no problem–you can pass a
Python string in to append() , or you can call the NavigableString constructor:  from  bs4  import  NavigableString  soup  =  BeautifulSoup ( "<b></b>" ,  'html.parser' )  tag  =  soup .
append ( "Hello" )  new_string  =  NavigableString ( " there" )  tag .
append ( new_string )  tag  # <b>Hello there.</b>  tag .
contents  # ['Hello', ' there']    If you want to create a comment or some other subclass of NavigableString , just call the constructor:  from  bs4  import  Comment  new_comment  =  Comment ( "Nice to see you."
)  tag .
append ( new_comment )  tag  # <b>Hello there<!--Nice to see you.--></b>  tag .
contents  # ['Hello', ' there', 'Nice to see you.']
(This is a new feature in Beautiful Soup 4.4.0.)
What if you need to create a whole new tag?
The best solution is to
call the factory method BeautifulSoup.new_tag() :  soup  =  BeautifulSoup ( "<b></b>" ,  'html.parser' )  original_tag  =  soup .
b  new_tag  =  soup .
new_tag ( "a" ,  href = "http://www.example.com" )  original_tag .
append ( new_tag )  original_tag  # <b><a href="http://www.example.com"></a></b>  new_tag .
string  =  "Link text."
original_tag  # <b><a href="http://www.example.com">Link text.</a></b>    Only the first argument, the tag name, is required.
insert() ¶  Tag.insert() is just like Tag.append() , except the new element
doesn’t necessarily go at the end of its parent’s .contents .
It’ll be inserted at whatever numeric position you
say.
It works just like .insert() on a Python list:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
insert ( 1 ,  "but did not endorse " )  tag  # <a href="http://example.com/">I linked to but did not endorse <i>example.com</i></a>  tag .
contents  # ['I linked to ', 'but did not endorse ', <i>example.com</i>]      insert_before() and insert_after() ¶  The insert_before() method inserts tags or strings immediately
before something else in the parse tree:  soup  =  BeautifulSoup ( "<b>leave</b>" ,  'html.parser' )  tag  =  soup .
new_tag ( "i" )  tag .
string  =  "Don't"  soup .
insert_before ( tag )  soup .
b  # <b><i>Don't</i>leave</b>    The insert_after() method inserts tags or strings immediately
following something else in the parse tree:  div  =  soup .
new_tag ( 'div' )  div .
string  =  'ever'  soup .
i .
insert_after ( " you " ,  div )  soup .
b  # <b><i>Don't</i> you <div>ever</div> leave</b>  soup .
contents  # [<i>Don't</i>, ' you', <div>ever</div>, 'leave']      clear() ¶  Tag.clear() removes the contents of a tag:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
clear ()  tag  # <a href="http://example.com/"></a>      extract() ¶  PageElement.extract() removes a tag or string from the tree.
It
returns the tag or string that was extracted:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
a  i_tag  =  soup .
extract ()  a_tag  # <a href="http://example.com/">I linked to</a>  i_tag  # <i>example.com</i>  print ( i_tag .
parent )  # None    At this point you effectively have two parse trees: one rooted at the BeautifulSoup object you used to parse the document, and one rooted
at the tag that was extracted.
You can go on to call extract() on
a child of the element you extracted:  my_string  =  i_tag .
extract ()  my_string  # 'example.com'  print ( my_string .
parent )  # None  i_tag  # <i></i>      decompose() ¶  Tag.decompose() removes a tag from the tree, then completely
destroys it and its contents :  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
i  i_tag .
decompose ()  a_tag  # <a href="http://example.com/">I linked to</a>    The behavior of a decomposed Tag or NavigableString is not
defined and you should not use it for anything.
If you’re not sure
whether something has been decomposed, you can check its .decomposed property (new in Beautiful Soup 4.9.0) :  i_tag .
decomposed  # True  a_tag .
decomposed  # False      replace_with() ¶  PageElement.replace_with() extracts a tag or string from the tree,
then replaces it with one or more tags or strings of your choice:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
a  new_tag  =  soup .
new_tag ( "b" )  new_tag .
string  =  "example.com"  a_tag .
replace_with ( new_tag )  a_tag  # <a href="http://example.com/">I linked to <b>example.com</b></a>  bold_tag  =  soup .
new_tag ( "b" )  bold_tag .
string  =  "example"  i_tag  =  soup .
new_tag ( "i" )  i_tag .
string  =  "net"  a_tag .
replace_with ( bold_tag ,  "."
,  i_tag )  a_tag  # <a href="http://example.com/">I linked to <b>example</b>.<i>net</i></a>    replace_with() returns the tag or string that got replaced, so
that you can examine it or add it back to another part of the tree.
The ability to pass multiple arguments into replace_with() is new
in Beautiful Soup 4.10.0.    wrap() ¶  PageElement.wrap() wraps an element in the Tag object you specify.
It
returns the new wrapper:  soup  =  BeautifulSoup ( "<p>I wish I was bold.</p>" ,  'html.parser' )  soup .
wrap ( soup .
new_tag ( "b" ))  # <b>I wish I was bold.</b>  soup .
new_tag ( "div" ))  # <div><p><b>I wish I was bold.</b></p></div>    This method is new in Beautiful Soup 4.0.5.    unwrap() ¶  Tag.unwrap() is the opposite of wrap() .
It replaces a tag with
whatever’s inside that tag.
It’s good for stripping out markup:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
a  a_tag .
unwrap ()  a_tag  # <a href="http://example.com/">I linked to example.com</a>    Like replace_with() , unwrap() returns the tag
that was replaced.
smooth() ¶  After calling a bunch of methods that modify the parse tree, you may end up
with two or more NavigableString objects next to each other.
Beautiful Soup doesn’t have any problems with this, but since it can’t happen
in a freshly parsed document, you might not expect behavior like the
following:  soup  =  BeautifulSoup ( "<p>A one</p>" ,  'html.parser' )  soup .
append ( ", a two" )  soup .
contents  # ['A one', ', a two']  print ( soup .
encode ())  # b'<p>A one, a two</p>'  print ( soup .
prettify ())  # <p>  #  A one  #  , a two  # </p>    You can call Tag.smooth() to clean up the parse tree by consolidating adjacent strings:  soup .
smooth ()  soup .
contents  # ['A one, a two']  print ( soup .
prettify ())  # <p>  #  A one, a two  # </p>    This method is new in Beautiful Soup 4.8.0.
Output ¶   Pretty-printing ¶  The prettify() method will turn a Beautiful Soup parse tree into a
nicely formatted Unicode string, with a separate line for each
tag and each string:  markup  =  '<html><head><body><a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
prettify ()  # '<html>\n <head>\n </head>\n <body>\n  <a href="http://example.com/">\n...'  print ( soup .
prettify ())  # <html>  #  <head>  #  </head>  #  <body>  #   <a href="http://example.com/">  #    I linked to  #    <i>  #     example.com  #    </i>  #   </a>  #  </body>  # </html>    You can call prettify() on the top-level BeautifulSoup object,
or on any of its Tag objects:  print ( soup .
prettify ())  # <a href="http://example.com/">  #  I linked to  #  <i>  #   example.com  #  </i>  # </a>    Since it adds whitespace (in the form of newlines), prettify() changes the meaning of an HTML document and should not be used to
reformat one.
The goal of prettify() is to help you visually
understand the structure of the documents you work with.
Non-pretty printing ¶  If you just want a string, with no fancy formatting, you can call str() on a BeautifulSoup object, or on a Tag within it:  str ( soup )  # '<html><head></head><body><a href="http://example.com/">I linked to <i>example.com</i></a></body></html>'  str ( soup .
a )  # '<a href="http://example.com/">I linked to <i>example.com</i></a>'    The str() function returns a string encoded in UTF-8.
See Encodings for other options.
You can also call encode() to get a bytestring, and decode() to get Unicode.
Output formatters ¶  If you give Beautiful Soup a document that contains HTML entities like
“&lquot;”, they’ll be converted to Unicode characters:  soup  =  BeautifulSoup ( "&ldquo;Dammit!&rdquo; he said."
,  'html.parser' )  str ( soup )  # '“Dammit!” he said.'
If you then convert the document to a bytestring, the Unicode characters
will be encoded as UTF-8.
You won’t get the HTML entities back:  soup .
encode ( "utf8" )  # b'\xe2\x80\x9cDammit!\xe2\x80\x9d he said.'
By default, the only characters that are escaped upon output are bare
ampersands and angle brackets.
These get turned into “&amp;”, “&lt;”,
and “&gt;”, so that Beautiful Soup doesn’t inadvertently generate
invalid HTML or XML:  soup  =  BeautifulSoup ( "<p>The law firm of Dewey, Cheatem, & Howe</p>" ,  'html.parser' )  soup .
p  # <p>The law firm of Dewey, Cheatem, &amp; Howe</p>  soup  =  BeautifulSoup ( '<a href="http://example.com/?foo=val1&bar=val2">A link</a>' ,  'html.parser' )  soup .
a  # <a href="http://example.com/?foo=val1&amp;bar=val2">A link</a>    You can change this behavior by providing a value for the formatter argument to prettify() , encode() , or decode() .
Beautiful Soup recognizes five possible values for formatter .
The default is formatter="minimal" .
Strings will only be processed
enough to ensure that Beautiful Soup generates valid HTML/XML:  french  =  "<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>"  soup  =  BeautifulSoup ( french ,  'html.parser' )  print ( soup .
prettify ( formatter = "minimal" ))  # <p>  #  Il a dit &lt;&lt;Sacré bleu!&gt;&gt;  # </p>    If you pass in formatter="html" , Beautiful Soup will convert
Unicode characters to HTML entities whenever possible:  print ( soup .
prettify ( formatter = "html" ))  # <p>  #  Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;  # </p>    If you pass in formatter="html5" , it’s similar to formatter="html" , but Beautiful Soup will
omit the closing slash in HTML void tags like “br”:  br  =  BeautifulSoup ( "<br>" ,  'html.parser' ) .
br  print ( br .
encode ( formatter = "html" ))  # b'<br/>'  print ( br .
encode ( formatter = "html5" ))  # b'<br>'    In addition, any attributes whose values are the empty string
will become HTML-style Boolean attributes:  option  =  BeautifulSoup ( '<option selected=""></option>' ) .
option  print ( option .
encode ( formatter = "html" ))  # b'<option selected=""></option>'  print ( option .
encode ( formatter = "html5" ))  # b'<option selected></option>'    (This behavior is new as of Beautiful Soup 4.10.0.)
If you pass in formatter=None , Beautiful Soup will not modify
strings at all on output.
This is the fastest option, but it may lead
to Beautiful Soup generating invalid HTML/XML, as in these examples:  print ( soup .
prettify ( formatter = None ))  # <p>  #  Il a dit <<Sacré bleu!>>  # </p>  link_soup  =  BeautifulSoup ( '<a href="http://example.com/?foo=val1&bar=val2">A link</a>' ,  'html.parser' )  print ( link_soup .
encode ( formatter = None ))  # b'<a href="http://example.com/?foo=val1&bar=val2">A link</a>'     Formatter objects ¶  If you need more sophisticated control over your output, you can
instantiate one of Beautiful Soup’s formatter classes and pass that
object in as formatter .
HTMLFormatter ¶   Used to customize the formatting rules for HTML documents.
Here’s a formatter that converts strings to uppercase, whether they
occur in a string object or an attribute value:  from  bs4.formatter  import  HTMLFormatter  def  uppercase ( str ):  return  str .
upper ()  formatter  =  HTMLFormatter ( uppercase )  print ( soup .
prettify ( formatter = formatter ))  # <p>  #  IL A DIT <<SACRÉ BLEU!>>  # </p>  print ( link_soup .
prettify ( formatter = formatter ))  # <a href="HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2">  #  A LINK  # </a>    Here’s a formatter that increases the indentation width when pretty-printing:  formatter  =  HTMLFormatter ( indent = 8 )  print ( link_soup .
prettify ( formatter = formatter ))  # <a href="http://example.com/?foo=val1&bar=val2">  #         A link  # </a>      class  bs4.
XMLFormatter ¶   Used to customize the formatting rules for XML documents.
Writing your own formatter ¶  Subclassing HTMLFormatter or XMLFormatter will
give you even more control over the output.
For example, Beautiful
Soup sorts the attributes in every tag by default:  attr_soup  =  BeautifulSoup ( b '<p z="1" m="2" a="3"></p>' ,  'html.parser' )  print ( attr_soup .
encode ())  # <p a="3" m="2" z="1"></p>    To turn this off, you can subclass the Formatter.attributes() method, which controls which attributes are output and in what
order.
This implementation also filters out the attribute called “m”
whenever it appears:  class  UnsortedAttributes ( HTMLFormatter ):  def  attributes ( self ,  tag ):  for  k ,  v  in  tag .
items ():  if  k  ==  'm' :  continue  yield  k ,  v  print ( attr_soup .
encode ( formatter = UnsortedAttributes ()))  # <p z="1" a="3"></p>    One last caveat: if you create a CData object, the text inside
that object is always presented exactly as it appears, with no
formatting .
Beautiful Soup will call your entity substitution
function, just in case you’ve written a custom function that counts
all the strings in the document or something, but it will ignore the
return value:  from  bs4.element  import  CData  soup  =  BeautifulSoup ( "<a></a>" ,  'html.parser' )  soup .
string  =  CData ( "one < three" )  print ( soup .
prettify ( formatter = "html" ))  # <a>  #  <!
[CDATA[one < three]]>  # </a>       get_text() ¶  If you only want the human-readable text inside a document or tag, you can use the get_text() method.
It returns all the text in a document or
beneath a tag, as a single Unicode string:  markup  =  '<a href="http://example.com/"> \n I linked to <i>example.com</i> \n </a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
get_text ()  ' \n I linked to example.com \n '  soup .
get_text ()  'example.com'    You can specify a string to be used to join the bits of text
together:  # soup.get_text("|")  ' \n I linked to |example.com| \n '    You can tell Beautiful Soup to strip whitespace from the beginning and
end of each bit of text:  # soup.get_text("|", strip=True)  'I linked to|example.com'    But at that point you might want to use the .stripped_strings generator instead, and process the text yourself:  [ text  for  text  in  soup .
stripped_strings ]  # ['I linked to', 'example.com']    As of Beautiful Soup version 4.9.0, when lxml or html.parser are in
use, the contents of <script>, <style>, and <template>
tags are generally not considered to be ‘text’, since those tags are not part of
the human-visible content of the page.
As of Beautiful Soup version 4.10.0, you can call get_text(),
.strings, or .stripped_strings on a NavigableString object.
It will
either return the object itself, or nothing, so the only reason to do
this is when you’re iterating over a mixed list.
Specifying the parser to use ¶  If you just need to parse some HTML, you can dump the markup into the BeautifulSoup constructor, and it’ll probably be fine.
Beautiful
Soup will pick a parser for you and parse the data.
But there are a
few additional arguments you can pass in to the constructor to change
which parser is used.
The first argument to the BeautifulSoup constructor is a string or
an open filehandle—the source of the markup you want parsed.
The second
argument is how you’d like the markup parsed.
If you don’t specify anything, you’ll get the best HTML parser that’s
installed.
Beautiful Soup ranks lxml’s parser as being the best, then
html5lib’s, then Python’s built-in parser.
You can override this by
specifying one of the following:   What type of markup you want to parse.
Currently supported values are
“html”, “xml”, and “html5”.
The name of the parser library you want to use.
Currently supported
options are “lxml”, “html5lib”, and “html.parser” (Python’s
built-in HTML parser).
The section Installing a parser contrasts the supported parsers.
If you don’t have an appropriate parser installed, Beautiful Soup will
ignore your request and pick a different parser.
Right now, the only
supported XML parser is lxml.
If you don’t have lxml installed, asking
for an XML parser won’t give you one, and asking for “lxml” won’t work
either.
Differences between parsers ¶  Beautiful Soup presents the same interface to a number of different
parsers, but each parser is different.
Different parsers will create
different parse trees from the same document.
The biggest differences
are between the HTML parsers and the XML parsers.
Here’s a short
document, parsed as HTML using the parser that comes with Python:  BeautifulSoup ( "<a><b/></a>" ,  "html.parser" )  # <a><b></b></a>    Since a standalone <b/> tag is not valid HTML, html.parser turns it into
a <b></b> tag pair.
Here’s the same document parsed as XML (running this requires that you
have lxml installed).
Note that the standalone <b/> tag is left alone, and
that the document is given an XML declaration instead of being put
into an <html> tag.
:  print ( BeautifulSoup ( "<a><b/></a>" ,  "xml" ))  # <?xml version="1.0" encoding="utf-8"?>  # <a><b/></a>    There are also differences between HTML parsers.
If you give Beautiful
Soup a perfectly-formed HTML document, these differences won’t
matter.
One parser will be faster than another, but they’ll all give
you a data structure that looks exactly like the original HTML
document.
But if the document is not perfectly-formed, different parsers will
give different results.
Here’s a short, invalid document parsed using
lxml’s HTML parser.
Note that the <a> tag gets wrapped in <body> and
<html> tags, and the dangling </p> tag is simply ignored:  BeautifulSoup ( "<a></p>" ,  "lxml" )  # <html><body><a></a></body></html>    Here’s the same document parsed using html5lib:  BeautifulSoup ( "<a></p>" ,  "html5lib" )  # <html><head></head><body><a><p></p></a></body></html>    Instead of ignoring the dangling </p> tag, html5lib pairs it with an
opening <p> tag.
html5lib also adds an empty <head> tag; lxml didn’t
bother.
Here’s the same document parsed with Python’s built-in HTML
parser:  BeautifulSoup ( "<a></p>" ,  "html.parser" )  # <a></a>    Like lxml, this parser ignores the closing </p> tag.
Unlike
html5lib or lxml, this parser makes no attempt to create a
well-formed HTML document by adding <html> or <body> tags.
Since the document “<a></p>” is invalid, none of these techniques is
the ‘correct’ way to handle it.
The html5lib parser uses techniques
that are part of the HTML5 standard, so it has the best claim on being
the ‘correct’ way, but all three techniques are legitimate.
Differences between parsers can affect your script.
If you’re planning
on distributing your script to other people, or running it on multiple
machines, you should specify a parser in the BeautifulSoup constructor.
That will reduce the chances that your users parse a
document differently from the way you parse it.
Encodings ¶  Any HTML or XML document is written in a specific encoding like ASCII
or UTF-8.
But when you load that document into Beautiful Soup, you’ll
discover it’s been converted to Unicode:  markup  =  "<h1>Sacr \xc3\xa9 bleu!</h1>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
h1  # <h1>Sacré bleu!</h1>  soup .
h1 .
string  # 'Sacr\xe9 bleu!'
It’s not magic.
(That sure would be nice.)
Beautiful Soup uses a
sub-library called Unicode, Dammit to detect a document’s encoding
and convert it to Unicode.
The autodetected encoding is available as
the .original_encoding attribute of the BeautifulSoup object:  soup .
original_encoding  'utf-8'    Unicode, Dammit guesses correctly most of the time, but sometimes it
makes mistakes.
Sometimes it guesses correctly, but only after a
byte-by-byte search of the document that takes a very long time.
If
you happen to know a document’s encoding ahead of time, you can avoid
mistakes and delays by passing it to the BeautifulSoup constructor
as from_encoding .
Here’s a document written in ISO-8859-8.
The document is so short that
Unicode, Dammit can’t get a lock on it, and misidentifies it as
ISO-8859-7:  markup  =  b "<h1> \xed\xe5\xec\xf9 </h1>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  print ( soup .
h1 )  # <h1>νεμω</h1>  print ( soup .
original_encoding )  # iso-8859-7    We can fix this by passing in the correct from_encoding :  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  from_encoding = "iso-8859-8" )  print ( soup .
h1 )  # <h1>םולש</h1>  print ( soup .
original_encoding )  # iso8859-8    If you don’t know what the correct encoding is, but you know that
Unicode, Dammit is guessing wrong, you can pass the wrong guesses in
as exclude_encodings :  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  exclude_encodings = [ "iso-8859-7" ])  print ( soup .
original_encoding )  # WINDOWS-1255    Windows-1255 isn’t 100% correct, but that encoding is a compatible
superset of ISO-8859-8, so it’s close enough.
( exclude_encodings is a new feature in Beautiful Soup 4.4.0.)
In rare cases (usually when a UTF-8 document contains text written in
a completely different encoding), the only way to get Unicode may be
to replace some characters with the special Unicode character
“REPLACEMENT CHARACTER” (U+FFFD, �).
If Unicode, Dammit needs to do
this, it will set the .contains_replacement_characters attribute
to True on the UnicodeDammit or BeautifulSoup object.
This
lets you know that the Unicode representation is not an exact
representation of the original–some data was lost.
If a document
contains �, but .contains_replacement_characters is False ,
you’ll know that the � was there originally (as it is in this
paragraph) and doesn’t stand in for missing data.
Output encoding ¶  When you write out an output document from Beautiful Soup, you get a UTF-8
document, even if the input document wasn’t in UTF-8 to begin with.
Here’s a
document written in the Latin-1 encoding:  markup  =  b '''  <html>  <head>  <meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" />  </head>  <body>  <p>Sacr \xe9 bleu!</p>  </body>  </html>  '''  soup  =  BeautifulSoup ( markup ,  'html.parser' )  print ( soup .
prettify ())  # <html>  #  <head>  #   <meta content="text/html; charset=utf-8" http-equiv="Content-type" />  #  </head>  #  <body>  #   <p>  #    Sacré bleu!
#   </p>  #  </body>  # </html>    Note that the <meta> tag has been rewritten to reflect the fact that
the document is now in UTF-8.
If you don’t want UTF-8, you can pass an encoding into prettify() :  print ( soup .
prettify ( "latin-1" ))  # <html>  #  <head>  #   <meta content="text/html; charset=latin-1" http-equiv="Content-type" />  # ...
   You can also call encode() on the BeautifulSoup object, or any
element in the soup, just as if it were a Python string:  soup .
encode ( "latin-1" )  # b'<p>Sacr\xe9 bleu!</p>'  soup .
encode ( "utf-8" )  # b'<p>Sacr\xc3\xa9 bleu!</p>'    Any characters that can’t be represented in your chosen encoding will
be converted into numeric XML entity references.
Here’s a document
that includes the Unicode character SNOWMAN:  markup  =  u "<b> \N{SNOWMAN} </b>"  snowman_soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  snowman_soup .
b    The SNOWMAN character can be part of a UTF-8 document (it looks like
☃), but there’s no representation for that character in ISO-Latin-1 or
ASCII, so it’s converted into “&#9731” for those encodings:  print ( tag .
encode ( "utf-8" ))  # b'<b>\xe2\x98\x83</b>'  print ( tag .
encode ( "latin-1" ))  # b'<b>&#9731;</b>'  print ( tag .
encode ( "ascii" ))  # b'<b>&#9731;</b>'      Unicode, Dammit ¶  You can use Unicode, Dammit without using Beautiful Soup.
It’s useful
whenever you have data in an unknown encoding and you just want it to
become Unicode:  from  bs4  import  UnicodeDammit  dammit  =  UnicodeDammit ( b " \xc2\xab Sacr \xc3\xa9 bleu!
\xc2\xbb " )  print ( dammit .
unicode_markup )  # «Sacré bleu!»  dammit .
original_encoding  # 'utf-8'    Unicode, Dammit’s guesses will get a lot more accurate if you install
one of these Python libraries: charset-normalizer , chardet , or cchardet .
The more data you give Unicode, Dammit, the more
accurately it will guess.
If you have your own suspicions as to what
the encoding might be, you can pass them in as a list:  dammit  =  UnicodeDammit ( "Sacr \xe9 bleu!"
,  [ "latin-1" ,  "iso-8859-1" ])  print ( dammit .
unicode_markup )  # Sacré bleu!
dammit .
original_encoding  # 'latin-1'    Unicode, Dammit has two special features that Beautiful Soup doesn’t
use.
Smart quotes ¶  You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML
entities:  markup  =  b "<p>I just \x93 love \x94 Microsoft Word \x92 s smart quotes</p>"  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "html" ) .
unicode_markup  # '<p>I just &ldquo;love&rdquo; Microsoft Word&rsquo;s smart quotes</p>'  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "xml" ) .
unicode_markup  # '<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'    You can also convert Microsoft smart quotes to ASCII quotes:  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "ascii" ) .
unicode_markup  # '<p>I just "love" Microsoft Word\'s smart quotes</p>'    Hopefully you’ll find this feature useful, but Beautiful Soup doesn’t
use it.
Beautiful Soup prefers the default behavior, which is to
convert Microsoft smart quotes to Unicode characters along with
everything else:  UnicodeDammit ( markup ,  [ "windows-1252" ]) .
unicode_markup  # '<p>I just “love” Microsoft Word’s smart quotes</p>'      Inconsistent encodings ¶  Sometimes a document is mostly in UTF-8, but contains Windows-1252
characters such as (again) Microsoft smart quotes.
This can happen
when a website includes data from multiple sources.
You can use UnicodeDammit.detwingle() to turn such a document into pure
UTF-8.
Here’s a simple example:  snowmen  =  ( u " \N{SNOWMAN} "  *  3 )  quote  =  ( u " \N{LEFT DOUBLE QUOTATION MARK} I like snowmen!
\N{RIGHT DOUBLE QUOTATION MARK} " )  doc  =  snowmen .
encode ( "utf8" )  +  quote .
encode ( "windows_1252" )    This document is a mess.
The snowmen are in UTF-8 and the quotes are
in Windows-1252.
You can display the snowmen or the quotes, but not
both:  print ( doc )  # ☃☃☃�I like snowmen!�  print ( doc .
decode ( "windows-1252" ))  # â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”    Decoding the document as UTF-8 raises a UnicodeDecodeError , and
decoding it as Windows-1252 gives you gibberish.
Fortunately, UnicodeDammit.detwingle() will convert the string to pure UTF-8,
allowing you to decode it to Unicode and display the snowmen and quote
marks simultaneously:  new_doc  =  UnicodeDammit .
detwingle ( doc )  print ( new_doc .
decode ( "utf8" ))  # ☃☃☃“I like snowmen!”    UnicodeDammit.detwingle() only knows how to handle Windows-1252
embedded in UTF-8 (or vice versa, I suppose), but this is the most
common case.
Note that you must know to call UnicodeDammit.detwingle() on your
data before passing it into BeautifulSoup or the UnicodeDammit constructor.
Beautiful Soup assumes that a document has a single
encoding, whatever it might be.
If you pass it a document that
contains both UTF-8 and Windows-1252, it’s likely to think the whole
document is Windows-1252, and the document will come out looking like â˜ƒâ˜ƒâ˜ƒ“I  like  snowmen!” .
UnicodeDammit.detwingle() is new in Beautiful Soup 4.1.0.
Line numbers ¶  The html.parser and html5lib parsers can keep track of where in
the original document each Tag was found.
You can access this
information as Tag.sourceline (line number) and Tag.sourcepos (position of the start tag within a line):  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  for  tag  in  soup .
find_all ( 'p' ):  print ( repr (( tag .
sourceline ,  tag .
sourcepos ,  tag .
string )))  # (1, 0, 'Paragraph 1')  # (3, 4, 'Paragraph 2')    Note that the two parsers mean slightly different things by sourceline and sourcepos .
For html.parser, these numbers
represent the position of the initial less-than sign.
For html5lib,
these numbers represent the position of the final greater-than sign:  soup  =  BeautifulSoup ( markup ,  'html5lib' )  for  tag  in  soup .
string )))  # (2, 0, 'Paragraph 1')  # (3, 6, 'Paragraph 2')    You can shut off this feature by passing store_line_numbers=False into the BeautifulSoup constructor:  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  store_line_numbers = False )  print ( soup .
sourceline )  # None    This feature is new in 4.8.1, and the parsers based on lxml don’t
support it.
Comparing objects for equality ¶  Beautiful Soup says that two NavigableString or Tag objects
are equal when they represent the same HTML or XML markup, even if their
attributes are in a different order or they live in different parts of the
object tree.
In this example, the two <b> tags are treated as equal, because
they both look like “<b>pizza</b>”:  markup  =  "<p>I want <b>pizza</b> and more <b>pizza</b>!</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  first_b ,  second_b  =  soup .
find_all ( 'b' )  print ( first_b  ==  second_b )  # True  print ( first_b .
previous_element  ==  second_b .
previous_element )  # False    If you want to see whether two variables refer to exactly the same
object, use is :  print ( first_b  is  second_b )  # False      Copying Beautiful Soup objects ¶  You can use copy.copy() to create a copy of any Tag or NavigableString :  import  copy  p_copy  =  copy .
copy ( soup .
p )  print ( p_copy )  # <p>I want <b>pizza</b> and more <b>pizza</b>!</p>    The copy is considered equal to the original, since it represents the
same markup as the original, but it’s not the same object:  print ( soup .
p  ==  p_copy )  # True  print ( soup .
p  is  p_copy )  # False    The only real difference is that the copy is completely detached from
the original Beautiful Soup object tree, just as if extract() had
been called on it:  print ( p_copy .
parent )  # None    This is because two different Tag objects can’t occupy the same
space at the same time.
Advanced parser customization ¶  Beautiful Soup offers a number of ways to customize how the parser
treats incoming HTML and XML.
This section covers the most commonly
used customization techniques.
Parsing only part of a document ¶  Let’s say you want to use Beautiful Soup to look at a document’s <a>
tags.
It’s a waste of time and memory to parse the entire document and
then go over it again looking for <a> tags.
It would be much faster to
ignore everything that wasn’t an <a> tag in the first place.
The SoupStrainer class allows you to choose which parts of an incoming
document are parsed.
You just create a SoupStrainer and pass it in
to the BeautifulSoup constructor as the parse_only argument.
(Note that this feature won’t work if you’re using the html5lib parser .
If you use html5lib, the whole document will be parsed, no
matter what.
This is because html5lib constantly rearranges the parse
tree as it works, and if some part of the document didn’t actually
make it into the parse tree, it’ll crash.
To avoid confusion, in the
examples below I’ll be forcing Beautiful Soup to use Python’s
built-in parser.)
SoupStrainer ¶   The SoupStrainer class takes the same arguments as a typical
method from Searching the tree : name , attrs , string , and **kwargs .
Here are
three SoupStrainer objects:  from  bs4  import  SoupStrainer  only_a_tags  =  SoupStrainer ( "a" )  only_tags_with_id_link2  =  SoupStrainer ( id = "link2" )  def  is_short_string ( string ):  return  string  is  not  None  and  len ( string )  <  10  only_short_strings  =  SoupStrainer ( string = is_short_string )    I’m going to bring back the “three sisters” document one more time,
and we’ll see what the document looks like when it’s parsed with these
three SoupStrainer objects:  html_doc  =  """<html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_a_tags ) .
prettify ())  # <a class="sister" href="http://example.com/elsie" id="link1">  #  Elsie  # </a>  # <a class="sister" href="http://example.com/lacie" id="link2">  #  Lacie  # </a>  # <a class="sister" href="http://example.com/tillie" id="link3">  #  Tillie  # </a>  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_tags_with_id_link2 ) .
prettify ())  # <a class="sister" href="http://example.com/lacie" id="link2">  #  Lacie  # </a>  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_short_strings ) .
prettify ())  # Elsie  # ,  # Lacie  # and  # Tillie  # ...
 #    The SoupStrainer behavior is as follows:   When a tag matches, it is kept (including all its contents, whether they also
match or not).
When a tag does not match, the tag itself is not kept, but parsing continues
into its contents to look for other tags that do match.
You can also pass a SoupStrainer into any of the methods covered
in Searching the tree .
This probably isn’t terribly useful, but I
thought I’d mention it:  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )  soup .
find_all ( only_short_strings )  # ['\n\n', '\n\n', 'Elsie', ',\n', 'Lacie', ' and\n', 'Tillie',  #  '\n\n', '...', '\n']      Customizing multi-valued attributes ¶  In an HTML document, an attribute like class is given a list of
values, and an attribute like id is given a single value, because
the HTML specification treats those attributes differently:  markup  =  '<a class="cls1 cls2" id="id1 id2">'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
a [ 'class' ]  # ['cls1', 'cls2']  soup .
a [ 'id' ]  # 'id1 id2'    You can turn this off by passing in multi_valued_attributes=None .
Than all attributes will be given a
single value:  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  multi_valued_attributes = None )  soup .
a [ 'class' ]  # 'cls1 cls2'  soup .
a [ 'id' ]  # 'id1 id2'    You can customize this behavior quite a bit by passing in a
dictionary for multi_valued_attributes .
If you need this, look at HTMLTreeBuilder.DEFAULT_CDATA_LIST_ATTRIBUTES to see the
configuration Beautiful Soup uses by default, which is based on the
HTML specification.
(This is a new feature in Beautiful Soup 4.8.0.)
Handling duplicate attributes ¶  When using the html.parser parser, you can use the on_duplicate_attribute constructor argument to customize what
Beautiful Soup does when it encounters a tag that defines the same
attribute more than once:  markup  =  '<a href="http://url1/" href="http://url2/">'    The default behavior is to use the last value found for the tag:  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
a [ 'href' ]  # http://url2/  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  on_duplicate_attribute = 'replace' )  soup .
a [ 'href' ]  # http://url2/    With on_duplicate_attribute='ignore' you can tell Beautiful Soup
to use the first value found and ignore the rest:  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  on_duplicate_attribute = 'ignore' )  soup .
a [ 'href' ]  # http://url1/    (lxml and html5lib always do it this way; their behavior can’t be
configured from within Beautiful Soup.)
If you need more control, you can pass in a function that’s called on each
duplicate value:  def  accumulate ( attributes_so_far ,  key ,  value ):  if  not  isinstance ( attributes_so_far [ key ],  list ):  attributes_so_far [ key ]  =  [ attributes_so_far [ key ]]  attributes_so_far [ key ] .
append ( value )  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  on_duplicate_attribute = accumulate )  soup .
a [ 'href' ]  # ["http://url1/", "http://url2/"]    (This is a new feature in Beautiful Soup 4.9.1.)
Instantiating custom subclasses ¶  When a parser tells Beautiful Soup about a tag or a string, Beautiful
Soup will instantiate a Tag or NavigableString object to
contain that information.
Instead of that default behavior, you can
tell Beautiful Soup to instantiate subclasses of Tag or NavigableString , subclasses you define with custom behavior:  from  bs4  import  Tag ,  NavigableString  class  MyTag ( Tag ):  pass  class  MyString ( NavigableString ):  pass  markup  =  "<div>some text</div>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  isinstance ( soup .
div ,  MyTag )  # False  isinstance ( soup .
div .
string ,  MyString )  # False  my_classes  =  {  Tag :  MyTag ,  NavigableString :  MyString  }  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  element_classes = my_classes )  isinstance ( soup .
div ,  MyTag )  # True  isinstance ( soup .
string ,  MyString )  # True    This can be useful when incorporating Beautiful Soup into a test
framework.
(This is a new feature in Beautiful Soup 4.8.1.)
Troubleshooting ¶   diagnose() ¶  If you’re having trouble understanding what Beautiful Soup does to a
document, pass the document into the diagnose() function.
(This function is new in
Beautiful Soup 4.2.0.)
Beautiful Soup will print out a report showing
you how different parsers handle the document, and tell you if you’re
missing a parser that Beautiful Soup could be using:  from  bs4.diagnose  import  diagnose  with  open ( "bad.html" )  as  fp :  data  =  fp .
read ()  diagnose ( data )  # Diagnostic running on Beautiful Soup 4.2.0  # Python version 2.7.3 (default, Aug  1 2012, 05:16:07)  # I noticed that html5lib is not installed.
Installing it may help.
# Found lxml version 2.3.2.0  #  # Trying to parse your data with html.parser  # Here's what html.parser did with the document:  # ...
   Just looking at the output of diagnose() might show you how to solve the
problem.
Even if not, you can paste the output of diagnose() when
asking for help.
Errors when parsing a document ¶  There are two different kinds of parse errors.
There are crashes,
where you feed a document to Beautiful Soup and it raises an
exception (usually an HTMLParser.HTMLParseError ).
And there is
unexpected behavior, where a Beautiful Soup parse tree looks a lot
different than the document used to create it.
These problems are almost never problems with Beautiful Soup itself.
This is not because Beautiful Soup is an amazingly well-written piece
of software.
It’s because Beautiful Soup doesn’t include any parsing
code.
Instead, it relies on external parsers.
If one parser isn’t
working on a certain document, the best solution is to try a different
parser.
See Installing a parser for details and a parser
comparison.
If this doesn’t help, you might need to inspect the
document tree found inside the BeautifulSoup object, to see where
the markup you’re looking for actually ended up.
Version mismatch problems ¶   SyntaxError:  Invalid  syntax (on the line ROOT_TAG_NAME  =  '[document]' ): Caused by running an old Python 2 version of
Beautiful Soup under Python 3, without converting the code.
ImportError:  No  module  named  HTMLParser - Caused by running an old
Python 2 version of Beautiful Soup under Python 3.
ImportError:  No  module  named  html.parser - Caused by running the
Python 3 version of Beautiful Soup under Python 2.
ImportError:  No  module  named  BeautifulSoup - Caused by running
Beautiful Soup 3 code in an environment that doesn’t have BS3
installed.
Or, by writing Beautiful Soup 4 code without knowing that
the package name has changed to bs4 .
ImportError:  No  module  named  bs4 - Caused by running Beautiful
Soup 4 code in an environment that doesn’t have BS4 installed.
Parsing XML ¶  By default, Beautiful Soup parses documents as HTML.
To parse a
document as XML, pass in “xml” as the second argument to the BeautifulSoup constructor:  soup  =  BeautifulSoup ( markup ,  "xml" )    You’ll need to have lxml installed .
Other parser problems ¶   If your script works on one computer but not another, or in one
virtual environment but not another, or outside the virtual
environment but not inside, it’s probably because the two
environments have different parser libraries available.
For example,
you may have developed the script on a computer that has lxml
installed, and then tried to run it on a computer that only has
html5lib installed.
See Differences between parsers for why this
matters, and fix the problem by mentioning a specific parser library
in the BeautifulSoup constructor.
Because HTML tags and attributes are case-insensitive , all three HTML
parsers convert tag and attribute names to lowercase.
That is, the
markup <TAG></TAG> is converted to <tag></tag>.
If you want to
preserve mixed-case or uppercase tags and attributes, you’ll need to parse the document as XML.
Miscellaneous ¶   UnicodeEncodeError:  'charmap'  codec  can't  encode  character  '\xfoo'  in  position  bar (or just about any other UnicodeEncodeError ) - This problem shows up in two main
situations.
First, when you try to print a Unicode character that
your console doesn’t know how to display.
(See this page on the
Python wiki for help.)
Second, when you’re writing to a file and you pass in a Unicode
character that’s not supported by your default encoding.
In this
case, the simplest solution is to explicitly encode the Unicode
string into UTF-8 with u.encode("utf8") .
KeyError:  [attr] - Caused by accessing tag['attr'] when the
tag in question doesn’t define the attr attribute.
The most
common errors are KeyError:  'href' and KeyError:  'class' .
Use tag.get('attr') if you’re not sure attr is
defined, just as you would with a Python dictionary.
AttributeError:  'ResultSet'  object  has  no  attribute  'foo' - This
usually happens because you expected find_all() to return a
single tag or string.
But find_all() returns a list of tags
and strings–a ResultSet object.
You need to iterate over the
list and look at the .foo of each one.
Or, if you really only
want one result, you need to use find() instead of find_all() .
AttributeError:  'NoneType'  object  has  no  attribute  'foo' - This
usually happens because you called find() and then tried to
access the .foo` attribute of the result.
But in your case, find() didn’t find anything, so it returned None , instead of
returning a tag or a string.
You need to figure out why your find() call isn’t returning anything.
AttributeError:  'NavigableString'  object  has  no  attribute  'foo' - This usually happens because you’re treating a string as
though it were a tag.
You may be iterating over a list, expecting
that it contains nothing but tags, when it actually contains both tags and
strings.
Improving Performance ¶  Beautiful Soup will never be as fast as the parsers it sits on top
of.
If response time is critical, if you’re paying for computer time
by the hour, or if there’s any other reason why computer time is more
valuable than programmer time, you should forget about Beautiful Soup
and work directly atop lxml .
That said, there are things you can do to speed up Beautiful Soup.
If
you’re not using lxml as the underlying parser, my advice is to start .
Beautiful Soup parses documents
significantly faster using lxml than using html.parser or html5lib.
You can speed up encoding detection significantly by installing the cchardet library.
Parsing only part of a document won’t save you much time parsing
the document, but it can save a lot of memory, and it’ll make searching the document much faster.
Translating this documentation ¶  New translations of the Beautiful Soup documentation are greatly
appreciated.
Translations should be licensed under the MIT license,
just like Beautiful Soup and its English documentation are.
There are two ways of getting your translation into the main code base
and onto the Beautiful Soup website:   Create a branch of the Beautiful Soup repository, add your
translation, and propose a merge with the main branch, the same
as you would do with a proposed change to the source code.
Send a message to the Beautiful Soup discussion group with a link to
your translation, or attach your translation to the message.
Use the Chinese or Brazilian Portuguese translations as your model.
In
particular, please translate the source file doc/source/index.rst ,
rather than the HTML version of the documentation.
This makes it
possible to publish the documentation in a variety of formats, not
just HTML.
Beautiful Soup 3 ¶  Beautiful Soup 3 is the previous release series, and is no longer
supported.
Development of Beautiful Soup 3 stopped in 2012, and the
package was completely discontinued in 2021.
There’s no reason to
install it unless you’re trying to get very old software to work, but
it’s published through PyPi as BeautifulSoup :  $  pip  install  BeautifulSoup  You can also download a tarball of the final release, 3.2.2 .
If you ran pip  install  beautifulsoup or pip  install  BeautifulSoup , but your code doesn’t work, you installed Beautiful
Soup 3 by mistake.
You need to run pip  install  beautifulsoup4 .
The documentation for Beautiful Soup 3 is archived online .
Porting code to BS4 ¶  Most code written against Beautiful Soup 3 will work against Beautiful
Soup 4 with one simple change.
All you should have to do is change the
package name from BeautifulSoup to bs4 .
So this:  from  BeautifulSoup  import  BeautifulSoup    becomes this:  from  bs4  import  BeautifulSoup     If you get the ImportError “No module named BeautifulSoup”, your
problem is that you’re trying to run Beautiful Soup 3 code, but you
only have Beautiful Soup 4 installed.
If you get the ImportError “No module named bs4”, your problem
is that you’re trying to run Beautiful Soup 4 code, but you only
have Beautiful Soup 3 installed.
Although BS4 is mostly backward-compatible with BS3, most of its
methods have been deprecated and given new names for PEP 8 compliance .
There are numerous other
renames and changes, and a few of them break backward compatibility.
Here’s what you’ll need to know to convert your BS3 code and habits to BS4:   You need a parser ¶  Beautiful Soup 3 used Python’s SGMLParser , a module that was
deprecated and removed in Python 3.0.
Beautiful Soup 4 uses html.parser by default, but you can plug in lxml or html5lib and
use that instead.
See Installing a parser for a comparison.
Since html.parser is not the same parser as SGMLParser , you
may find that Beautiful Soup 4 gives you a different parse tree than
Beautiful Soup 3 for the same markup.
If you swap out html.parser for lxml or html5lib, you may find that the parse tree changes yet
again.
If this happens, you’ll need to update your scraping code to
process the new tree.
Method names ¶   renderContents -> encode_contents  replaceWith -> replace_with  replaceWithChildren -> unwrap  findAll -> find_all  findAllNext -> find_all_next  findAllPrevious -> find_all_previous  findNext -> find_next  findNextSibling -> find_next_sibling  findNextSiblings -> find_next_siblings  findParent -> find_parent  findParents -> find_parents  findPrevious -> find_previous  findPreviousSibling -> find_previous_sibling  findPreviousSiblings -> find_previous_siblings  getText -> get_text  nextSibling -> next_sibling  previousSibling -> previous_sibling   Some arguments to the Beautiful Soup constructor were renamed for the
same reasons:   BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...)  BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...)   I renamed one method for compatibility with Python 3:   Tag.has_key() -> Tag.has_attr()   I renamed one attribute to use more accurate terminology:   Tag.isSelfClosing -> Tag.is_empty_element   I renamed three attributes to avoid using words that have special
meaning to Python.
Unlike the others, these changes are not backwards
compatible.
If you used these attributes in BS3, your code will break
in BS4 until you change them.
UnicodeDammit.unicode -> UnicodeDammit.unicode_markup  Tag.next -> Tag.next_element  Tag.previous -> Tag.previous_element   These methods are left over from the Beautiful Soup 2 API.
They’ve
been deprecated since 2006 and should not be used at all:   Tag.fetchNextSiblings  Tag.fetchPreviousSiblings  Tag.fetchPrevious  Tag.fetchPreviousSiblings  Tag.fetchParents  Tag.findChild  Tag.findChildren     Generators ¶  I gave the generators PEP 8-compliant names, and transformed them into
properties:   childGenerator() -> children  nextGenerator() -> next_elements  nextSiblingGenerator() -> next_siblings  previousGenerator() -> previous_elements  previousSiblingGenerator() -> previous_siblings  recursiveChildGenerator() -> descendants  parentGenerator() -> parents   So instead of this:  for  parent  in  tag .
parentGenerator ():  ...
   You can write this:  for  parent  in  tag .
parents :  ...
   (But the old code will still work.)
Some of the generators used to yield None after they were done, and
then stop.
That was a bug.
Now the generators just stop.
There are two new generators, .strings and
.stripped_strings .
.strings yields
NavigableString objects, and .stripped_strings yields Python
strings that have had whitespace stripped.
XML ¶  There is no longer a BeautifulStoneSoup class for parsing XML.
To
parse XML you pass in “xml” as the second argument to the BeautifulSoup constructor.
For the same reason, the BeautifulSoup constructor no longer recognizes the isHTML argument.
Beautiful Soup’s handling of empty-element XML tags has been
improved.
Previously when you parsed XML you had to explicitly say
which tags were considered empty-element tags.
The selfClosingTags argument to the constructor is no longer recognized.
Instead,
Beautiful Soup considers any empty tag to be an empty-element tag.
If
you add a child to an empty-element tag, it stops being an
empty-element tag.
Entities ¶  An incoming HTML or XML entity is always converted into the
corresponding Unicode character.
Beautiful Soup 3 had a number of
overlapping ways of dealing with entities, which have been
removed.
The BeautifulSoup constructor no longer recognizes the smartQuotesTo or convertEntities arguments.
( Unicode,
Dammit still has smart_quotes_to , but its default is now to turn
smart quotes into Unicode.)
The constants HTML_ENTITIES , XML_ENTITIES , and XHTML_ENTITIES have been removed, since they
configure a feature (transforming some but not all entities into
Unicode characters) that no longer exists.
If you want to turn Unicode characters back into HTML entities on
output, rather than turning them into UTF-8 characters, you need to
use an output formatter .
Miscellaneous ¶  Tag.string now operates recursively.
If tag A
contains a single tag B and nothing else, then A.string is the same as
B.string.
(Previously, it was None.)
Multi-valued attributes like class have lists of strings as
their values, not simple strings.
This may affect the way you search by CSS
class.
Tag objects now implement the __hash__ method, such that two Tag objects are considered equal if they generate the same
markup.
This may change your script’s behavior if you put Tag objects into a dictionary or set.
If you pass one of the find* methods both string  and a tag-specific argument like name , Beautiful Soup will
search for tags that match your tag-specific criteria and whose Tag.string matches your string value.
It will not find the strings themselves.
Previously,
Beautiful Soup ignored the tag-specific arguments and looked for
strings.
The BeautifulSoup constructor no longer recognizes the markupMassage argument.
It’s now the parser’s responsibility to
handle markup correctly.
The rarely-used alternate parser classes like ICantBelieveItsBeautifulSoup and BeautifulSOAP have been
removed.
It’s now the parser’s decision how to handle ambiguous
markup.
The prettify() method now returns a Unicode string, not a bytestring.
Table of Contents   Beautiful Soup Documentation  Getting help    Quick Start  Installing Beautiful Soup  Installing a parser    Making the soup  Kinds of objects  Tag  Tag.name  Tag.attrs    NavigableString  BeautifulSoup  Special strings  Comment  For HTML documents  Stylesheet  Script  Template    For XML documents  Declaration  Doctype  CData  ProcessingInstruction        Navigating the tree  Going down  Navigating using tag names  .contents and .children  .descendants  .string  .strings and stripped_strings    Going up  .parent  .parents    Going sideways  .next_sibling and .previous_sibling  .next_siblings and .previous_siblings    Going back and forth  .next_element and .previous_element  .next_elements and .previous_elements      Searching the tree  Kinds of filters  A string  A regular expression  True  A function  A list    find_all()  The name argument  The keyword arguments  Searching by CSS class  The string argument  The limit argument  The recursive argument    Calling a tag is like calling find_all()  find()  find_parents() and find_parent()  find_next_siblings() and find_next_sibling()  find_previous_siblings() and find_previous_sibling()  find_all_next() and find_next()  find_all_previous() and find_previous()  CSS selectors through the .css property  Advanced Soup Sieve features  Namespaces in CSS selectors  History of CSS selector support      Modifying the tree  Changing tag names and attributes  Modifying .string  append()  extend()  NavigableString() and .new_tag()  insert()  insert_before() and insert_after()  clear()  extract()  decompose()  replace_with()  wrap()  unwrap()  smooth()    Output  Pretty-printing  Non-pretty printing  Output formatters  Formatter objects  HTMLFormatter  XMLFormatter    Writing your own formatter    get_text()    Specifying the parser to use  Differences between parsers    Encodings  Output encoding  Unicode, Dammit  Smart quotes  Inconsistent encodings      Line numbers  Comparing objects for equality  Copying Beautiful Soup objects  Advanced parser customization  Parsing only part of a document  SoupStrainer    Customizing multi-valued attributes  Handling duplicate attributes  Instantiating custom subclasses    Troubleshooting  diagnose()  Errors when parsing a document  Version mismatch problems  Parsing XML  Other parser problems  Miscellaneous  Improving Performance    Translating this documentation  Beautiful Soup 3  Porting code to BS4  You need a parser  Method names  Generators  XML  Entities  Miscellaneous         This Page   Show Source     Quick search               Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation    © Copyright 2004-2023, Leonard Richardson.
Created using Sphinx 7.2.6.
Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Index        Index   A | B | C | D | H | M | N | P | S | T | X   A    attrs (bs4.Tag attribute)     B    BeautifulSoup (class in bs4)     bs4  module      C    CData (class in bs4)     Comment (class in bs4)     D    Declaration (class in bs4)     Doctype (class in bs4)     H    HTMLFormatter (class in bs4)     M    module  bs4      N    name (bs4.Tag attribute)     NavigableString (class in bs4)     P    ProcessingInstruction (class in bs4)     S    Script (class in bs4)     SoupStrainer (class in bs4)   Stylesheet (class in bs4)     T    Tag (class in bs4)     Template (class in bs4)     X    XMLFormatter (class in bs4)            Quick search               Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Index    © Copyright 2004-2023, Leonard Richardson.
Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Python Module Index        Python Module Index   b     b    bs4          Quick search               Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Python Module Index    © Copyright 2004-2023, Leonard Richardson.
Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
[ Download | Documentation | Hall of Fame | For enterprise | Source | Changelog | Discussion group | Zine ]   Beautiful Soup   You didn't write that awful page.
You're just trying to get some
data out of it.
Beautiful Soup is here to help.
Since 2004, it's been
saving programmers hours or days of work on quick-turnaround
screen scraping projects.
Beautiful Soup is a Python library designed for quick turnaround
projects like screen-scraping.
Three features make it powerful:  Beautiful Soup provides a few simple methods and Pythonic idioms
for navigating, searching, and modifying a parse tree: a toolkit for
dissecting a document and extracting what you need.
It doesn't take
much code to write an application Beautiful Soup automatically converts incoming documents to
Unicode and outgoing documents to UTF-8.
You don't have to think
about encodings, unless the document doesn't specify an encoding and
Beautiful Soup can't detect one.
Then you just have to specify the
original encoding.
Beautiful Soup sits on top of popular Python parsers like lxml and html5lib , allowing you
to try out different parsing strategies or trade speed for
flexibility.
Beautiful Soup parses anything you give it, and does the tree
traversal stuff for you.
You can tell it "Find all the links", or
"Find all the links of class externalLink ", or "Find all the
links whose urls match "foo.com", or "Find the table heading that's
got bold text, then give me that text."
Valuable data that was once locked up in poorly-designed websites
is now within your reach.
Projects that would have taken hours take
only minutes with Beautiful Soup.
Interested?
Read more.
Getting and giving support    Beautiful Soup for enterprise available via Tidelift    If you have questions, send them to the discussion
group .
If you find a bug, file it on Launchpad .
If it's a security vulnerability, report it confidentially through Tidelift .
If you use Beautiful Soup as part of your work, please consider a Tidelift subscription .
This will support many of the free software projects your organization depends on, not just Beautiful Soup.
If Beautiful Soup is useful to you on a personal level, you might like to read Tool Safety , a short zine I wrote about what I learned about software development from working on Beautiful Soup.
Thanks!
Download Beautiful Soup  The current release is Beautiful Soup
4.12.3 (January 17, 2024).
You can install Beautiful Soup 4 with pip install beautifulsoup4 .
In Debian and Ubuntu, Beautiful Soup is available as the python3-bs4 package.
In Fedora it's
available as the python3-beautifulsoup4 package.
Beautiful Soup is licensed under the MIT license, so you can also
download the tarball, drop the bs4/ directory into almost
any Python application (or into your library path) and start using it
immediately.
Beautiful Soup 4 is supported on Python versions 3.6 and
greater.
Support for Python 2 was discontinued on January 1, 2021—one
year after the Python 2 sunsetting date.
Beautiful Soup 3  Beautiful Soup 3 was the official release line of Beautiful Soup
from May 2006 to March 2012.
It does not support Python 3 and was
discontinued or January 1, 2021—one year after the Python 2
sunsetting date.
If you have any active projects using Beautiful Soup
3, you should migrate to Beautiful Soup 4 as part of your Python 3
conversion.
Here's
the Beautiful Soup 3 documentation.
The current and hopefully final release of Beautiful Soup 3 is 3.2.2 (October 5,
2019).
It's the BeautifulSoup package on pip.
It's also
available as python-beautifulsoup in Debian and Ubuntu,
and as python-BeautifulSoup in Fedora.
Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.
Beautiful Soup 3, like Beautiful Soup 4, is supported through Tidelift .
Hall of Fame  Over the years, Beautiful Soup has been used in hundreds of
different projects.
There's no way I can list them all, but I want to
highlight a few high-profile projects.
Beautiful Soup isn't what makes
these projects interesting, but it did make their completion easier:  "Movable
 Type" , a work of digital art on display in the lobby of the New
 York Times building, uses Beautiful Soup to scrape news feeds.
Jiabao Lin's DXY-COVID-19-Crawler uses Beautiful Soup to scrape a Chinese medical site for information
about COVID-19, making it easier for researchers to track the spread
of the virus.
(Source: "How open source software is fighting COVID-19" ) Reddit uses Beautiful Soup to parse
a page that's been linked to and find a representative image .
Alexander Harrowell uses Beautiful Soup to track the business
 activities of an arms merchant.
The developers of Python itself used Beautiful Soup to migrate the Python
bug tracker from Sourceforge to Roundup .
The Lawrence Journal-World uses Beautiful Soup to gather
statewide election results .
The NOAA's Forecast
Applications Branch uses Beautiful Soup in TopoGrabber , a script for
downloading "high resolution USGS datasets."
If you've used Beautiful Soup in a project you'd like me to know
about, please do send email to me or the discussion
group .
Development  Development happens at Launchpad .
You can get the source
code or file
bugs .
This document is part of Crummy, the webspace of Leonard Richardson ( contact information ).
It was last modified on Wednesday, January 17 2024, 16:54:42 Nowhere Standard Time and last built on Wednesday, April 17 2024, 20:00:01 Nowhere Standard Time.
Crummy is © 1996-2024 Leonard Richardson.
Unless otherwise noted, all text licensed under a Creative Commons License .
[Внешняя ссылка] (This document is
also available in Russian translation.
[External link])  Beautiful Soup 3 has been replaced
 by Beautiful
 Soup 4 .
You may be looking for the Beautiful
 Soup 4 documentation  Beautiful Soup 3 only works on Python 2.x, but Beautiful Soup 4
 also works on Python 3.x.
Beautiful Soup 4 is faster, has more
 features, and works with third-party parsers like lxml and
 html5lib.
You should use Beautiful Soup 4 for all new projects, and you should port any existing projects to Beautiful Soup 4.
Beautiful Soup 3 will be discontinued on or after December 31, 2020.
Beautiful Soup 3 は Beautiful
Soup 4 に更新されました。 あなたが探しているのは、 Beautiful
Soup 4 documentation ではありませんか。 Beautiful Soup 4 ドキュメント は日本語
でも読むことができます。 このドキュメントでは、（外部リンク）日本語訳でもご覧になれます。  Beautiful Soup 3は2020年12月31日以降に中止されます。   Beautiful Soup 3已经被Beautiful Soup 4替代.请在新的项目中查看 Beautiful Soup 4 的文档.
Beautiful Soup 3只能在python2.x版本中运行,而Beautiful Soup 4还可以在python3.x版本中运行.Beautiful Soup 4速度更快,特性更多,而且与第三方的文档解析库(如lxml和html5lib)协同工作.推荐在新的项目中使用Beautiful Soup 4.
Beautiful Soup 3 将于2020年12月31日或之后停产。    Beautiful
Soup is an HTML/XML parser for Python that can turn even invalid
markup into a parse tree.
It provides simple, idiomatic ways of
navigating, searching, and modifying the parse tree.
It commonly saves
programmers hours or days of work.
There's also a Ruby port called Rubyful Soup .
This document illustrates all major features of Beautiful Soup
version 3.0, with examples.
It shows you what the library is good for,
how it works, how to use it, how to make it do what you want, and what
to do when it violates your expectations.
Table of Contents   Quick Start  Parsing a Document   Parsing HTML  Parsing XML  If That Doesn't Work   Beautiful Soup Gives You Unicode, Dammit  Printing a Document  The Parse Tree   The attributes of Tag s   Navigating the Parse Tree   parent  contents  string  nextSibling and previousSibling  next and previous  Iterating over a Tag  Using tag names as members   Searching the Parse Tree   The basic find method: findAll(name, attrs, recursive, text, limit, **kwargs)   Searching by CSS class  Calling a tag is like calling findall   find(name, attrs, recursive, text, **kwargs)  What happened to first ?
Searching Within the Parse Tree   findNextSiblings(name, attrs, text, limit, **kwargs) and findNextSibling(name, attrs, text, **kwargs)  findPreviousSiblings(name, attrs, text, limit, **kwargs) and findPreviousSibling(name, attrs, text, **kwargs)  findAllNext(name, attrs, text, limit, **kwargs) and findNext(name, attrs, text, **kwargs)  findAllPrevious(name, attrs, text, limit, **kwargs) and findPrevious(name, attrs, text, **kwargs)   Modifying the Parse Tree   Changing attribute values  Removing elements  Replacing one Element with Another  Adding a Brand New Element   Troubleshooting   Why can't Beautiful Soup print out the non-ASCII characters I gave it?
Beautiful Soup loses the data I fed it!
Why?
WHY?????
Beautiful Soup is too slow!
Advanced Topics   Generators  Other Built-In Parsers  Customizing the Parser  Entity Conversion  Sanitizing Bad Data with Regexps  Fun With SoupStrainer s  Improving Performance by Parsing Only Part of the Document  Improving Memory Usage with extract   See Also   Applications that use Beautiful Soup  Similar libraries   Conclusion   Quick Start  Get Beautiful Soup here .
The changelog describes differences between 3.0 and earlier versions.
Include Beautiful Soup in your application with a line like one of
the following:  from BeautifulSoup import BeautifulSoup          # For processing HTML  from BeautifulSoup import BeautifulStoneSoup     # For processing XML  import BeautifulSoup                             # To get everything   If you get the message "No module named BeautifulSoup", but you
know Beautiful Soup is installed, you're probably using the Beautiful
Soup 4 beta.
Use this code instead:  from bs4 import BeautifulSoup # To get everything   This document only covers Beautiful Soup 3.
Beautiful Soup 4 has some slight differences; see the README.txt file for details.
Here's some code demonstrating the basic features of Beautiful
Soup.
You can copy and paste this code into a Python session to run it
yourself.
from BeautifulSoup import BeautifulSoup  import re   doc = ['<html><head><title>Page title</title></head>',  '<body><p id="firstpara" align="center">This is paragraph <b>one</b>.
',  '<p id="secondpara" align="blah">This is paragraph <b>two</b>.
',  '</html>']  soup = BeautifulSoup(''.join(doc))   print soup.prettify() # <html> # <head> # <title> # Page title # </title> # </head> # <body> # <p id="firstpara" align="center"> # This is paragraph # <b> # one # </b> # .
# </p> # <p id="secondpara" align="blah"> # This is paragraph # <b> # two # </b> # .
# </p> # </body> # </html>   Here are some ways to navigate the soup:  soup.contents[0].name # u'html'   soup.contents[0].contents[0].name # u'head'   head = soup.contents[0].contents[0]  head.parent.name # u'html'   head.next # <title>Page title</title>   head.nextSibling.name # u'body'   head.nextSibling.contents[0] # <p id="firstpara" align="center">This is paragraph <b>one</b>.</p>   head.nextSibling.contents[0].nextSibling # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>   Here are a couple of ways to search the soup for certain tags, or
tags with certain properties:  titleTag = soup.html.head.title  titleTag # <title>Page title</title>   titleTag.string # u'Page title'   len(soup('p')) # 2   soup.findAll('p', align="center") # [<p id="firstpara" align="center">This is paragraph <b>one</b>.
</p>]   soup.find('p', align="center") # <p id="firstpara" align="center">This is paragraph <b>one</b>.
</p>   soup('p', align="center")[0]['id'] # u'firstpara'   soup.find('p', align=re.compile('^b.
*'))['id'] # u'secondpara'   soup.find('p').b.string # u'one'   soup('p')[1].b.string # u'two'   It's easy to modify the soup:  titleTag['id'] = 'theTitle'  titleTag.contents[0].replaceWith("New title")  soup.html.head # <head><title id="theTitle">New title</title></head>   soup.p.extract()  soup.prettify() # <html> # <head> # <title id="theTitle"> # New title # </title> # </head> # <body> # <p id="secondpara" align="blah"> # This is paragraph # <b> # two # </b> # .
# </p> # </body> # </html>   soup.p.replaceWith(soup.b) # <html> # <head> # <title id="theTitle"> # New title # </title> # </head> # <body> # <b> # two # </b> # </body> # </html>   soup.body.insert(0, "This page used to have ")  soup.body.insert(2, " &lt;p&gt; tags!")
soup.body # <body>This page used to have <b>two</b> &lt;p&gt; tags!</body>   Here's a real-world example.
It fetches the ICC Commercial
Crime Services weekly piracy report , parses it with Beautiful
Soup, and pulls out the piracy incidents:  import urllib2  from BeautifulSoup import BeautifulSoup   page = urllib2.urlopen("http://www.icc-ccs.org/prc/piracyreport.php")  soup = BeautifulSoup(page)  for incident in soup('td', width="90%"):  where, linebreak, what = incident.contents[:3]  print where.strip()  print what.strip()  print   Parsing a Document  A Beautiful Soup constructor takes an XML or HTML document in the
form of a string (or an open file-like object).
It parses the document
and creates a corresponding data structure in memory.
If you give Beautiful Soup a perfectly-formed document, the parsed
data structure looks just like the original document.
But if there's
something wrong with the document, Beautiful Soup uses heuristics to
figure out a reasonable structure for the data structure.
Parsing HTML  Use the BeautifulSoup class to parse an HTML
document.
Here are some of the things that BeautifulSoup knows:  Some tags can be nested (<BLOCKQUOTE>) and some can't (<P>).
Table and list tags have a natural nesting order.
For instance,
<TD> tags go inside <TR> tags, not the other way around.
The contents of a <SCRIPT> tag should not be parsed as HTML.
A <META> tag may specify an encoding for the document.
Here it is in action:  from BeautifulSoup import BeautifulSoup  html = "<html><p>Para 1<p>Para 2<blockquote>Quote 1<blockquote>Quote 2"  soup = BeautifulSoup(html)  print soup.prettify() # <html> # <p> # Para 1 # </p> # <p> # Para 2 # <blockquote> # Quote 1 # <blockquote> # Quote 2 # </blockquote> # </blockquote> # </p> # </html>   Note that BeautifulSoup figured out sensible places to put the
closing tags, even though the original document lacked them.
That document isn't valid HTML, but it's not too bad either.
Here's
a really horrible document.
Among other problems, it's got a <FORM>
tag that starts outside of a <TABLE> tag and ends inside the <TABLE>
tag.
(HTML like this was found on a website run by a major web
company.)
from BeautifulSoup import BeautifulSoup  html = """  <html>  <form>  <table>  <td><input name="input1">Row 1 cell 1  <tr><td>Row 2 cell 1  </form>  <td>Row 2 cell 2<br>This</br> sure is a long cell  </body>  </html>"""   Beautiful Soup handles this document as well:  print BeautifulSoup(html).prettify() # <html> # <form> # <table> # <td> # <input name="input1" /> # Row 1 cell 1 # </td> # <tr> # <td> # Row 2 cell 1 # </td> # </tr> # </table> # </form> # <td> # Row 2 cell 2 # <br /> # This # sure is a long cell # </td> # </html>   The last cell of the table is outside the <TABLE> tag; Beautiful
Soup decided to close the <TABLE> tag when it closed the <FORM>
tag.
The author of the original document probably intended the <FORM>
tag to extend to the end of the table, but Beautiful Soup has no way
of knowing that.
Even in a bizarre case like this, Beautiful Soup
parses the invalid document and gives you access to all the data.
Parsing XML  The BeautifulSoup class is full of web-browser-like
heuristics for divining the intent of HTML authors.
But XML doesn't
have a fixed tag set, so those heuristics don't apply.
So BeautifulSoup doesn't do XML very well.
Use the BeautifulStoneSoup class to parse XML
documents.
It's a general class with no special knowledge of any XML
dialect and very simple rules about tag nesting: Here it is in action:  from BeautifulSoup import BeautifulStoneSoup  xml = "<doc><tag1>Contents 1<tag2>Contents 2<tag1>Contents 3"  soup = BeautifulStoneSoup(xml)  print soup.prettify() # <doc> # <tag1> # Contents 1 # <tag2> # Contents 2 # </tag2> # </tag1> # <tag1> # Contents 3 # </tag1> # </doc>   The most common shortcoming of BeautifulStoneSoup is that it doesn't know about
self-closing tags.
HTML has a fixed set of self-closing tags, but with
XML it depends on what the DTD says.
You can tell BeautifulStoneSoup that certain tags are self-closing by
passing in their names as the selfClosingTags argument to
the constructor:  from BeautifulSoup import BeautifulStoneSoup  xml = "<tag>Text 1<selfclosing>Text 2"  print BeautifulStoneSoup(xml).prettify() # <tag> # Text 1 # <selfclosing> # Text 2 # </selfclosing> # </tag>   print BeautifulStoneSoup(xml, selfClosingTags=['selfclosing']).prettify() # <tag> # Text 1 # <selfclosing /> # Text 2 # </tag>   If That Doesn't Work  There are several other parser classes with different heuristics from these two.
You can also subclass and customize a parser and
give it your own heuristics.
Beautiful Soup Gives You Unicode, Dammit  By the time your document is parsed, it has been transformed into
Unicode.
Beautiful Soup stores only Unicode strings in its data
structures.
from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("Hello")  soup.contents[0] # u'Hello'  soup.originalEncoding # 'ascii'   Here's an example with a Japanese document encoded in UTF-8:  from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("\xe3\x81\x93\xe3\x82\x8c\xe3\x81\xaf")  soup.contents[0] # u'\u3053\u308c\u306f'  soup.originalEncoding # 'utf-8'   str(soup) # '\xe3\x81\x93\xe3\x82\x8c\xe3\x81\xaf'  # Note: this bit uses EUC-JP, so it only works if you have cjkcodecs # installed, or are running Python 2.4.  soup.__str__('euc-jp') # '\xa4\xb3\xa4\xec\xa4\xcf'   Beautiful Soup uses a class called UnicodeDammit to
detect the encodings of documents you give it and convert them to
Unicode, no matter what.
If you need to do this for other documents
(without using Beautiful Soup to parse them), you can use UnicodeDammit by itself.
It's heavily based on code from
the Universal Feed Parser .
If you're running an older version of Python than 2.4, be sure to
download and install cjkcodecs and iconvcodec , which make Python capable of supporting
more codecs, especially CJK codecs.
Also install the chardet library, for better autodetection.
Beautiful Soup tries the following encodings, in order of priority,
to turn your document into Unicode:  An encoding you pass in as the fromEncoding argument
to the soup constructor.
An encoding discovered in the document itself: for instance, in an
XML declaration or (for HTML documents) an http-equiv META tag.
If Beautiful Soup finds this kind of encoding within the
document, it parses the document again from the beginning and gives
the new encoding a try.
The only exception is if you explicitly
specified an encoding, and that encoding actually worked: then it will
ignore any encoding it finds in the document.
An encoding sniffed by looking at the first few bytes of the
file.
If an encoding is detected at this stage, it will be one of the
UTF-* encodings, EBCDIC, or ASCII.
An encoding sniffed by the chardet library, if you have it installed.
UTF-8 Windows-1252  Beautiful Soup will almost always guess right if it can make a
guess at all.
But for documents with no declarations and in strange
encodings, it will often not be able to guess.
It will fall back to
Windows-1252, which will probably be wrong.
Here's an EUC-JP example
where Beautiful Soup guesses the encoding wrong.
(Again, because it
uses EUC-JP, this example will only work if you are running Python 2.4
or have cjkcodecs installed):  from BeautifulSoup import BeautifulSoup  euc_jp = '\xa4\xb3\xa4\xec\xa4\xcf'   soup = BeautifulSoup(euc_jp)  soup.originalEncoding # 'windows-1252'   str(soup) # '\xc2\xa4\xc2\xb3\xc2\xa4\xc3\xac\xc2\xa4\xc3\x8f'     # Wrong!
But if you specify the encoding with fromEncoding , it
parses the document correctly, and can convert it to UTF-8 or back to
EUC-JP.
soup = BeautifulSoup(euc_jp, fromEncoding="euc-jp")  soup.originalEncoding # 'windows-1252'   str(soup) # '\xe3\x81\x93\xe3\x82\x8c\xe3\x81\xaf'                 # Right!
soup.__str__(self, 'euc-jp') == euc_jp # True   If you give Beautiful Soup a document in the Windows-1252 encoding
(or a similar encoding like ISO-8859-1 or ISO-8859-2), Beautiful Soup
finds and destroys the document's smart quotes and other
Windows-specific characters.
Rather than transforming those characters
into their Unicode equivalents, Beautiful Soup transforms them into
HTML entities ( BeautifulSoup ) or XML entities
( BeautifulStoneSoup ).
To prevent this, you can pass smartQuotesTo=None into the soup
constructor: then smart quotes will be converted to Unicode like any
other native-encoding characters.
You can also pass in "xml" or "html"
for smartQuotesTo , to change the default behavior of BeautifulSoup and BeautifulStoneSoup .
from BeautifulSoup import BeautifulSoup, BeautifulStoneSoup  text = "Deploy the \x91SMART QUOTES\x92!"
str(BeautifulSoup(text)) # 'Deploy the &lsquo;SMART QUOTES&rsquo;!'
str(BeautifulStoneSoup(text)) # 'Deploy the &#x2018;SMART QUOTES&#x2019;!'
str(BeautifulSoup(text, smartQuotesTo="xml")) # 'Deploy the &#x2018;SMART QUOTES&#x2019;!'
BeautifulSoup(text, smartQuotesTo=None).contents[0] # u'Deploy the \u2018SMART QUOTES\u2019!'
Printing a Document  You can turn a Beautiful Soup document (or any subset of it) into a
string with the str function, or the prettify or renderContents methods.
You can also use the unicode function to get the whole
document as a Unicode string.
The prettify method adds strategic newlines and spacing to make
the structure of the document obvious.
It also strips out text nodes
that contain only whitespace, which might change the meaning of an XML
document.
The str and unicode functions don't strip out text nodes
that contain only whitespace, and they don't add any whitespace
between nodes either.
Here's an example.
from BeautifulSoup import BeautifulSoup  doc = "<html><h1>Heading</h1><p>Text"  soup = BeautifulSoup(doc)   str(soup) # '<html><h1>Heading</h1><p>Text</p></html>'  soup.renderContents() # '<html><h1>Heading</h1><p>Text</p></html>'  soup.__str__() # '<html><h1>Heading</h1><p>Text</p></html>'  unicode(soup) # u'<html><h1>Heading</h1><p>Text</p></html>'   soup.prettify() # '<html>\n <h1>\n  Heading\n </h1>\n <p>\n  Text\n </p>\n</html>'   print soup.prettify() # <html> # <h1> # Heading # </h1> # <p> # Text # </p> # </html>   Note that str and renderContents give
different results when used on a tag within the document.
str prints a tag and its contents, and renderContents only prints the contents.
heading = soup.h1  str(heading) # '<h1>Heading</h1>'  heading.renderContents() # 'Heading'   When you call __str__ , prettify , or renderContents , you can specify an output encoding.
The
default encoding (the one used by str ) is UTF-8.
Here's
an example that parses an ISO-8851-1 string and then outputs the same
string in different encodings:  from BeautifulSoup import BeautifulSoup  doc = "Sacr\xe9 bleu!"
soup = BeautifulSoup(doc)  str(soup) # 'Sacr\xc3\xa9 bleu!'
# UTF-8  soup.__str__("ISO-8859-1") # 'Sacr\xe9 bleu!'
soup.__str__("UTF-16") # '\xff\xfeS\x00a\x00c\x00r\x00\xe9\x00 \x00b\x00l\x00e\x00u\x00!\x00'  soup.__str__("EUC-JP") # 'Sacr\x8f\xab\xb1 bleu!'
If the original document contained an encoding declaration, then
Beautiful Soup rewrites the declaration to mention the new encoding
when it converts the document back to a string.
This means that if you
load an HTML document into BeautifulSoup and print it
back out, not only should the HTML be cleaned up, but it should be
transparently converted to UTF-8.
Here's an HTML example:  from BeautifulSoup import BeautifulSoup  doc = """<html>  <meta http-equiv="Content-type" content="text/html; charset=ISO-Latin-1" >  Sacr\xe9 bleu!
</html>"""   print BeautifulSoup(doc).prettify() # <html> # <meta http-equiv="Content-type" content="text/html; charset=utf-8" /> # Sacré bleu!
# </html>   Here's an XML example:  from BeautifulSoup import BeautifulStoneSoup  doc = """<?xml version="1.0" encoding="ISO-Latin-1">Sacr\xe9 bleu!"""
print BeautifulStoneSoup(doc).prettify() # <?xml version='1.0' encoding='utf-8'> # Sacré bleu!
The Parse Tree  So far we've focused on loading documents and writing them back
out.
Most of the time, though, you're interested in the parse tree:
the data structure Beautiful Soup builds as it parses the document.
A parser object (an instance of BeautifulSoup or BeautifulStoneSoup ) is a deeply-nested, well-connected data
structure that corresponds to the structure of an XML or HTML
document.
The parser object contains two other types of objects: Tag objects, which correspond to tags like the <TITLE> tag and the <B>
tags; and NavigableString objects, which correspond to strings like
"Page title" and "This is paragraph".
There are also some subclasses of NavigableString ( CData , Comment , Declaration , and ProcessingInstruction ), which
correspond to special XML constructs.
They act like NavigableString s, except that when it's time to print them out they
have some extra data attached to them.
Here's a document that includes
a comment:  from BeautifulSoup import BeautifulSoup  import re  hello = "Hello!
<!--I've got to be nice to get what I want.-->"  commentSoup = BeautifulSoup(hello)  comment = commentSoup.find(text=re.compile("nice"))   comment.__class__ # <class 'BeautifulSoup.Comment'>  comment # u"I've got to be nice to get what I want."
comment.previousSibling # u'Hello! '
str(comment) # "<!--I've got to be nice to get what I want.-->"  print commentSoup # Hello!
<!--I've got to be nice to get what I want.-->   Now, let's take a closer look at
the document used at the beginning of the documentation:  from BeautifulSoup import BeautifulSoup  doc = ['<html><head><title>Page title</title></head>',  '<body><p id="firstpara" align="center">This is paragraph <b>one</b>.
# </p> # </body> # </html>   The attributes of Tag s  Tag and NavigableString objects have lots of useful members,
most of which are covered in Navigating the Parse Tree and Searching the Parse Tree .
However, there's one aspect of Tag objects we'll cover here: the
attributes.
SGML tags have attributes:.
for instance, each of the <P> tags in the example HTML above has an "id"
attribute and an "align" attribute.
You can access a tag's attributes
by treating the Tag object as though it were a dictionary:  firstPTag, secondPTag = soup.findAll('p')   firstPTag['id'] # u'firstPara'   secondPTag['id'] # u'secondPara'   NavigableString objects don't have attributes; only Tag objects
have them.
Navigating the Parse Tree  All Tag objects have all of the members listed below (though the
actual value of the member may be None ).
NavigableString objects
have all of them except for contents and string .
parent  In the example above , the parent
of the <HEAD> Tag is the <HTML> Tag .
The parent of the <HTML> Tag is the BeautifulSoup parser object itself.
The parent of the
parser object is None .
By following parent , you can move up the
parse tree:  soup.head.parent.name # u'html'  soup.head.parent.parent.__class__.__name__ # 'BeautifulSoup'  soup.parent == None # True   contents  With parent you move up the parse tree.
With contents you move
down the tree.
contents is an ordered list of the Tag and NavigableString objects contained within a page element.
Only the
top-level parser object and Tag objects have contents .
NavigableString objects are just strings and can't
contain sub-elements, so they don't have contents .
In the example above , the contents of the first <P> Tag is a list containing a NavigableString ("This is paragraph "), a <B> Tag , and another NavigableString (".").
The contents of the <B> Tag : a list
containing a NavigableString ("one").
pTag = soup.p  pTag.contents # [u'This is paragraph ', <b>one</b>, u'.']
pTag.contents[1].contents # [u'one']  pTag.contents[0].contents # AttributeError: 'NavigableString' object has no attribute 'contents'   string  For your convenience, if a tag has only one child node, and that
child node is a string, the child node is made available as tag.string , as well as tag.contents[0] .
In the example above , soup.b.string is a NavigableString representing the Unicode string
"one".
That's the string contained in the first <B> Tag in the parse
tree.
soup.b.string # u'one'  soup.b.contents[0] # u'one'   But soup.p.string is None , because the first <P> Tag in the
parse tree has more than one child.
soup.head.string is also None ,
even though the <HEAD> Tag has only one child, because that child is a Tag (the <TITLE> Tag ), not a NavigableString .
soup.p.string == None # True  soup.head.string == None # True   nextSibling and previousSibling  These members let you skip to the next or previous thing on the
same level of the parse tree.
In the
document above , the nextSibling of the <HEAD> Tag is the
<BODY> Tag , because the <BODY> Tag is the next thing directly
beneath the <html> Tag .
The nextSibling of the <BODY> tag is None , because there's nothing else directly beneath the <HTML> Tag .
soup.head.nextSibling.name # u'body'  soup.html.nextSibling == None # True   Conversely, the previousSibling of the <BODY> Tag is the <HEAD>
tag, and the previousSibling of the <HEAD> Tag is None :  soup.body.previousSibling.name # u'head'  soup.head.previousSibling == None # True   Some more examples: the nextSibling of the first <P> Tag is the
second <P> Tag .
The previousSibling of the <B> Tag inside the
second <P> Tag is the NavigableString "This is paragraph".
The previousSibling of that NavigableString is None , not anything
inside the first <P> Tag .
soup.p.nextSibling # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>   secondBTag = soup.findAll('b')[1]  secondBTag.previousSibling # u'This is paragraph'  secondBTag.previousSibling.previousSibling == None # True   next and previous  These members let you move through the document elements in the
order they were processed by the parser, rather than in the order they
appear in the tree.
For instance, the next of the <HEAD> Tag is
the <TITLE> Tag , not the <BODY> Tag .
This is because, in the original document , the <TITLE>
tag comes immediately after the <HEAD> tag.
soup.head.next # u'title'  soup.head.nextSibling.name # u'body'  soup.head.previous.name # u'html'   Where next and previous are concerned, a Tag 's contents come
before its nextSibling .
You usually won't have to use these members,
but sometimes it's the easiest way to get to something buried inside
the parse tree.
Iterating over a Tag  You can iterate over the contents of a Tag by treating it as a
list.
This is a useful shortcut.
Similarly, to see how many child
nodes a Tag has, you can call len(tag) instead of len(tag.contents) .
In terms of the
document above :  for i in soup.body:  print i # <p id="firstpara" align="center">This is paragraph <b>one</b>.</p> # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>   len(soup.body) # 2  len(soup.body.contents) # 2   Using tag names as members  It's easy to navigate the parse tree by acting as though the name
of the tag you want is a member of a parser or Tag object.
We've
been doing it throughout these examples.
In terms of the document above , soup.head gives us the first
(and, as it happens, only) <HEAD> Tag in the document:  soup.head # <head><title>Page title</title></head>   In general, calling mytag.foo returns the first child of mytag that happens to be a <FOO> Tag .
If there aren't any <FOO> Tag s
beneath mytag , then mytag.foo returns None .
You can use this to traverse the parse tree very quickly:  soup.head.title # <title>Page title</title>   soup.body.p.b.string # u'one'   You can also use this to quickly jump to a certain part of a parse
tree.
For instance, if you're not worried about <TITLE> tags in weird
places outside of the <HEAD> tag, you can just use soup.title to get
an HTML document's title.
You don't have to use soup.head.title :  soup.title.string # u'Page title'   soup.p jumps to the first <P> tag inside a document, wherever it
is.
soup.table.tr.td jumps to the first column of the first row of
the first table in the document.
These members actually alias to the first method, covered below .
I mention it here because
the alias makes it very easy to zoom in on an interesting part of a
well-known parse tree.
An alternate form of this idiom lets you access the first <FOO> tag
as .fooTag instead of .foo .
For instance, soup.table.tr.td could
also be expressed as soup.tableTag.trTag.tdTag , or even soup.tableTag.tr.tdTag .
This is useful if you like to be more
explicit about what you're doing, or if you're parsing XML whose tag
names conflict with the names of Beautiful Soup methods and members.
from BeautifulSoup import BeautifulStoneSoup  xml = '<person name="Bob"><parent rel="mother" name="Alice">'  xmlSoup = BeautifulStoneSoup(xml)   xmlSoup.person.parent                      # A Beautiful Soup member # <person name="Bob"><parent rel="mother" name="Alice"></parent></person>  xmlSoup.person.parentTag                   # A tag name # <parent rel="mother" name="Alice"></parent>   If you're looking for tag names that aren't valid Python
identifiers (like hyphenated-name ), you need to use find .
Searching the Parse Tree  Beautiful Soup provides many methods that traverse the parse tree,
gathering Tag s and NavigableString s that match criteria you
specify.
There are several ways to define
criteria for matching Beautiful Soup objects.
Let's demonstrate by
examining in depth the most basic of all Beautiful Soup search
methods, findAll .
As before, we'll demonstrate on the following
document:   from BeautifulSoup import BeautifulSoup  doc = ['<html><head><title>Page title</title></head>',  '<body><p id="firstpara" align="center">This is paragraph <b>one</b>.
',  '</html>']  soup = BeautifulSoup(''.join(doc))  print soup.prettify() # <html> # <head> # <title> # Page title # </title> # </head> # <body> # <p id="firstpara" align="center"> # This is paragraph # <b> # one # </b> # .
# </p> # </body> # </html>   Incidentally, the two methods described in this section ( findAll and find ) are available only to Tag objects and the top-level
parser objects, not to NavigableString objects.
The methods defined
in Searching Within the
Parse Tree are also available to NavigableString objects.
The basic find method: findAll( name , attrs , recursive , text , limit , **kwargs )  The findAll method traverses the tree, starting at the given
point, and finds all the Tag and NavigableString objects that match
the criteria you give.
The signature for the findall method is this: findAll(name=None, attrs={}, recursive=True, text=None,
limit=None, **kwargs)  These arguments show up
over and over again throughout the Beautiful Soup API.
The most
important arguments are name and the keyword arguments.
The name argument restricts the set
of tags by name.
There are several ways to restrict the name, and
these too show up over and over again throughout the Beautiful Soup
API.
The simplest usage is to just pass in a tag name.
This code finds
all the <B> Tag s in the document:  soup.findAll('b') # [<b>one</b>, <b>two</b>]   You can also pass in a regular expression.
This code finds all the
tags whose names start with B:  import re  tagsStartingWithB = soup.findAll(re.compile('^b'))  [tag.name for tag in tagsStartingWithB] # [u'body', u'b', u'b']   You can pass in a list or a dictionary.
These two calls find all
the <TITLE> and all the <P> tags.
They work the same way, but the
second call runs faster:  soup.findAll(['title', 'p']) # [<title>Page title</title>, # <p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   soup.findAll({'title' : True, 'p' : True}) # [<title>Page title</title>, # <p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   You can pass in the special value True , which matches every tag
with a name: that is, it matches every tag.
allTags = soup.findAll(True)  [tag.name for tag in allTags]  [u'html', u'head', u'title', u'body', u'p', u'b', u'p', u'b']   This doesn't look useful, but True is very useful when
restricting attribute values.
You can pass in a callable object which
takes a Tag object as its only argument, and returns a
boolean.
Every Tag object that findAll encounters will be passed
into this object, and if the call returns True then the tag is
considered to match.
This code finds the tags that have two, and only two, attributes:  soup.findAll(lambda tag: len(tag.attrs) == 2) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   This code finds the tags that have one-character names and no
attributes:  soup.findAll(lambda tag: len(tag.name) == 1 and not tag.attrs) # [<b>one</b>, <b>two</b>]    The keyword arguments impose
restrictions on the attributes of a tag.
This simple example finds all
the tags which have a value of "center" for their "align" attribute:  soup.findAll(align="center") # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>]   As with the name argument, you can pass a keyword argument
different kinds of object to impose different restrictions on the
corresponding attribute.
You can pass a string, as seen above, to
restrict an attribute to a single value.
You can also pass a regular
expression, a list, a hash, the special values True or None , or a
callable that takes the attribute value as its argument (note that the
value may be None ).
Some examples:  soup.findAll(id=re.compile("para$")) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   soup.findAll(align=["center", "blah"]) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   soup.findAll(align=lambda(value): value and len(value) < 5) # [<p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   The special values True and None are of special
interest.
True matches a tag that has any value for the given
attribute, and None matches a tag that has no value for the
given attribute.
Some examples:  soup.findAll(align=True) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   [tag.name for tag in soup.findAll(align=None)] # [u'html', u'head', u'title', u'body', u'b', u'b']   If you need to impose complex or interlocking restrictions on a
tag's attributes, pass in a callable object for name , as seen above , and deal with the Tag object.
You might have noticed a problem here.
What
if you have a document with a tag that defines an attribute called name ?
You can't use a keyword argument called name because the
Beautiful Soup search methods already define a name argument.
You
also can't use a Python reserved word like for as a keyword
argument.
Beautiful Soup provides a special argument called attrs which you
can use in these situations.
attrs is a dictionary that acts just
like the keyword arguments:  soup.findAll(id=re.compile("para$")) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   soup.findAll(attrs={'id' : re.compile("para$")}) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]   You can use attrs if you need to put restrictions on attributes
whose names are Python reserved words, like class , for , or import ; or attributes whose names are non-keyword arguments to the
Beautiful Soup search methods: name , recursive , limit , text ,
or attrs itself.
from BeautifulSoup import BeautifulStoneSoup  xml = '<person name="Bob"><parent rel="mother" name="Alice">'  xmlSoup = BeautifulStoneSoup(xml)   xmlSoup.findAll(name="Alice") # []   xmlSoup.findAll(attrs={"name" : "Alice"}) # [parent rel="mother" name="Alice"></parent>]   Searching by CSS class  The attrs argument would be a pretty obscure feature
were it not for one thing: CSS.
It's very useful to search for a tag
that has a certain CSS class, but the name of the CSS attribute, class , is also a Python reserved word.
You could search by CSS class with soup.find("tagName", {
"class" : "cssClass" }) , but that's a lot of code for such a
common operation.
Instead, you can pass a string for attrs instead
of a dictionary.
The string will be used to restrict the CSS class.
from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("""Bob's <b>Bold</b> Barbeque Sauce now available in  <b class="hickory">Hickory</b> and <b class="lime">Lime</a>""")   soup.find("b", { "class" : "lime" }) # <b class="lime">Lime</b>   soup.find("b", "hickory") # <b class="hickory">Hickory</b>   text is an argument that lets
you search for NavigableString objects instead of Tag s.
Its value
can be a string, a regular expression, a list or dictionary, True or None , or a callable that takes a NavigableString object as its
argument:  soup.findAll(text="one") # [u'one']  soup.findAll(text=u'one') # [u'one']   soup.findAll(text=["one", "two"]) # [u'one', u'two']   soup.findAll(text=re.compile("paragraph")) # [u'This is paragraph ', u'This is paragraph ']   soup.findAll(text=True) # [u'Page title', u'This is paragraph ', u'one', u'.
', u'This is paragraph ', # u'two', u'.']
soup.findAll(text=lambda(x): len(x) < 12) # [u'Page title', u'one', u'.
', u'two', u'.']
If you use text , then any values you give for name and the
keyword arguments are ignored.
recursive is a boolean
argument (defaulting to True ) which tells Beautiful Soup whether to
go all the way down the parse tree, or whether to only look at the
immediate children of the Tag or the parser object.
Here's the
difference:  [tag.name for tag in soup.html.findAll()] # [u'head', u'title', u'body', u'p', u'b', u'p', u'b']   [tag.name for tag in soup.html.findAll(recursive=False)] # [u'head', u'body']   When recursive is false, only the immediate children of the
<HTML> tag are searched.
If you know that's all you need to search,
you can save some time this way.
Setting limit argument lets you
stop the search once Beautiful Soup finds a certain number of matches.
If there are a thousand tables in your document, but you only need the
fourth one, pass in 4 to limit and you'll save time.
By default,
there is no limit.
soup.findAll('p', limit=1) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>]   soup.findAll('p', limit=100) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>, # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>]    Calling a tag is like calling findall  A little shortcut for you.
If you call the parser object or a Tag like a function, then you can pass in all of findall 's arguments and
it's the same as calling findall .
In terms of the document above :  soup(text=lambda(x): len(x) < 12) # [u'Page title', u'one', u'.
soup.body('p', limit=1) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>]   find( name , attrs , recursive , text , **kwargs )  Okay, now let's look at the other search methods.
They all take
pretty much the same arguments as findAll .
The find method is almost exactly like findAll , except that
instead of finding all the matching objects, it only finds the first
one.
It's like imposing a limit of 1 on the result set, and then
extracting the single result from the array.
In terms of the document above :  soup.findAll('p', limit=1) # [<p id="firstpara" align="center">This is paragraph <b>one</b>.</p>]   soup.find('p', limit=1) # <p id="firstpara" align="center">This is paragraph <b>one</b>.</p>   soup.find('nosuchtag', limit=1) == None # True   In general, when you see a search method with a plural name (like findAll or findNextSiblings ), that method takes a limit argument
and returns a list of results.
When you see a search method that
doesn't have a plural name (like find or findNextSibling ), you
know that the method doesn't take a limit and returns a single
result.
What happened to first ?
Previous versions of Beautiful Soup had methods like first , fetch , and fetchPrevious .
These methods are sitll there, but
they're deprecated, and may go away soon.
The total effect of all
those names was very confusing.
The new names are named consistently:
as mentioned above, if the method name is plural or refers to All ,
it returns multiple objects.
Otherwise, it returns one object.
Searching Within the Parse Tree  The methods described above, findAll and find , start at a
certain point in the parse tree and go down.
They recursively iterate
through an object's contents until they bottom out.
This means that you can't call these methods on NavigableString objects, because they have no contents : they're always the leaves of
the parse tree.
But downwards isn't the only way you can iterate through a
document.
Back in Navigating the
Parse Tree I showed you many other ways: parent , nextSibling ,
and so on.
Each of these iteration techniques has two corresponding
methods: one that works like findAll , and one that works like find .
And since NavigableString objects do support these
operations, you can call these methods on them as well as on Tag objects and the main parser object.
Why is this useful?
Well, sometimes you just can't use findAll or find to get to the Tag or NavigableString you want.
For
instance, consider some HTML like this:  from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup('''<ul>  <li>An unrelated list  </ul>   <h1>Heading</h1>  <p>This is <b>the list you want</b>:</p>  <ul><li>The data you want</ul>''')   There are a number of ways to navigate to the <LI> tag that contains
the data you want.
The most obvious is this:  soup('li', limit=2)[1] # <li>The data you want</li>   It should be equally obvious that that's not a very stable way to get
that <LI> tag.
If you're only scraping this page once it doesn't
matter, but if you're going to scrape it many times over a long
period, such considerations become important.
If the irrelevant list
grows another <LI> tag, you'll get that tag instead of the one you
want, and your script will break or give the wrong data.
soup('ul', limit=2)[1].li # <li>The data you want</li>   That's is a little better, because it can survive changes to the
irrelevant list.
But if the document grows another irrelevant list at
the top, you'll get the first <LI> tag of that list instead of the one
you want.
A more reliable way of referring to the ul tag you want
would better reflect that tag's place in the structure of the
document.
When you look at that HTML, you might think of the list you want as
'the <UL> tag beneath the <H1> tag'.
The problem is that the tag isn't
contained inside the <H1> tag; it just happens to comes after it.
It's
easy enough to get the <H1> tag, but there's no way to get from there
to the <UL> tag using first and fetch , because those methods only
search the contents of the <H1> tag.
You need to navigate to the
<UL> tag with the next or nextSibling members:  s = soup.h1  while getattr(s, 'name', None) != 'ul':  s = s.nextSibling  s.li # <li>The data you want</li>   Or, if you think this might be more stable:  s = soup.find(text='Heading')  while getattr(s, 'name', None) != 'ul':  s = s.next  s.li # <li>The data you want</li>   But that's more trouble than you should need to go through.
The
methods in this section provide a useful shorthand.
They can be used
whenever you find yourself wanting to write a while loop over one of
the navigation members.
Given a starting point somewhere in the tree,
they navigate the tree in some way and keep track of Tag or NavigableString objects that match the criteria you specify.
Instead of
the first loop in the example code above, you can just write this:  soup.h1.findNextSibling('ul').li # <li>The data you want</li>   Instead of the second loop, you can write this:  soup.find(text='Heading').findNext('ul').li # <li>The data you want</li>   The loops are replaced with calls to findNextSibling and findNext .
The rest of this section is a reference to all the methods
of this kind.
Again, there are two methods for every navigation
member: one that returns a list the way findAll does, and one that
returns a scalar the way find does.
One last time, let's load up
the familiar soup document for example's sake:   from BeautifulSoup import BeautifulSoup  doc = ['<html><head><title>Page title</title></head>',  '<body><p id="firstpara" align="center">This is paragraph <b>one</b>.
# </p> # </body> # </html>   findNextSiblings( name , attrs , text , limit , **kwargs ) and findNextSibling( name , attrs , text , **kwargs )  These methods repeatedly follow an object's nextSibling member,
gathering Tag or NavigableText objects that match the criteria you
specify.
In terms of the document
above :  paraText = soup.find(text='This is paragraph ')  paraText.findNextSiblings('b') # [<b>one</b>]   paraText.findNextSibling(text = lambda(text): len(text) == 1) # u'.'
findPreviousSiblings( name , attrs , text , limit , **kwargs ) and findPreviousSibling( name , attrs , text , **kwargs )  These methods repeatedly follow an object's previousSibling member,
gathering Tag or NavigableText objects that match the criteria you
specify.
In terms of the document
above :  paraText = soup.find(text='.')
paraText.findPreviousSiblings('b') # [<b>one</b>]   paraText.findPreviousSibling(text = True) # u'This is paragraph '   findAllNext( name , attrs , text , limit , **kwargs ) and findNext( name , attrs , text , **kwargs )  These methods repeatedly follow an object's next member,
gathering Tag or NavigableText objects that match the criteria you
specify.
In terms of the document
above :  pTag = soup.find('p')  pTag.findAllNext(text=True) # [u'This is paragraph ', u'one', u'.
', u'This is paragraph ', u'two', u'.']
pTag.findNext('p') # <p id="secondpara" align="blah">This is paragraph <b>two</b>.</p>   pTag.findNext('b') # <b>one</b>   findAllPrevious( name , attrs , text , limit , **kwargs ) and findPrevious( name , attrs , text , **kwargs )  These methods repeatedly follow an object's previous member,
gathering Tag or NavigableText objects that match the criteria you
specify.
In terms of the document
above :  lastPTag = soup('p')[-1]  lastPTag.findAllPrevious(text=True) # [u'.
', u'one', u'This is paragraph ', u'Page title'] # Note the reverse order!
lastPTag.findPrevious('p') # <p id="firstpara" align="center">This is paragraph <b>one</b>.</p>   lastPTag.findPrevious('b') # <b>one</b>   findParents( name , attrs , limit , **kwargs ) and findParent( name , attrs , **kwargs )  These methods repeatedly follow an object's parent member,
gathering Tag or NavigableText objects that match the criteria you
specify.
They don't take a text argument, because there's no way any
object can have a NavigableString for a parent.
In terms of the document above :  bTag = soup.find('b')   [tag.name for tag in bTag.findParents()] # [u'p', u'body', u'html', '[document]'] # NOTE: "u'[document]'" means that that the parser object itself matched.
bTag.findParent('body').name # u'body'   Modifying the Parse Tree  Now you know how to find things in the parse tree.
But maybe you
want to modify it and print it back out.
You can just rip an element
out of its parent's contents , but the rest of the document will
still have references to the thing you ripped out.
Beautiful Soup
offers several methods that let you modify the parse tree while
maintaining its internal consistency.
Changing attribute values  You can use dictionary assignment to modify the attribute values of Tag objects.
from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("<b id="2">Argh!</b>")  print soup # <b id="2">Argh!</b>  b = soup.b   b['id'] = 10  print soup # <b id="10">Argh!</b>   b['id'] = "ten"  print soup # <b id="ten">Argh!</b>   b['id'] = 'one "million"'  print soup # <b id='one "million"'>Argh!</b>   You can also delete attribute values, and add new ones:   del(b['id'])  print soup # <b>Argh!</b>   b['class'] = "extra bold and brassy!"
print soup # <b class="extra bold and brassy!
">Argh!</b>   Removing elements  Once you have a reference to an element, you can rip it out of the
tree with the extract method.
This code removes all the comments
from a document:  from BeautifulSoup import BeautifulSoup, Comment  soup = BeautifulSoup("""1<!--The loneliest number-->  <a>2<!--Can be as bad as one--><b>3""")  comments = soup.findAll(text=lambda text:isinstance(text, Comment))  [comment.extract() for comment in comments]  print soup # 1 # <a>2<b>3</b></a>   This code removes a whole subtree from a document:  from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("<a1></a1><a><b>Amazing content<c><d></a><a2></a2>")  soup.a1.nextSibling # <a><b>Amazing content<c><d></d></c></b></a>  soup.a2.previousSibling # <a><b>Amazing content<c><d></d></c></b></a>   subtree = soup.a  subtree.extract()   print soup # <a1></a1><a2></a2>  soup.a1.nextSibling # <a2></a2>  soup.a2.previousSibling # <a1></a1>   The extract method turns one parse tree into two disjoint
trees.
The navigation members are changed so that it looks like the
trees had never been together:  soup.a1.nextSibling # <a2></a2>  soup.a2.previousSibling # <a1></a1>  subtree.previousSibling == None # True  subtree.parent == None # True   Replacing one Element with Another  The replaceWith method extracts one page element and replaces it
with a different one.
The new element can be a Tag (possibly with a
whole parse tree beneath it) or a NavigableString .
If you pass a
plain old string into replaceWith , it gets turned into a NavigableString .
The navigation members are changed as though the
document had been parsed that way in the first place.
Here's a simple example:  from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("<b>Argh!</b>")  soup.find(text="Argh!").replaceWith("Hooray!")
print soup # <b>Hooray!</b>   newText = soup.find(text="Hooray!")
newText.previous # <b>Hooray!</b>  newText.previous.next # u'Hooray!'
newText.parent # <b>Hooray!</b>  soup.b.contents # [u'Hooray!']
Here's a more complex example that replaces one tag with another:  from BeautifulSoup import BeautifulSoup, Tag  soup = BeautifulSoup("<b>Argh!<a>Foo</a></b><i>Blah!</i>")  tag = Tag(soup, "newTag", [("id", 1)])  tag.insert(0, "Hooray!")
soup.a.replaceWith(tag)  print soup # <b>Argh!<newTag id="1">Hooray!</newTag></b><i>Blah!</i>   You can even rip out an element from one part of the document and
stick it in another part:  from BeautifulSoup import BeautifulSoup  text = "<html>There's <b>no</b> business like <b>show</b> business</html>"  soup = BeautifulSoup(text)   no, show = soup.findAll('b')  show.replaceWith(no)  print soup # <html>There's  business like <b>no</b> business</html>   Adding a Brand New Element  The Tag class and the parser classes support a method called insert .
It works just like a Python list's insert method: it takes
an index to the tag's contents member, and sticks a new element in
that slot.
This was demonstrated in the previous section, when we replaced a
tag in the document with a brand new tag.
You can use insert to
build up an entire parse tree from scratch:  from BeautifulSoup import BeautifulSoup, Tag, NavigableString  soup = BeautifulSoup()  tag1 = Tag(soup, "mytag")  tag2 = Tag(soup, "myOtherTag")  tag3 = Tag(soup, "myThirdTag")  soup.insert(0, tag1)  tag1.insert(0, tag2)  tag1.insert(1, tag3)  print soup # <mytag><myOtherTag></myOtherTag><myThirdTag></myThirdTag></mytag>   text = NavigableString("Hello!")
tag3.insert(0, text)  print soup # <mytag><myOtherTag></myOtherTag><myThirdTag>Hello!</myThirdTag></mytag>   An element can occur in only one place in one parse tree.
If you
give insert an element that's already connected to a soup object, it
gets disconnected (with extract ) before it gets connected
elsewhere.
In this example, I try to insert my NavigableString into
a second part of the soup, but it doesn't get inserted again.
It gets
moved:  tag2.insert(0, text)  print soup # <mytag><myOtherTag>Hello!</myOtherTag><myThirdTag></myThirdTag></mytag>   This happens even if the element previously belonged to a
completely different soup object.
An element can only have one parent , one nextSibling , et cetera, so it can only be in one place
at a time.
Troubleshooting  This section covers common problems people have with Beautiful Soup.
Why can't Beautiful Soup print out the non-ASCII characters I gave it?
If you're getting errors that say: "'ascii' codec can't encode character 'x' in position y: ordinal not in range(128)" , 
the problem is probably with your Python installation rather than with
Beautiful Soup.
Try printing out the non-ASCII characters without
running them through Beautiful Soup and you should have the same
problem.
For instance, try running code like this:  latin1word = 'Sacr\xe9 bleu!'
unicodeword = unicode(latin1word, 'latin-1')  print unicodeword   If this works but Beautiful Soup doesn't, there's probably a bug in
Beautiful Soup.
However, if this doesn't work, the problem's with your
Python setup.
Python is playing it safe and not sending non-ASCII
characters to your terminal.
There are two ways to override this
behavior.
The easy way is to remap standard output to a converter that's
not afraid to send ISO-Latin-1 or UTF-8 characters to the terminal.
import codecs  import sys  streamWriter = codecs.lookup('utf-8')[-1]  sys.stdout = streamWriter(sys.stdout)   codecs.lookup returns a number of bound methods and
other objects related to a codec.
The last one is a StreamWriter object capable of wrapping an output
stream.
The hard way is to create a sitecustomize.py file
in your Python installation which sets the default encoding to
ISO-Latin-1 or to UTF-8.
Then all your Python programs will use that
encoding for standard output, without you having to do something for
each program.
In my installation, I have a /usr/lib/python/sitecustomize.py which looks like this:  import sys  sys.setdefaultencoding("utf-8")    For more information about Python's Unicode support, look at Unicode for
Programmers or End to End Unicode
Web Applications in Python .
Recipes 1.20 and 1.21 in the Python
cookbook are also very helpful.
Remember, even if your terminal display is restricted to ASCII, you
can still use Beautiful Soup to parse, process, and write documents in
UTF-8 and other encodings.
You just can't print certain strings with print .
Beautiful Soup can handle poorly-structured SGML, but sometimes
it loses data when it gets stuff that's not SGML at all.
This is
not nearly as common as poorly-structured markup, but if you're
building a web crawler or something you'll surely run into it.
The only solution is to sanitize the data ahead of
time with a regular expression.
Here are some examples that I and
Beautiful Soup users have discovered:  Beautiful Soup treats ill-formed XML definitions as data.
However,
it loses well-formed XML definitions that don't actually exist:  from BeautifulSoup import BeautifulSoup  BeautifulSoup("< !
If the document ends in the middle of the declaration,
Beautiful Soup ignores the declaration totally.
A couple examples:  from BeautifulSoup import BeautifulSoup   BeautifulSoup("foo<!bar") # foo   soup = BeautifulSoup("<html>foo<!bar</html>")  print soup.prettify() # <html> # foo<!bar</html> # </html>   There are a couple ways to fix this; one is detailed here .
Beautiful Soup also ignores an entity reference that's not finished
by the end of the document:  BeautifulSoup("&lt;foo&gt") # &lt;foo   I've never seen this in real web pages, but it's probably out there
somewhere.
A malformed comment will make Beautiful Soup ignore the rest of
the document.
This is covered as the example in Sanitizing Bad Data with Regexps .
The parse tree built by the BeautifulSoup class offends my
senses!
To get your markup parsed differently, check out Other Built-In Parsers , or else build a custom parser .
Beautiful Soup will never run as fast as ElementTree or a
custom-built SGMLParser subclass.
ElementTree is written in C, and SGMLParser lets you write your own mini-Beautiful Soup that only
does what you want.
The point of Beautiful Soup is to save programmer
time, not processor time.
That said, you can speed up Beautiful Soup quite a lot by only 

parsing the parts of the document you need , and you
can make unneeded objects get garbage-collected by using extract or decompose .
Advanced Topics  That does it for the basic usage of Beautiful Soup.
But HTML and
XML are tricky, and in the real world they're even trickier.
So
Beautiful Soup keeps some extra tricks of its own up its sleeve.
Generators  The search methods described above are driven by generator
methods.
You can use these methods yourself: they're called nextGenerator , previousGenerator , nextSiblingGenerator , previousSiblingGenerator , and parentGenerator .
Tag and parser
objects also have childGenerator and recursiveChildGenerator available.
Here's a simple example that strips HTML tags out of a document by
iterating over the document and collecting all the strings.
from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("""<div>You <i>bet</i>  <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>  rocks!</div>""")   ''.join([e for e in soup.recursiveChildGenerator()  if isinstance(e,unicode)]) # u'You bet\nBeautifulSoup\nrocks!'
Of course, you don't really need a generator to find only the text
beneath a tag.
That code does the same thing as .findAll(text=True) .
''.join(soup.findAll(text=True)) # u'You bet\nBeautifulSoup\nrocks!'
Here's a more complex example that uses recursiveChildGenerator to iterate over the elements of a document, printing each one as it
gets it.
from BeautifulSoup import BeautifulSoup  soup = BeautifulSoup("1<a>2<b>3")  g = soup.recursiveChildGenerator()  while True:  try:  print g.next()  except StopIteration:  break # 1 # <a>2<b>3</b></a> # 2 # <b>3</b> # 3   Other Built-In Parsers  Beautiful Soup comes with three parser classes besides BeautifulSoup and  BeautifulStoneSoup :  MinimalSoup is a subclass of BeautifulSoup .
It knows most
facts about HTML like which tags are self-closing, the special
behavior of the <SCRIPT> tag, the possibility of an encoding mentioned
in a <META> tag, etc.
But it has no nesting heuristics at all.
So it
doesn't know that <LI> tags go underneath <UL> tags and not the other
way around.
It's useful for parsing pathologically bad markup, and for
subclassing.
ICantBelieveItsBeautifulSoup is also a subclass of BeautifulSoup .
It has HTML heuristics that conform more
closely to the HTML standard, but ignore how HTML is used in the real
world.
For instance, it's valid HTML to nest <B> tags, but in the real
world a nested <B> tag almost always means that the author forgot to
close the first <B> tag.
If you run into someone who actually nests
<B> tags, then you can use ICantBelieveItsBeautifulSoup .
BeautifulSOAP is a subclass of BeautifulStoneSoup .
It's useful for parsing documents
like SOAP messages, which use a subelement when they could just use an
attribute of the parent element.
Here's an example:  from BeautifulSoup import BeautifulStoneSoup, BeautifulSOAP  xml = "<doc><tag>subelement</tag></doc>"  print BeautifulStoneSoup(xml) # <doc><tag>subelement</tag></doc>  print BeautifulSOAP(xml)  <doc tag="subelement"><tag>subelement</tag></doc>   With BeautifulSOAP you can access the contents of the
<TAG> tag without descending into the tag.
Customizing the Parser  When the built-in parser classes won't do the job, you need to
customize.
This usually means customizing the lists of nestable and
self-closing tags.
You can customize the list of self-closing tags by
passing a selfClosingTags argument
into the soup constructor.
To customize the lists of nestable tags,
though, you'll have to subclass.
The most useful classes to subclass are MinimalSoup (for HTML)
and BeautifulStoneSoup (for XML).
I'm going to show you how to
override RESET_NESTING_TAGS and NESTABLE_TAGS in a subclass.
This
is the most complicated part of Beautiful Soup and I'm not going to
explain it very well here, but I'll get something written and then I
can improve it with feedback.
When Beautiful Soup is parsing a document, it keeps a stack of open
tags.
Whenever it sees a new start tag, it tosses that tag on top of
the stack.
But before it does, it might close some of the open tags
and remove them from the stack.
Which tags it closes depends on the
qualities of tag it just found, and the qualities of the tags in the
stack.
The best way to explain it is through example.
Let's say the stack
looks like ['html', 'p', 'b'] , and Beautiful Soup encounters a <P>
tag.
If it just tossed another 'p' onto the stack, this would imply
that the second <P> tag is within the first <P> tag, not to mention
the open <B> tag.
But that's not the way <P> tags work.
You can't
stick a <P> tag inside another <P> tag.
A <P> tag isn't "nestable" at
all.
So when Beautiful Soup encounters a <P> tag, it closes and pops all
the tags up to and including the previously encountered tag of the
same type.
This is the default behavior, and this is how BeautifulStoneSoup treats every tag.
It's what you get when a
tag is not mentioned in either NESTABLE_TAGS or RESET_NESTING_TAGS .
It's also what you get when a tag shows up in RESET_NESTING_TAGS but has no entry in NESTABLE_TAGS , the way the
<P> tag does.
from BeautifulSoup import BeautifulSoup  BeautifulSoup.RESET_NESTING_TAGS['p'] == None # True  BeautifulSoup.NESTABLE_TAGS.has_key('p') # False   print BeautifulSoup("<html><p>Para<b>one<p>Para two") # <html><p>Para<b>one</b></p><p>Para two</p></html> # ^---^--The second <p> tag made those two tags get closed   Let's say the stack looks like ['html', 'span', 'b'] , and
Beautiful Soup encounters a <SPAN> tag.
Now, <SPAN> tags can contain
other <SPAN> tags without limit, so there's no need to pop up to the
previous <SPAN> tag when you encounter one.
This is represented by
mapping the tag name to an empty list in NESTABLE_TAGS .
This kind of
tag should not be mentioned in RESET_NESTING_TAGS : there are no
circumstances when encountering a <SPAN> tag would cause any tags to
be popped.
from BeautifulSoup import BeautifulSoup  BeautifulSoup.NESTABLE_TAGS['span'] # []  BeautifulSoup.RESET_NESTING_TAGS.has_key('span') # False   print BeautifulSoup("<html><span>Span<b>one<span>Span two") # <html><span>Span<b>one<span>Span two</span></b></span></html>   Third example: suppose the stack looks like ['ol','li','ul'] :
that is, we've got an ordered list, the first element of which
contains an unordered list.
Now suppose Beautiful Soup encounters a
<LI> tag.
It shouldn't pop up to the first <LI> tag, because this new
<LI> tag is part of the unordered sublist.
It's okay for an <LI> tag
to be inside another <LI> tag, so long as there's a <UL> or <OL> tag
in the way.
from BeautifulSoup import BeautifulSoup  print BeautifulSoup("<ol><li>1<ul><li>A").prettify() # <ol> # <li> # 1 # <ul> # <li> # A # </li> # </ul> # </li> # </ol>   But if there is no intervening <UL> or <OL>, then one <LI> tag
can't be underneath another:  print BeautifulSoup("<ol><li>1<li>A").prettify() # <ol> # <li> # 1 # </li> # <li> # A # </li> # </ol>   We tell Beautiful Soup to treat <LI> tags this way by putting "li"
in RESET_NESTING_TAGS , and by giving "li" a NESTABLE_TAGS entry
showing list of tags under which it can nest.
BeautifulSoup.RESET_NESTING_TAGS.has_key('li') # True  BeautifulSoup.NESTABLE_TAGS['li'] # ['ul', 'ol']   This is also how we handle the nesting of table tags:  BeautifulSoup.NESTABLE_TAGS['td'] # ['tr']  BeautifulSoup.NESTABLE_TAGS['tr'] # ['table', 'tbody', 'tfoot', 'thead']  BeautifulSoup.NESTABLE_TAGS['tbody'] # ['table']  BeautifulSoup.NESTABLE_TAGS['thead'] # ['table']  BeautifulSoup.NESTABLE_TAGS['tfoot'] # ['table']  BeautifulSoup.NESTABLE_TAGS['table'] # []   That is: <TD> tags can be nested within <TR> tags.
<TR> tags can be
nested within <TABLE>, <TBODY>, <TFOOT>, and <THEAD> tags.
<TBODY>,
<TFOOT>, and <THEAD> tags can be nested in <TABLE> tags, and <TABLE>
tags can be nested in other <TABLE> tags.
If you know about HTML
tables, these rules should already make sense to you.
One more example.
Say the stack looks like ['html', 'p', 'table'] and Beautiful Soup encounters a <P> tag.
At first glance, this looks just like the example where the stack
is ['html', 'p', 'b'] and Beautiful Soup encounters a <P> tag.
In
that example, we closed the <B> and <P> tags, because you can't have
one paragraph inside another.
Except...
you can have a paragraph that contains a table,
and then the table contains a paragraph.
So the right thing to do is
to not close any of these tags.
Beautiful Soup does the right thing:  from BeautifulSoup import BeautifulSoup  print BeautifulSoup("<p>Para 1<b><p>Para 2") # <p> # Para 1 # <b> # </b> # </p> # <p> # Para 2 # </p>   print BeautifulSoup("<p>Para 1<table><p>Para 2").prettify() # <p> # Para 1 # <table> # <p> # Para 2 # </p> # </table> # </p>   What's the difference?
The difference is that <TABLE> is in RESET_NESTING_TAGS and <B> is not.
A tag that's in RESET_NESTING_TAGS doesn't get popped off the stack as easily as a
tag that's not.
Okay, hopefully you get the idea.
Here's the NESTABLE_TAGS for
the BeautifulSoup class.
Correlate this with what you know about
HTML, and you should be able to create your own NESTABLE_TAGS for
bizarre HTML documents that don't follow the normal rules, and for
other XML dialects that have different nesting rules.
from BeautifulSoup import BeautifulSoup  nestKeys = BeautifulSoup.NESTABLE_TAGS.keys()  nestKeys.sort()  for key in nestKeys:  print "%s: %s" % (key, BeautifulSoup.NESTABLE_TAGS[key]) # bdo: [] # blockquote: [] # center: [] # dd: ['dl'] # del: [] # div: [] # dl: [] # dt: ['dl'] # fieldset: [] # font: [] # ins: [] # li: ['ul', 'ol'] # object: [] # ol: [] # q: [] # span: [] # sub: [] # sup: [] # table: [] # tbody: ['table'] # td: ['tr'] # tfoot: ['table'] # th: ['tr'] # thead: ['table'] # tr: ['table', 'tbody', 'tfoot', 'thead'] # ul: []   And here's BeautifulSoup 's RESET_NESTING_TAGS .
Only the keys
are important: RESET_NESTING_TAGS is actually a list, put into the
form of a dictionary for quick random access.
from BeautifulSoup import BeautifulSoup  resetKeys = BeautifulSoup.RESET_NESTING_TAGS.keys()  resetKeys.sort()  resetKeys # ['address', 'blockquote', 'dd', 'del', 'div', 'dl', 'dt', 'fieldset', # 'form', 'ins', 'li', 'noscript', 'ol', 'p', 'pre', 'table', 'tbody', # 'td', 'tfoot', 'th', 'thead', 'tr', 'ul']   Since you're subclassing anyway, you might as well override SELF_CLOSING_TAGS while you're at it.
It's a dictionary that maps
self-closing tag names to any values at all (like RESET_NESTING_TAGS , it's actually a list in the form of a
dictionary).
Then you won't have to pass that list in to the
constructor (as selfClosingTags ) every time you instantiate your
subclass.
Entity Conversion  When you parse a document, you can convert HTML or XML entity
references to the corresponding Unicode characters.
This code converts
the HTML entity "&eacute;" to the Unicode character LATIN SMALL
LETTER E WITH ACUTE, and the numeric entity "&#101;" to the Unicode
character LATIN SMALL LETTER E.
 from BeautifulSoup import BeautifulStoneSoup  BeautifulStoneSoup("Sacr&eacute; bl&#101;u!
",  convertEntities=BeautifulStoneSoup.HTML_ENTITIES).contents[0] # u'Sacr\xe9 bleu!'
That's if you use HTML_ENTITIES (which is just the string
"html").
If you use XML_ENTITIES (or the string "xml"), then only
numeric entities and the five XML entities ("&quot;",
"&apos;", "&gt;", "&lt;", and "&amp;") get
converted.
If you use ALL_ENTITIES (or the list ["xml", "html"] ),
then both kinds of entities will be converted.
This last one is
neccessary because &apos; is an XML entity but not an HTML
entity.
BeautifulStoneSoup("Sacr&eacute; bl&#101;u!
",  convertEntities=BeautifulStoneSoup.XML_ENTITIES) # Sacr&eacute; bleu!
from BeautifulSoup import BeautifulStoneSoup  BeautifulStoneSoup("Il a dit, &lt;&lt;Sacr&eacute; bl&#101;u!&gt;&gt;",  convertEntities=BeautifulStoneSoup.XML_ENTITIES) # Il a dit, <<Sacr&eacute; bleu!>>   If you tell Beautiful Soup to convert XML or HTML entities into the
corresponding Unicode characters, then Windows-1252 characters (like
Microsoft smart quotes) also get transformed into Unicode
characters.
This happens even if you told Beautiful Soup to convert
those characters to entities.
from BeautifulSoup import BeautifulStoneSoup  smartQuotesAndEntities = "Il a dit, \x8BSacr&eacute; bl&#101;u!\x9b"   BeautifulStoneSoup(smartQuotesAndEntities, smartQuotesTo="html").contents[0] # u'Il a dit, &lsaquo;Sacr&eacute; bl&#101;u!&rsaquo;'   BeautifulStoneSoup(smartQuotesAndEntities, convertEntities="html",  smartQuotesTo="html").contents[0] # u'Il a dit, \u2039Sacr\xe9 bleu!\u203a'   BeautifulStoneSoup(smartQuotesAndEntities, convertEntities="xml",  smartQuotesTo="xml").contents[0] # u'Il a dit, \u2039Sacr&eacute; bleu!\u203a'   It doesn't make sense to create new HTML/XML entities while you're
busy turning all the existing entities into Unicode
characters.
Sanitizing Bad Data with Regexps  Beautiful Soup does pretty well at handling bad markup when "bad
markup" means tags in the wrong places.
But sometimes the markup is
just malformed, and the underlying parser can't handle it.
So
Beautiful Soup runs regular expressions against an input document
before trying to parse it.
By default, Beautiful Soup uses regular expressions and replacement
functions to do search-and-replace on input documents.
It finds
self-closing tags that look like <BR/>, and changes them to look like
<BR />.
It finds declarations that have extraneous whitespace, like 
<!
--Comment-->, and removes the whitespace: <!--Comment-->.
If you have bad markup that needs fixing in some other way, you can
pass your own list of (regular expression, replacement function) tuples into the soup constructor, as the markupMassage argument.
Let's take an example: a page that has a malformed comment.
The
underlying SGML parser can't cope with this, and ignores the comment
and everything afterwards:  from BeautifulSoup import BeautifulSoup  badString = "Foo<!-This comment is malformed.-->Bar<br/>Baz"  BeautifulSoup(badString) # Foo   Let's fix it up with a regular expression and a function:  import re  myMassage = [(re.compile('<!-([^-])'), lambda match: '<!--' + match.group(1))]  BeautifulSoup(badString, markupMassage=myMassage) # Foo<!--This comment is malformed.-->Bar   Oops, we're still missing the <BR> tag.
Our markupMassage overrides the parser's default massage, so the default
search-and-replace functions don't get run.
The parser makes it past
the comment, but it dies at the malformed self-closing tag.
Let's add
our new massage function to the default list, so we run all the
functions.
import copy  myNewMassage = copy.copy(BeautifulSoup.MARKUP_MASSAGE)  myNewMassage.extend(myMassage)  BeautifulSoup(badString, markupMassage=myNewMassage) # Foo<!--This comment is malformed.-->Bar<br />Baz   Now we've got it all.
If you know for a fact that your markup doesn't need any regular
expressions run on it, you can get a faster startup time by passing in False for markupMassage .
Fun With SoupStrainer s  Recall that all the search methods take more or less the same arguments .
Behind the scenes, your arguments
to a search method get transformed into a SoupStrainer object.
If
you call one of the methods that returns a list (like findAll ), the SoupStrainer object is made available as the source property of
the resulting list.
from BeautifulSoup import BeautifulStoneSoup  xml = '<person name="Bob"><parent rel="mother" name="Alice">'  xmlSoup = BeautifulStoneSoup(xml)  results = xmlSoup.findAll(rel='mother')   results.source # <BeautifulSoup.SoupStrainer instance at 0xb7e0158c>  str(results.source) # "None|{'rel': 'mother'}"   The SoupStrainer constructor takes most of the same arguments as find : name , attrs , text , and **kwargs .
You can pass
in a SoupStrainer as the name argument to any search method:  xmlSoup.findAll(results.source) == results # True   customStrainer = BeautifulSoup.SoupStrainer(rel='mother')  xmlSoup.findAll(customStrainer) == results # True   Yeah, who cares, right?
You can carry around a method call's
arguments in many other ways.
But another thing you can do with SoupStrainer is pass it into the soup constructor to restrict the
parts of the document that actually get parsed.
That brings us to the
next section: Improving Performance by Parsing Only Part of the Document  Beautiful Soup turns every element of a document into a Python
object and connects it to a bunch of other Python objects.
If you only
need a subset of the document, this is really slow.
But you can pass
in a SoupStrainer as the parseOnlyThese argument to the soup constructor.
Beautiful Soup
checks each element against the SoupStrainer , and only if it matches
is the element turned into a Tag or NavigableText , and added to
the tree.
If an element is added to to the tree, then so are its
children—even if they wouldn't have matched the SoupStrainer on their own.
This lets you parse only the chunks of a document that
contain the data you want.
Here's a pretty varied document:  doc = '''Bob reports <a href="http://www.bob.com/">success</a>  with his plasma breeding <a  href="http://www.bob.com/plasma">experiments</a>.
<i>Don't get any on  us, Bob!</i>   <br><br>Ever hear of annular fusion?
The folks at <a  href="http://www.boogabooga.net/">BoogaBooga</a> sure seem obsessed  with it.
Secret project, or <b>WEB MADNESS?</b> You decide!'''
Here are several different ways of parsing the document into soup,
depending on which parts you want.
All of these are faster and use
less memory than parsing the whole document and then using the same SoupStrainer to pick out the parts you want.
from BeautifulSoup import BeautifulSoup, SoupStrainer  import re   links = SoupStrainer('a')  [tag for tag in BeautifulSoup(doc, parseOnlyThese=links)] # [<a href="http://www.bob.com/">success</a>, # <a href="http://www.bob.com/plasma">experiments</a>, # <a href="http://www.boogabooga.net/">BoogaBooga</a>]   linksToBob = SoupStrainer('a', href=re.compile('bob.com/'))  [tag for tag in BeautifulSoup(doc, parseOnlyThese=linksToBob)] # [<a href="http://www.bob.com/">success</a>, # <a href="http://www.bob.com/plasma">experiments</a>]   mentionsOfBob = SoupStrainer(text=re.compile("Bob"))  [text for text in BeautifulSoup(doc, parseOnlyThese=mentionsOfBob)] # [u'Bob reports ', u"Don't get any on\nus, Bob!"]
allCaps = SoupStrainer(text=lambda(t):t.upper()==t)  [text for text in BeautifulSoup(doc, parseOnlyThese=allCaps)] # [u'.
', u'\n', u'WEB MADNESS?']
There is one major difference between the SoupStrainer you pass
into a search method and the one you pass into a soup
constructor.
Recall that the name argument can take a function whose argument is a Tag object .
You can't do this for a SoupStrainer 's name , because
the SoupStrainer is used to decide whether or not a Tag object
should be created in the first place.
You can pass in a function for a SoupStrainer 's name , but it can't take a Tag object: it can only
take the tag name and a map of arguments.
shortWithNoAttrs = SoupStrainer(lambda name, attrs: \  len(name) == 1 and not attrs)  [tag for tag in BeautifulSoup(doc, parseOnlyThese=shortWithNoAttrs)] # [<i>Don't get any on us, Bob!</i>, # <b>WEB MADNESS?</b>]   Improving Memory Usage with extract  When Beautiful Soup parses a document, it loads into memory a large,
densely connected data structure.
If you just need a string from that
data structure, you might think that you can grab the string and leave
the rest of it to be garbage collected.
Not so.
That string is a NavigableString object.
It's got a parent member that points to a Tag object, which points to other Tag objects, and so on.
So long
as you hold on to any part of the tree, you're keeping the whole thing
in memory.
The extract method breaks those connections.
If you call extract on the string you need, it gets disconnected from the rest
of the parse tree.
The rest of the tree can then go out of scope and
be garbage collected, while you use the string for something else.
If
you just need a small part of the tree, you can call extract on its
top-level Tag and let the rest of the tree get garbage collected.
This works the other way, too.
If there's a big chunk of the
document you don't need, you can call extract to rip it out
of the tree, then abandon it to be garbage collected while retaining
control of the (smaller) tree.
If extract doesn't work for you, you can try Tag.decompose .
It's slower than extract but more thorough.
It
recursively disassembles a Tag and its contents, disconnecting every
part of a tree from every other part.
If you find yourself destroying big chunks of the tree, you might
have been able to save time by not
parsing that part of the tree in the first place .
See Also  Applications that use Beautiful Soup  Lots of real-world applications use Beautiful Soup.
Here are the
publicly visible applications that I know about:  Scrape 'N'
Feed is designed to work with Beautiful Soup to build RSS feeds
for sites that don't have them.
htmlatex uses
Beautiful Soup to find LaTeX equations and render them as graphics.
chmtopdf converts
CHM files to PDF format.
Who am I to argue with that?
Duncan Gough's Fotopic backup uses
Beautiful Soup to scrape the Fotopic website.
Iñigo Serna's googlenews.py uses Beautiful Soup to scrape Google News (it's in the parse_entry and
parse_category functions) The Weather
Office Screen Scraper uses Beautiful Soup to scrape the Canadian
government's weather office site.
News
Clues uses Beautiful Soup to parse RSS feeds.
BlinkFlash uses Beautiful Soup to automate form submission for an online service.
The linky link checker uses Beautiful Soup to find a page's links and images
that need checking.
Matt
Croydon got Beautiful Soup 1.x to work on his Nokia Series 60
smartphone.
C.R.
Sandeep wrote a real-time currency converter for the Series 60 using Beautiful
Soup, but he won't show us how he did it.
Here's a
short script from jacobian.org to fix the metadata on music files
downloaded from allofmp3.com.
The Python Community Server uses Beautiful Soup in its spam detector.
Similar libraries  I've found several other parsers for various languages that can
handle bad markup, do tree traversal for you, or are otherwise more
useful than your average parser.
I've ported Beautiful Soup to Ruby.
The result is Rubyful Soup .
Hpricot is
giving Rubyful Soup a run for its money.
ElementTree is a fast Python XML parser with a bad attitude.
I love it.
Tag Soup is
an XML/HTML parser written in Java which rewrites bad HTML into
parseable HTML.
HtmlPrag is a
Scheme library for parsing bad HTML.
xmltramp is a
nice take on a 'standard' XML/XHTML parser.
Like most parsers, it
makes you traverse the tree yourself, but it's easy to use.
pullparser includes a tree-traversal method.
Mike Foord didn't like the way Beautiful Soup can change HTML if
you write the tree back out, so he wrote HTML
Scraper .
It's basically a version of HTMLParser that can handle
bad HTML.
It might be obsolete with the release of Beautiful Soup 3.0,
though; I'm not sure.
Ka-Ping Yee's scrape.py combines page
scraping with URL opening.
Conclusion  That's it!
Have fun!
I wrote Beautiful Soup to save everybody
time.
Once you get used to it, you should be able to wrangle data out
of poorly-designed websites in just a few minutes.
Send me email if
you have any comments, run into problems, or want me to know about
your project that uses Beautiful Soup.
--Leonard  This document is part of Crummy, the webspace of Leonard Richardson ( contact information ).
It was last modified on Monday, November 11 2019, 17:40:04 Nowhere Standard Time and last built on Wednesday, April 17 2024, 20:00:01 Nowhere Standard Time.
Document tree:  http://www.crummy.com/ software/ BeautifulSoup/ bs3/ documentation.html     Site Search:       Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup 4.12.0 文档         Beautiful Soup 4.12.0 文档 ¶  Beautiful Soup 是一个
可以从 HTML 或 XML 文件中提取数据的 Python 库。它能用你喜欢的解析器和习惯的方式实现
文档树的导航、查找、和修改。它会帮你节省数小时甚至数天的工作时间。  这篇文档介绍了 Beautiful Soup 4 中所有主要特性，并附带例子。文档会展示这个库的适合场景，
工作原理，怎样使用，如何达到预期效果，以及如何处理异常情况。  文档覆盖了 Beautful Soup 4.12.0 版本，文档中的例子使用 Python 3.8 版本编写。  你可能在寻找 Beautiful Soup3 的文档，Beautiful Soup 3 目前已经停止开发，并且自 2020年12月31日以后就停止维护了。
如果想要了解 Beautiful Soup 3 和 Beautiful Soup 4 的不同，参考 迁移到 BS4 。  这篇文档已经被翻译成多种语言:   这篇文档当然还有中文版 ,
( Github 地址 ).
Este documento también está disponible en español.
寻求帮助 ¶  如果有关于 Beautiful Soup 4 的疑问，或遇到了问题，可以发送邮件到 讨论组 。  如果问题中包含要解析的 HTML 代码，那么请在你的问题描述中附带这段HTML文档的 代码诊断  [ 1 ] 。  如果报告文档中的错误，请指出具体文档的语言版本。     快速开始 ¶  下面的一段HTML代码将作为例子被多次用到。这是 爱丽丝梦游仙境 的一段内容(以后简称 爱丽丝 的文档):  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """    上面的 爱丽丝 文档经过 Beautiful Soup 的解析后，会得到一个 BeautifulSoup 的对象，
一个嵌套结构的对象:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )  print ( soup .
prettify ())  # <html>  #  <head>  #   <title>  #    The Dormouse's story  #   </title>  #  </head>  #  <body>  #   <p class="title">  #    <b>  #     The Dormouse's story  #    </b>  #   </p>  #   <p class="story">  #    Once upon a time there were three little sisters; and their names were  #    <a class="sister" href="http://example.com/elsie" id="link1">  #     Elsie  #    </a>  #    ,  #    <a class="sister" href="http://example.com/lacie" id="link2">  #     Lacie  #    </a>  #    and  #    <a class="sister" href="http://example.com/tillie" id="link2">  #     Tillie  #    </a>  #    ; and they lived at the bottom of a well.
#   </p>  #   <p class="story">  #    ...
 #   </p>  #  </body>  # </html>    这是几个简单的浏览结构化数据的方法:  soup .
find ( id = "link3" )  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>    常见任务之一，就是从文档中找到所有 <a> 标签的链接:  for  link  in  soup .
get ( 'href' ))  # http://example.com/elsie  # http://example.com/lacie  # http://example.com/tillie    另一种常见任务，是从文档中获取所有文字内容:  print ( soup .
#  # ...
   这是你想要的吗？是的话，继续看下去。    安装 Beautiful Soup ¶  如果你用的是新版的 Debain 或 Ubuntu，那么可以通过系统的软件包管理来安装:  $  apt - get  install  python3 - bs4  Beautiful Soup 4 通过 PyPi 发布，所以如果无法使用系统包管理安装，那么
也可以通过 easy_install 或 pip 来安装。包的名字是 beautifulsoup4 。
确保使用的是与 Python 版本对应的 pip 或 easy_install 版本
(他们的名字也可能是 pip3 和 easy_install )。  $  easy_install  beautifulsoup4  $  pip  install  beautifulsoup4  (在 PyPi 中还有一个名字是 BeautifulSoup 的包，但那可能不是你想要的，那是 Beautiful Soup3 版本。因为很多项目还在使用BS3, 所以 BeautifulSoup 包依然有效。但是新项目中，应该安装 beautifulsoup4 。)  如果没有安装 easy_install 或 pip ，那也可以 下载 BS4 的源码 ,
然后通过 setup.py 来安装。  $  Python  setup.py  install  如果上述安装方法都行不通，根据 Beautiful Soup 的协议，可以将项目的代码打包在
你的项目中，这样无须安装即可使用。  Beautiful Soup 用 Python 3.10 版本开发，但也可以在当前的其它版本中运行。   安装解析器 ¶  Beautiful Soup 支持 Python 标准库中的 HTML 解析器，还支持一些第三方的解析器，
其中一个是 lxml parser 。根据安装方法的不同，
可以选择下列方法来安装 lxml:  $  apt - get  install  Python - lxml  $  easy_install  lxml  $  pip  install  lxml  另一个可供选择的解析器是纯 Python 实现的 html5lib ,
html5lib 的解析方式与浏览器相同，根据安装方法的不同，可以选择下列方法来安装html5lib:  $  apt - get  install  python - html5lib  $  easy_install  html5lib  $  pip  install  html5lib  下表描述了几种解析器的优缺点:   解析器  使用方法  优势  劣势     Python 标准库       BeautifulSoup(markup,  "html.parser")       - Python的内置标准库  - 执行速度较快  - 容错能力强     - 速度没有 lxml 快，容错没有 html5lib强        lxml HTML 解析器       BeautifulSoup(markup,  "lxml")       - 速度快  - 容错能力强      - 额外的 C 依赖        lxml XML 解析器      BeautifulSoup(markup,  ["lxml-xml"])  BeautifulSoup(markup,  "xml")     - 速度快  - 唯一支持 XML 的解析器     - 额外的 C 依赖       html5lib       BeautifulSoup(markup,  "html5lib")       - 最好的容错性  - 以浏览器的方式解析文档  - 生成 HTML5 格式的文档     - 速度慢  - 额外的 Python 依赖       如果可以，推荐使用 lxml 来获得更高的速度。  注意，如果一段文档格式不标准，那么在不同解析器生成的 Beautiful Soup 数可能不一样。
查看 解析器之间的区别 了解更多细节。     如何使用 ¶  解析文档是，将文档传入 BeautifulSoup 的构造方法。也可以传入一段字符串
或一个文件句柄:  from  bs4  import  BeautifulSoup  with  open ( "index.html" )  as  fp :  soup  =  BeautifulSoup ( fp ,  'html.parser' )  soup  =  BeautifulSoup ( "<html>a web page</html>" ,  'html.parser' )    首先，文档被转换成 Unicode，并且 HTML 中的实体也都被转换成 Unicode 编码  print ( BeautifulSoup ( "<html><head></head><body>Sacr&eacute; bleu!</body></html>" ,  "html.parser" ))  # <html><head></head><body>Sacré bleu!</body></html>    然后，Beautiful Soup 选择最合适的解析器来解析这段文档。如果指定了解析器那么 Beautiful Soup
会选择指定的解析器来解析文档。(参考 解析成XML )。    对象的种类 ¶  Beautiful Soup 将复杂的 HTML 文档转换成一个复杂的由 Python 对象构成的树形结构，但处理对象
的过程只包含 4 种类型的对象: Tag , NavigableString , BeautifulSoup , 和 Comment 。  Tag  Tag 对象与 XML 或 HTML 原生文档中的 tag 相同:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
b  type ( tag )  # <class 'bs4.element.Tag'>    Tag有很多属性和方法，在 遍历文档树 和 搜索文档树 中有详细解释。
现在介绍一下 tag 中最重要的属性: name 和 attributes。    bs4.
name ¶   每个 tag 都有一个名字:  tag .
name  # u'b'    如果改变了 tag 的 name，那将影响所有通过当前 Beautiful Soup 对象生成的HTML文档:  tag .
name  =  "blockquote"  tag  # <blockquote class="boldest">Extremely bold</blockquote>      bs4.
attrs ¶   一个 HTML 或 XML 的 tag 可能有很多属性。tag <b  id="boldest"> 有
一个 “id” 的属性，值为 “boldest”。你可以想处理一个字段一样来处理 tag 的属性:  tag  =  BeautifulSoup ( '<b id="boldest">bold</b>' ,  'html.parser' ) .
b  tag [ 'id' ]  # 'boldest'    也可以直接”点”取属性，比如: .attrs :  tag .
attrs  # {u'class': u'boldest'}    tag 的属性可以被添加、删除或修改。再说一次，tag的属性操作方法与字典一样  tag [ 'id' ]  =  'verybold'  tag [ 'another-attribute' ]  =  1  tag  # <b another-attribute="1" id="verybold"></b>  del  tag [ 'id' ]  del  tag [ 'another-attribute' ]  tag  # <b>bold</b>  tag [ 'id' ]  # KeyError: 'id'  tag .
get ( 'id' )  # None     多值属性 ¶  HTML 4 定义了一系列可以包含多个值的属性。在 HTML5 中移除了一些，却增加更多。
最常见的多值的属性是 class (一个 tag 可以有多个 CSS class)。还有一些
属性 rel 、 rev 、 accept-charset 、 headers 、 accesskey 。
默认情况，Beautiful Soup 中将多值属性解析为一个列表:  css_soup = BeautifulSoup('<p class="body"></p>', 'html.parser')
 css_soup.p['class']
 # ['body']

 css_soup = BeautifulSoup('<p class="body strikeout"></p>', 'html.parser')
 css_soup.p['class']
 # ['body', 'strikeout']

If an attribute `looks` like it has more than one value, but it's not
a multi-valued attribute as defined by any version of the HTML
standard, Beautiful Soup will leave the attribute alone::

 id_soup = BeautifulSoup('<p id="my id"></p>', 'html.parser')
 id_soup.p['id']
 # 'my id'   如果某个属性看起来好像有多个值，但在任何版本的 HTML 定义中都没有将其定义为多值属性，
那么 Beautiful Soup 会将这个属性作为单值返回  id_soup  =  BeautifulSoup ( '<p id="my id"></p>' ,  'html.parser' )  id_soup .
p [ 'class' ]  # 'body strikeout'    或者使用 get_attribute_list 方法来获取多值列表，不管是不是一个多值属性:  id_soup .
get_attribute_list ( 'id' )  # ["my id"]    如果以 XML 方式解析文档，则没有多值属性:  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' )  xml_soup .
p [ 'class' ]  # 'body strikeout'    但是，可以通过配置 multi_valued_attributes 参数来修改:  class_is_multi =  {  '*'  :  'class' }  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' ,  multi_valued_attributes = class_is_multi )  xml_soup .
p [ 'class' ]  # ['body', 'strikeout']    可能实际当中并不需要修改默认配置，默认采用的是 HTML 标准:  from  bs4.builder  import  builder_registry  builder_registry .
DEFAULT_CDATA_LIST_ATTRIBUTES      class  bs4.
NavigableString ¶     可遍历的字符串 ¶  字符串对应 tag 中的一段文本。Beautiful Soup 用 NavigableString 类来包装 tag 中的字符串:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
string )  # <class 'bs4.element.NavigableString'>    一个 NavigableString 对象与 Python 中的Unicode 字符串相同，
并且还支持包含在 遍历文档树 和 搜索文档树 中的一些特性。通过 str 方法可以直接将 NavigableString 对象转换成 Unicode 字符串:  unicode_string  =  str ( tag .
string )  unicode_string  # 'Extremely bold'  type ( unicode_string )  # <type 'str'>    tag 中包含的字符串不能直接编辑，但是可以被替换成其它的字符串，用 replace_with() 方法:  tag .
replace_with ( "No longer bold" )  tag  # <blockquote>No longer bold</blockquote>    NavigableString 对象支持 遍历文档树 和 搜索文档树 中定义的大部分属性，
并非全部。尤其是，一个字符串不能包含其它内容(tag 能够包含字符串或是其它 tag)，字符串不支持 .contents 或 .string 属性或 find() 方法。  如果想在 Beautiful Soup 之外使用 NavigableString 对象，需要调用 unicode() 方法，将该对象转换成普通的Unicode字符串，否则就算 Beautiful Soup 方法已经执行结束，该对象的输出
也会带有对象的引用地址。这样会浪费内存。    class  bs4.
BeautifulSoup ¶    BeautifulSoup 对象表示的是一个文档的全部内容。大部分时候，可以把它当作 Tag 对象，
它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法。  因为 BeautifulSoup 对象并不是真正的HTML或XML的tag,所以它没有name和attribute属性。
但有时查看它的 .name 属性是很方便的，所以 BeautifulSoup 对象包含了一个
值为 “[document]” 的特殊属性 .name  soup .
name  # u'[document]'      注释及特殊字符串 ¶  Tag , NavigableString , BeautifulSoup 几乎覆盖了html和xml中的所有内容，但是还有一些特殊对象。容易让人担心的内容是文档的注释部分:  markup  =  "<b><!--Hey, buddy.
string  type ( comment )  # <class 'bs4.element.Comment'>    Comment 对象是一个特殊类型的 NavigableString 对象:  comment  # u'Hey, buddy.
Want to buy a used parser'    但是当它出现在 HTML 文档中时， Comment 对象会使用特殊的格式输出:  print ( soup .
Want to buy a used parser?-->  # </b>     针对 HTML 文档 ¶  Beautiful Soup 定义了一些 NavigableString 子类来处理特定的 HTML 标签。
通过忽略页面中表示程序指令的字符串，可以更容易挑出页面的 body 内容。
（这些类是在 Beautiful Soup 4.9.0 版本中添加的，html5lib 解析器不会使用它们）    class  bs4.
Stylesheet ¶   有一种 NavigableString 子类表示嵌入的 CSS 脚本；
内容是 <style> 标签内部的所有字符串。    class  bs4.
Script ¶   有一种 NavigableString 子类表示嵌入的 JavaScript 脚本；
内容是 <script> 标签内部的所有字符串。    class  bs4.
Template ¶   有一种 NavigableString 子类表示嵌入的 HTML 模板，
内容是 <template> 标签内部的所有字符串。    针对 XML 文档 ¶  Beautiful Soup 定义了一些 NavigableString 子类来处理 XML 文档中的特定
字符串。比如 Comment ，这些 NavigableString 的子类生成字符
串时会添加额外内容。    class  bs4.
Declaration ¶   有一种 NavigableString 子类表示 XML 文档开头的 declaration 。    class  bs4.
Doctype ¶   有一种 NavigableString 子类表示可能出现在 XML 文档开头的 document type
declaration 。    class  bs4.
CData ¶   有一种 NavigableString 子类表示 CData section 。    class  bs4.
ProcessingInstruction ¶   有一种 NavigableString 子类表示 XML 处理指令 。      遍历文档树 ¶  还是用”爱丽丝”的文档来做例子:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    通过这段例子来演示怎样从文档的一段内容找到另一段内容   子节点 ¶  tag 可能包含多个字符串或其它的 tag，这些都是这个 Tag 的子节点。Beautiful Soup 提供了许多查找
和操作子节点的方法。  注意: Beautiful Soup中字符串节点不支持这些属性，因为字符串没有子节点。   Tag 的名字 ¶  操作文档树最简单的方法就是告诉它你想获取的 tag 的 name。如果想获取 <head> 标签，只要用 soup.head :  soup .
title  # <title>The Dormouse's story</title>    这是个获取tag的小窍门，可以在文档树的tag中多次调用这个方法。下面的代码可以获取 <body> 标签中的
第一个 <b> 标签:  soup .
b  # <b>The Dormouse's story</b>    通过点取属性的方式只能获得当前名字的第一个tag:  soup .
a  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    如果想要得到所有的 <a> 标签，或是比通过名字获取内容更复杂的方法时，就需要用到 搜索文档树 中描述的方法，比如: find_all()  soup .
find_all ( 'a' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      .contents 和 .children ¶  Tag 的 .contents 属性可以将 tag 的全部子节点以列表的方式输出:  head_tag  =  soup .
contents  [ < title > The  Dormouse 's story</title>]  title_tag  =  head_tag .
contents  # [u'The Dormouse's story']    BeautifulSoup 对象一定会包含子节点。下面例子中 <html> 标签就是 BeautifulSoup 对象的子节点:  len ( soup .
name  # u'html'    字符串没有 .contents 属性，因为字符串没有子节点:  text  =  title_tag .
contents  # AttributeError: 'NavigableString' object has no attribute 'contents'    通过 tag 的 .children 生成器，可以对 tag 的子节点进行循环:  for  child  in  title_tag .
children :  print ( child )  # The Dormouse's story    如果想要修改 tag 的子节点，使用 修改文档树 中描述的方法。不要直接修改 contents 列表:
那样会导致细微且难以定位的问题。    .descendants ¶  .contents 和 .children 属性仅包含 tag 的直接子节点。例如，<head> 标签只有一个直接
子节点 <title>  head_tag .
contents  # [<title>The Dormouse's story</title>]    但是 <title> 标签也包含一个子节点：字符串 “The Dormouse’s story”。这种情况下字符串
“The Dormouse’s story” 也属于 <head> 标签的子节点。 .descendants 属性可以对
所有 tag 的子孙节点进行递归循环 [ 5 ] ，包括子节点，子节点的子节点:  for  child  in  head_tag .
descendants :  print ( child )  # <title>The Dormouse's story</title>  # The Dormouse's story    上面的例子中，<head> 标签只有一个子节点，但是有 2 个子孙节点: <head> 标签和 <head> 的子节点。 BeautifulSoup 对象只有一个直接子节点(<html> 节点)，却有很多子孙节点:  len ( list ( soup .
descendants ))  # 25      .string ¶  如果 tag 只有一个 NavigableString 类型子节点，那么这个tag可以使用 .string 得到子节点:  title_tag .
string  # u'The Dormouse's story'    如果一个tag仅有一个子节点，那么这个tag也可以使用 .string 方法，输出结果与当前唯一
子节点的 .string 结果相同:  head_tag .
string  # u'The Dormouse's story'    如果tag包含了多个子节点，tag就无法确定 .string 方法应该调用哪个子节点的内容， .string 的输出结果是 None :  print ( soup .
string )  # None      .strings 和 stripped_strings ¶  如果 tag 中包含多个字符串 [ 2 ] ,可以使用 .strings 来循环获取:  for  string  in  soup .
strings :  print ( repr ( string ))  # u"The Dormouse's story"  # u'\n\n'  # u"The Dormouse's story"  # u'\n\n'  # u'Once upon a time there were three little sisters; and their names were\n'  # u'Elsie'  # u',\n'  # u'Lacie'  # u' and\n'  # u'Tillie'  # u';\nand they lived at the bottom of a well.'
# u'\n\n'  # u'...'  # u'\n'    输出的字符串中可能包含了很多空格或空行，使用 .stripped_strings 可以去除多余空白内容:  for  string  in  soup .
stripped_strings :  print ( repr ( string ))  # u"The Dormouse's story"  # u"The Dormouse's story"  # u'Once upon a time there were three little sisters; and their names were'  # u'Elsie'  # u','  # u'Lacie'  # u'and'  # u'Tillie'  # u';\nand they lived at the bottom of a well.'
# u'...'    全部是空格的行会被忽略掉，段首和段末的空白会被删除     父节点 ¶  继续分析文档树，每个 tag 或字符串都有父节点: 包含当前内容的 tag   .parent ¶  通过 .parent 属性来获取某个元素的父节点。在例子“爱丽丝”的文档中，<head> 标签是
<title> 标签的父节点:  title_tag  =  soup .
parent  # <head><title>The Dormouse's story</title></head>    文档的 title 字符串也有父节点: <title> 标签  title_tag .
parent  # <title>The Dormouse's story</title>    文档的顶层节点比如 <html> 的父节点是 BeautifulSoup 对象:  html_tag  =  soup .
parent )  # <class 'bs4.BeautifulSoup'>    BeautifulSoup 对象的 .parent 是None:  print ( soup .
parent )  # None      .parents ¶  通过元素的 .parents 属性可以递归得到元素的所有父辈节点，下面的例子使用了 .parents 方法遍历了 <a> 标签到根节点的所有节点。  link  =  soup .
parents :  if  parent  is  None :  print ( parent )  else :  print ( parent .
name )  # p  # body  # html  # [document]  # None       兄弟节点 ¶  看一段简单的例子:  sibling_soup  =  BeautifulSoup ( "<a><b>text1</b><c>text2</c></a>" ,  'html.parser' )  print ( sibling_soup .
prettify ())  #   <a>  #    <b>  #     text1  #    </b>  #    <c>  #     text2  #    </c>  #   </a>    因为 <b> 标签和 <c> 标签是同一层: 他们是同一个元素的子节点，所以 <b> 和 <c> 可以被称为兄弟节点。
一段文档以标准格式输出时，兄弟节点有相同的缩进级别。在代码中也可以使用这种关系。   .next_sibling 和 .previous_sibling ¶  在文档树中，使用 .next_sibling 和 .previous_sibling 属性来查询兄弟节点:  sibling_soup .
previous_sibling  # <b>text1</b>    <b> 标签有 .next_sibling 属性，但是没有 .previous_sibling 属性，
因为 <b> 标签在同级节点中是第一个。同理，<c>标签有 .previous_sibling 属性，
却没有 .next_sibling 属性:  print ( sibling_soup .
next_sibling )  # None    例子中的字符串 “text1” 和 “text2” 不是兄弟节点，因为它们的父节点不同:  sibling_soup .
string  # u'text1'  print ( sibling_soup .
next_sibling )  # None    实际文档中的 tag 的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白。
看看“爱丽丝”文档:  < a  href = "http://example.com/elsie"  class = "sister"  id = "link1" > Elsie </ a >  < a  href = "http://example.com/lacie"  class = "sister"  id = "link2" > Lacie </ a >  < a  href = "http://example.com/tillie"  class = "sister"  id = "link3" > Tillie </ a >    如果以为第一个 <a> 标签的 .next_sibling 结果是第二个 <a> 标签，那就错了，
真实结果是第一个 <a> 标签和第二个<a> 标签之间的顿号和换行符:  link  =  soup .
next_sibling  # u',\n'    第二个<a>标签是顿号的 .next_sibling 属性:  link .
next_sibling  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>      .next_siblings 和 .previous_siblings ¶  通过 .next_siblings 和 .previous_siblings 属性可以对当前节点的兄弟节点迭代输出:  for  sibling  in  soup .
previous_siblings :  print ( repr ( sibling ))  # ' and\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # ',\n'  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  # 'Once upon a time there were three little sisters; and their names were\n'       回退和前进 ¶  看一下“爱丽丝” 文档:  < html >< head >< title > The  Dormouse 's story</title></head>  < p  class = "title" >< b > The  Dormouse 's story</b></p>    HTML解析器把这段字符串转换成一连串的事件: “打开<html>标签”,”打开一个<head>标签”,
“打开一个<title>标签”,”添加一段字符串”,”关闭<title>标签”,”打开<p>标签”,等等。
Beautiful Soup提供了重现解析器初始化过程的方法。   .next_element 和 .previous_element ¶  .next_element 属性指向解析过程中下一个被解析的对象(字符串或tag),
结果可能与 .next_sibling 相同，但通常是不一样的。  这是“爱丽丝”文档中最后一个 <a> 标签，它的 .next_sibling 结果是一个字符串，
因为当前的解析过程 [ 2 ] 因为当前的解析过程因为遇到了<a>标签而中断了:  last_a_tag  =  soup .
next_sibling  # '; and they lived at the bottom of a well.'
但这个 <a> 标签的 .next_element 属性结果是在 <a> 标签被解析之后的解析内容，
不是 <a> 标签后的句子部分，而是字符串 “Tillie”:  last_a_tag .
next_element  # u'Tillie'    这是因为在原始文档中，字符串 “Tillie” 在分号前出现，解析器先进入 <a> 标签，
然后是字符串 “Tillie”，然后关闭 </a> 标签，然后是分号和剩余部分。
分号与 <a> 标签在同一层级，但是字符串 “Tillie” 会先被解析。  .previous_element 属性刚好与 .next_element 相反，
它指向当前被解析的对象的前一个解析对象:  last_a_tag .
previous_element  # u' and\n'  last_a_tag .
next_element  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>      .next_elements 和 .previous_elements ¶  通过 .next_elements 和 .previous_elements 的迭代器就可以向前或向后
访问文档的解析内容，就好像文档正在被解析一样:  for  element  in  last_a_tag .
next_elements :  print ( repr ( element ))  # u'Tillie'  # u';\nand they lived at the bottom of a well.'
# u'\n\n'  # <p class="story">...</p>  # u'...'  # u'\n'  # None        搜索文档树 ¶  Beautiful Soup 定义了很多相似的文档搜索方法，这里着重介绍2个: find() 和 find_all() ，
其它方法的参数和用法类似，所以一笔带过。  再以“爱丽丝”文档作为例子:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    使用 find_all() 这种过滤方法，就可以检索想要查找的文档内容。   过滤器类型 ¶  介绍 find_all() 或类似方法前，先介绍一下这些方法可以使用哪些过滤器的类型 [ 3 ] ,
这些过滤器在搜索的 API 中反复出现。过滤器可以作用在 tag 的 name 上，节点的属性上，
字符串上或与他们混合使用。   字符串 ¶  最简单的过滤器是字符串。在搜索方法中传入一个字符串参数，Beautiful Soup
会查找与字符串完整匹配的内容，下面的例子用于查找文档中所有的 <b> 标签:  soup .
find_all ( 'b' )  # [<b>The Dormouse's story</b>]    如果传入字节码参数，Beautiful Soup会当作UTF-8编码，可以传入一段Unicode 编码来避免
Beautiful Soup 解析编码出错。    正则表达式 ¶  如果传入正则表达式作为参数，Beautiful Soup 会通过正则表达式的 match() 来匹配内容。
下面例子中找出所有以 b 开头的标签，这种情况下 <body> 和 <b> 标签都会被找到:  import  re  for  tag  in  soup .
name )  # body  # b    下面代码找出所有名字中包含 “t” 的标签:  for  tag  in  soup .
name )  # html  # title      列表 ¶  如果传入列表参数，Beautiful Soup会 将与列表中任一元素匹配的内容返回。
下面代码找到文档中所有 <a> 标签和 <b> 标签:  soup .
find_all ([ "a" ,  "b" ])  # [<b>The Dormouse's story</b>,  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      True ¶  True 可以匹配任何值，下面代码查找到所有的 tag，但是不会返回字符串节点  for  tag  in  soup .
name )  # html  # head  # title  # body  # p  # b  # p  # a  # a  # a  # p      函数 ¶  如果没有合适过滤器，那么还可以定义一个函数方法，参数是一个元素 [ 4 ] ，
如果这个方法返回 True 表示当前元素匹配并且被找到，如果不是则反回 False 。  下面方法实现的匹配功能是，如果包含 class 属性却不包含 id 属性，
那么将返回 True :  def  has_class_but_no_id ( tag ):  return  tag .
has_attr ( 'id' )    将这个方法作为参数传入 find_all() 方法，将得到所有 <p> 标签:  soup .
find_all ( has_class_but_no_id )  # [<p class="title"><b>The Dormouse's story</b></p>,  #  <p class="story">Once upon a time there were...</p>,  #  <p class="story">...</p>]    返回结果中只有 <p> 标签，没有 <a> 标签，因为 <a> 标签还定义了”id”，
没有返回 <html> 和 <head>，因为 <html> 和 <head> 中没有定义 “class” 属性。  如果通过方法来筛选特殊属性，比如 href ，传入方法的参数应该是对应属性的值，
而不是整个元素。下面的例子是找出那些 a 标签中的 href 属性不匹配指定正则:  def  not_lacie ( href ):  return  href  and  not  re .
find_all ( href = not_lacie )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    标签过滤方法可以使用复杂方法。下面的例子可以过滤出前后都有文字的标签。  from  bs4  import  NavigableString  def  surrounded_by_strings ( tag ):  return  ( isinstance ( tag .
find_all ( surrounded_by_strings ):  print  tag .
name  # p  # a  # a  # a  # p    现在来了解一下搜索方法的细节     find_all() ¶  find_all( name , attrs , recursive , string , **kwargs )  find_all() 方法搜索当前 tag 的所有子节点，并判断是否符合过滤器的条件。 过滤器类型 中已经举过几个例子，这里再展示几个新例子:  soup .
compile ( "sisters" ))  # u'Once upon a time there were three little sisters; and their names were\n'    有几个方法很相似，还有几个方法是新的，参数中的 string 和 id 是什么含义?
为什么 find_all("p",  "title") 返回的是CSS Class为”title”的<p>标签?
我们来仔细看一下 find_all() 的参数   name 参数 ¶  传一个值给 name 参数，就可以查找所有名字为 name 的 tag。所有文本都会被忽略掉，
因为它们不匹配标签名字。  简单的用法如下:  soup .
find_all ( "title" )  # [<title>The Dormouse's story</title>]    回忆 过滤器类型 中描述的内容，搜索 name 的参数值可以是：
字符串、正则表达式、列表、方法或是 True 。    keyword 参数 ¶  如果动态参数中出现未能识别的参数名，搜索时会把该参数当作 tag 属性来搜索，
比如搜索参数中包含一个名字为 id 的参数，Beautiful Soup 会搜索每个
tag 上的 id 属性  soup .
find_all ( id = 'link2' )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    如果传入 href 参数，Beautiful Soup会搜索每个 tag 的 href 属性  soup .
compile ( "elsie" ))  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    搜索指定名字的属性时可以使用的参数值包括 字符串 , 正则表达式 , 列表 , True .
下面的例子在文档树中查找所有包含 id 属性的 tag，无论 id 的值是什么:  soup .
find_all ( id = True )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    使用多个指定名字的参数可以同时过滤多个 tag 属性:  soup .
compile ( "elsie" ),  id = 'link1' )  # [<a class="sister" href="http://example.com/elsie" id="link1">three</a>]    有些 tag 属性在搜索不能使用，比如HTML5中的 data-* 属性:  data_soup  =  BeautifulSoup ( '<div data-foo="value">foo!</div>' )  data_soup .
find_all ( data - foo = "value" )  # SyntaxError: keyword can't be an expression    这种情况下可以通过 find_all() 方法的 attrs 参数定义一个字典参数
来搜索包含特殊属性的 tag:  data_soup .
find_all ( attrs = { "data-foo" :  "value" })  # [<div data-foo="value">foo!</div>]    不要使用 “name” 作为关键字参数搜索 HTML 元素，因为 Beautiful Soup 用 name 来识别 tag 本身的名字。换一种方法，你可以这样搜索属性中的 “name” 值  name_soup  =  BeautifulSoup ( '<input name="email"/>' ,  'html.parser' )  name_soup .
find_all ( attrs = { "name" :  "email" })  # [<input name="email"/>]      按CSS搜索 ¶  按照 CSS 类名搜索的功能非常实用，但标识 CSS 类名的关键字 class 在Python中是保留字，
使用 class 做参数会导致语法错误。从 Beautiful Soup 4.1.2 版本开始，可以通过 class_ 参数搜索有指定CSS类名的 tag:  soup .
find_all ( "a" ,  class_ = "sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    作为关键字形式的参数 class_ 同样接受不同类型的 过滤器 ，字符串、正则表达式、
方法或 True :  soup .
find_all ( class_ = has_six_characters )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    tag 的 class 属性是 多值属性 。按照 CSS 类名搜索时，表示匹配到 tag 中任意 CSS 类名:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' )  css_soup .
find_all ( "p" ,  class_ = "body" )  # [<p class="body strikeout"></p>]    搜索 class 属性时也可以通过 CSS 值进行完全匹配:  css_soup .
find_all ( "p" ,  class_ = "body strikeout" )  # [<p class="body strikeout"></p>]    完全匹配 class 的值时，如果CSS类名的顺序与实际不符，将搜索不到结果:  css_soup .
find_all ( "p" ,  class_ = "strikeout body" )  # []    如果想要通过多个 CSS 类型来搜索 tag，应该使用 CSS 选择器  css_soup .
select ( "p.strikeout.body" )  # [<p class="body strikeout"></p>]    在旧版本的 Beautiful Soup 中，可能不支持 class_ ，这时可以使用 attrs 实现相同效果。
创建一个字典，包含要搜索的 class 类名（或者正则表达式等形式）  soup .
find_all ( "a" ,  attrs = { "class" :  "sister" })  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      string 参数 ¶  通过 string 参数可以搜索文档中的字符串内容。与 name 参数接受的值一样， string 参数接受 字符串 , 正则表达式 , 列表 , 函数 , True 。看例子:  soup .
find_all ( string = "Elsie" )  # [u'Elsie']  soup .
find_all ( string = [ "Tillie" ,  "Elsie" ,  "Lacie" ])  # [u'Elsie', u'Lacie', u'Tillie']  soup .
compile ( "Dormouse" ))  [ u "The Dormouse's story" ,  u "The Dormouse's story" ]  def  is_the_only_string_within_a_tag ( s ):  "" Return  True  if  this  string  is  the  only  child  of  its  parent  tag .
""
find_all ( string = is_the_only_string_within_a_tag )  # [u"The Dormouse's story", u"The Dormouse's story", u'Elsie', u'Lacie', u'Tillie', u'...']    虽然 string 参数用于搜索字符串，同时也以与其它参数混合使用来搜索 tag。
Beautiful Soup 会过滤那些 string 值与 .string 参数相符的 tag。
下面代码用来搜索内容里面包含 “Elsie” 的 <a> 标签:  soup .
find_all ( "a" ,  string = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]    string 参数是在 4.4.0 中新增的。早期版本中该参数名为 text 。  soup .
find_all ( "a" ,  text = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]      limit 参数 ¶  find_all() 方法会返回全部的搜索结构，如果文档树很大那么搜索会很慢。
如果我们不需要全部结果，可以使用 limit 参数限制返回结果的数量。
效果与SQL中的limit关键字类似，当搜索到的结果数量达到 limit 的限制时，
就停止搜索返回结果。  “爱丽丝”文档例子中有 3 个 tag 符合搜索条件，但下面例子中的结果只返回了 2 个，
因为我们限制了返回数量:  soup .
find_all ( "a" ,  limit = 2 )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]      recursive 参数 ¶  如果调用 mytag.find_all() 方法，Beautiful Soup 会检索 mytag 的所有子孙节点，
如果只想搜索直接子节点，可以使用参数 recursive=False 。查看下面例子  soup .
find_all ( "title" ,  recursive = False )  # []    下面一段简单的文档:  < html >  < head >  < title >  The  Dormouse 's story  </ title >  </ head >  ...
   <title> 标签在 <html> 标签之下，但并不是直接子节点，<head> 标签才是直接子节点。
在允许查询所有后代节点时 Beautiful Soup 能够查找到 <title> 标签。
但是使用了 recursive=False 参数之后，只能查找直接子节点，这样就查不到 <title> 标签了。  Beautiful Soup 提供了多种 DOM 树搜索方法。这些方法都使用了类似的参数定义。
比如这些方法: find_all() : name , attrs , text , limit .
但是只有 find_all() 和 find() 支持 recursive 参数。     像调用 find_all() 一样调用tag ¶  find_all() 几乎是 Beautiful Soup 中最常用的搜索方法，所以我们定义了它的简写方法。 BeautifulSoup 对象和 Tag 对象可以被当作一个方法来使用，这个方法的执行结果与
调用这个对象的 find_all() 方法相同，下面两行代码是等价的:  soup .
find_all ( "a" )  soup ( "a" )    这两行代码也是等价的:  soup .
title ( string = True )      find() ¶  find( name , attrs , recursive , string , **kwargs )  find_all() 方法将返回文档中符合条件的所有 tag，尽管有时候我们只想得到一个结果。
比如文档中只有一个 <body> 标签，那么使用 find_all() 方法来查找 <body> 标签就
不太合适，使用 find_all 方法并设置 limit=1 参数不如直接使用 find() 方法。
下面两行代码是等价的:  soup .
find ( 'title' )  # <title>The Dormouse's story</title>    唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表，而 find() 方法
直接返回结果。  find_all() 方法没有找到目标是返回空列表， find() 方法找不到目标时，返回 None 。  print ( soup .
find ( "nosuchtag" ))  # None    soup.head.title 是 Tag 的名字 方法的简写。这个简写就是通过多次调用 find() 方
法实现的:  soup .
find ( "title" )  # <title>The Dormouse's story</title>      find_parents() 和 find_parent() ¶  find_parents( name , attrs , recursive , string , **kwargs )  find_parent( name , attrs , recursive , string , **kwargs )  我们已经用了很大篇幅来介绍 find_all() 和 find() 方法，Beautiful Soup 中
还有 10 个用于搜索的 API。它们中有 5 个用的是与 find_all() 相同的搜索参数，
另外 5 个与 find() 方法的搜索参数类似。区别仅是它们搜索文档的位置不同。  首先来看看 find_parents() 和 find_parent() 。
记住: find_all() 和 find() 只搜索当前节点的所有子节点，孙子节点等。
而这 2 个方法刚好相反，它们用来搜索当前节点的父辈节点。
我们来试试看，从例子文档中的一个深层叶子节点开始:  a_string  =  soup .
find ( string = "Lacie" )  a_string  # u'Lacie'  a_string .
find_parents ( "p" ,  class = "title" )  # []    文档中的一个 <a> 标签是是当前叶子节点的直接父节点，所以可以被找到。
还有一个 <p> 标签，是目标叶子节点的间接父辈节点，所以也可以被找到。
包含 class 值为 “title” 的 <p> 标签不是不是目标叶子节点的父辈节点，
所以通过 find_parents() 方法搜索不到。  find_parent() 和 find_parents() 方法会让人联想到 .parent 和 .parents 属性。
它们之间的联系非常紧密。搜索父辈节点的方法实际上就是对 .parents 属性的迭代搜索。    find_next_siblings() 和 find_next_sibling() ¶  find_next_siblings( name , attrs , recursive , string , **kwargs )  find_next_sibling( name , attrs , recursive , string , **kwargs )  这 2 个方法通过 .next_siblings 属性对当 tag 的所有后面解析 [ 5 ] 的兄弟tag节点进行迭代， find_next_siblings() 方法返回所有符合条件的后面的兄弟节点， find_next_sibling() 只返回符合条件的后面的第一个 tag 节点。  first_link  =  soup .
find_next_sibling ( "p" )  # <p class="story">...</p>      find_previous_siblings() 和 find_previous_sibling() ¶  find_previous_siblings( name , attrs , recursive , string , **kwargs )  find_previous_sibling( name , attrs , recursive , string , **kwargs )  这 2 个方法通过 .previous_siblings 属性对当前 tag 的前面解析 [ 5 ] 的兄弟 tag 节点进行迭代， find_previous_siblings() 方法返回所有符合条件的前面的兄弟节点， find_previous_sibling() 方法返回第一个符合条件的前面的兄弟节点:  last_link  =  soup .
find_previous_sibling ( "p" )  # <p class="title"><b>The Dormouse's story</b></p>      find_all_next() 和 find_next() ¶  find_all_next( name , attrs , recursive , string , **kwargs )  find_next( name , attrs , recursive , string , **kwargs )  这 2 个方法通过 .next_elements 属性对当前 tag 的之后的 [ 5 ] tag 和字符串进行迭代， find_all_next() 方法返回所有符合条件的节点， find_next() 方法返回第一个符合条件的节点:  first_link  =  soup .
find_all_next ( string = True )  # [u'Elsie', u',\n', u'Lacie', u' and\n', u'Tillie',  #  u';\nand they lived at the bottom of a well.
', u'\n\n', u'...', u'\n']  first_link .
find_next ( "p" )  # <p class="story">...</p>    第一个例子中，字符串 “Elsie”也被显示出来，尽管它被包含在我们开始查找的 <a> 标签的里面。
第二个例子中，最后一个<p>标签也被显示出来，尽管它与我们开始查找位置的 <a> 标签不属于同一部分。
例子中，搜索的重点是要匹配过滤器的条件，以及元素在文档中出现的顺序要在查找的元素的之后。    find_all_previous() 和 find_previous() ¶  find_all_previous( name , attrs , recursive , string , **kwargs )  find_previous( name , attrs , recursive , string , **kwargs )  这 2 个方法通过 .previous_elements 属性对当前节点前面 [ 5 ] 的
tag 和字符串进行迭代， find_all_previous() 方法返回所有符合条件的节点， find_previous() 方法返回第一个符合条件的节点。  first_link  =  soup .
find_previous ( "title" )  # <title>The Dormouse's story</title>    find_all_previous("p") 既返回了文档中的第一段(class=”title”的那段)，还返回了第二段，
包含了我们开始查找的 <a> 标签的那段。不用惊讶，这段代码的功能是查找所有出现在指定 <a> 标签之前
的 <p> 标签，因为这个 <p> 标签包含了开始的 <a> 标签，所以 <p> 标签当然是在 <a> 之前出现的。    CSS 选择器 ¶  BeautifulSoup 对象和 Tag 对象支持通过 .css 属性实现 CSS 选择器。具体选择功能是通过 Soup Sieve 库实现的，在 PyPI 上通
过关键字 soupsieve 可以找到。通过 pip 安装 Beautiful Soup 时，Soup Sieve 也会自
动安装，不用其它额外操作。  Soup Sieve 文档列出了 当前支持的 CSS 选择器 ，
下面是一些基本应用  soup .
select ( "p:nth-of-type(3)" )  # [<p class="story">...</p>]    查找指定层级的 tag:  soup .
select ( "html head title" )  # [<title>The Dormouse's story</title>]    找到某个 tag 标签下的直接子标签 [ 6 ] :  soup .
select ( "p > #link1" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]  soup .
select ( "body > a" )  # []    找到兄弟节点标签:  soup .
select ( "#link1 ~ .sister" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie"  id="link3">Tillie</a>]  soup .
select ( "#link1 + .sister" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    通过 CSS 的类名查找:  soup .
select ( "[class~=sister]" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    通过 id 查找 tag:  soup .
select ( "a#link2" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    查找符合列表中任意一个选择器的 tag：  soup .
select ( "#link1,#link2" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    通过是否存在某个属性来查找:  soup .
select ( 'a[href]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    通过属性的值来查找:  soup .
select ( 'a[href*=".com/el"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    还有一个 select_one() 方法，它会返回符合筛选条件的元素列表中的第一个  soup .
select_one ( ".sister" )  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    为了方便使用，在 BeautifulSoup 或 Tag 对象上直接调用 select() 和 select_one() 方法，
中间省略 .css 属性  soup .
select_one ( ".sister" )  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    CSS 选择器对于熟悉 CSS 语法的人来说非常方便。你可以在 Beautiful Soup 中使用相同的方法。
但是如果你只需要使用 CSS 选择器就够了，那么应该 lxml 作为文档解析器：因为速度快很多。
但是 Soup Sieve 也有优势，它允许 组合 使用 CSS 选择器和 Beautiful Soup 的 API。    Soup Sieve 高级特性 ¶  Soup Sieve 提供的是比 select() 和 select_one() 更底层的方法，通过 Tag 或
Beautiful Soup 对象的 .css 属性，可以调用大部分的 API。下面是支持这种调用方式的方法列表，
查看 Soup Sieve 文档了解全部细节。  iselect() 方法与 select() 效果相同，区别是返回的结果是迭代器。  [ tag [ 'id' ]  for  tag  in  soup .
iselect ( ".sister" )]  # ['link1', 'link2', 'link3']    closest() 方法与 find_parent() 方法相似，返回符合 CSS 选择器的 Tag 对象的最近父级。  elsie  =  soup .
closest ( "p.story" )  # <p class="story">Once upon a time there were three little sisters; and their names were  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;  #  and they lived at the bottom of a well.</p>    match() 方法返回布尔结果，标记指定 Tag 是否符合指定筛选器  # elsie.css.match("#link1")  True  # elsie.css.match("#link2")  False    filter() 方法返回 tag 直接子节点中符合筛选器的节点列表  [ tag .
filter ( 'a' )]  # ['Elsie', 'Lacie', 'Tillie']    escape() 方法可以对 CSS 标识符中的特殊字符进行转义，否则是非法 CSS 标识符  soup .
escape ( "1-strange-identifier" )  # '\\31 -strange-identifier'      CSS 筛选器中的命名空间 ¶  如果解析的 XML 文档中定义了命名空间，那么 CSS 筛选器中也可以使用  from  bs4  import  BeautifulSoup  xml  =  """<tag xmlns:ns1="http://namespace1/" xmlns:ns2="http://namespace2/">  <ns1:child>I'm in namespace 1</ns1:child>  <ns2:child>I'm in namespace 2</ns2:child>  </tag> """  namespace_soup  =  BeautifulSoup ( xml ,  "xml" )  namespace_soup .
select ( "ns1|child" )  # [<ns1:child>I'm in namespace 1</ns1:child>]    Beautiful Soup 尝试自动匹配解析文档中的命名空间前缀，除此之外，你还可以自定义目录的缩写  namespaces  =  dict ( first = "http://namespace1/" ,  second = "http://namespace2/" )  namespace_soup .
select ( "second|child" ,  namespaces = namespaces )  # [<ns1:child>I'm in namespace 2</ns1:child>]      支持 CSS 筛选器的历史版本 ¶   .css 属性是在 Beautiful Soup 4.12.0 中添加的。在此之前，只能使用 .select() 和 .select_one() 方法。    Soup Sieve 是在 Beautiful Soup 4.7.0 开始集成的。早期版本中有 .select() 方法，但
仅能支持最常用的 CSS 选择器。     修改文档树 ¶  Beautiful Soup 的强项是文档树的搜索，但也支持修改文档数，或者编写新的 HTML、XML 文档。   修改 tag 的名称和属性 ¶  在 Tag.attrs 的章节中已经介绍过这个功能，但是再看一遍也无妨。重命名一个 tag,
改变属性的值，添加或删除属性  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
name  =  "blockquote"  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>      修改 .string ¶  如果设置 tag 的 .string 属性值，就相当于用新的内容替代了原来的内容:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
tag  # <a href="http://example.com/">New link text.</a>    注意：如果 tag 原本包含了其它子节点，原有的所有内容包括子 tag 都会被覆盖掉。    append() ¶  向 tag 中添加内容可以使用 Tag.append() 方法，就好像调用 Python 列表的 .append() 方法:  soup  =  BeautifulSoup ( "<a>Foo</a>" ,  'html.parser' )  soup .
contents  # ['Foo', 'Bar']      extend() ¶  从 Beautiful Soup 4.7.0 版本开始，tag 增加了 .extend() 方法，可以把一个列表中内容，
按顺序全部添加到一个 tag 当中  soup  =  BeautifulSoup ( "<a>Soup</a>" ,  'html.parser' )  soup .
contents  # ['Soup', ''s', ' ', 'on']      NavigableString() 和 .new_tag() ¶  如果想添加一段文本内容到文档中，可以将一个 Python 字符串对象传给 append() 方法，
或调用 NavigableString 构造方法:  from  bs4  import  NavigableString  soup  =  BeautifulSoup ( "<b></b>" ,  'html.parser' )  tag  =  soup .
contents  # ['Hello', ' there']    如果想要创建一段注释，或其它 NavigableString 的子类，只要调用构造方法:  from  bs4  import  Comment  new_comment  =  Comment ( "Nice to see you."
(这是 Beautiful Soup 4.4.0 中新增的方法)  如果需要新创建一个 tag，最好的方法是调用工厂方法 BeautifulSoup.new_tag()  soup  =  BeautifulSoup ( "<b></b>" ,  'html.parser' )  original_tag  =  soup .
original_tag  # <b><a href="http://www.example.com">Link text.</a></b>    只有第一个参数用作 tag 的 name，是必填的。    insert() ¶  Tag.insert() 方法与 Tag.append() 方法类似，区别是不会把新元素添加到
父节点 .contents 属性的最后。而是把元素插入到按顺序指定的位置。与 Python 列表
中的 .insert() 方法的用法相同  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
contents  # ['I linked to ', 'but did not endorse', <i>example.com</i>]      insert_before() 和 insert_after() ¶  insert_before() 方法可以在文档树中直接在目标之前添加 tag 或文本  soup  =  BeautifulSoup ( "<b>leave</b>" ,  'html.parser' )  tag  =  soup .
b  # <b><i>Don't</i>leave</b>    insert_after() 方法可以在文档树中直接在目标之后添加 tag 或文本  div  =  soup .
contents  # [<i>Don't</i>, ' you', <div>ever</div>, 'leave']      clear() ¶  Tag.clear() 方法可以移除 tag 的内容:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
clear ()  tag  # <a href="http://example.com/"></a>      extract() ¶  PageElement.extract() 方法将当前 tag 或文本从文档树中移除，并返回被删除的内容:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
parent )  # None    这个方法实际上产生了 2 个文档树: 一个是原始文档的 BeautifulSoup 对象，
另一个是被移除并且返回的文档树。还可以在新生成的文档树上继续调用 extract 方法:  my_string  =  i_tag .
parent )  # None  i_tag  # <i></i>      decompose() ¶  Tag.decompose() 方法会将前节点从文档书中移除并完全销毁:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
decompose ()  a_tag  # <a href="http://example.com/">I linked to</a>    被 decompose 的 Tag 或者 NavigableString 是不稳定的，什么时候都不要使用它。如果不确定
某些内容是否被 decompose 了，可以通过 .decomposed 属性进行检查 (Beautiful Soup 4.9.0 新增)  i_tag .
decomposed  # False      replace_with() ¶  PageElement.replace_with() 方法移除文档树中的某段内容，并用新 tag 或文本节点替代它:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
,  i_tag )  a_tag  # <a href="http://example.com/">I linked to <b>example</b>.<i>net</i></a>    replace_with() 方法返回被替代的 tag 或文本节点，可以用来检查或添加到文档树其它地方。  传递多个参数给 replace_with() 方法在 Beautiful Soup 4.10.0 版本中新增    wrap() ¶  PageElement.wrap() 方法可以对指定的tag元素进行包装 [ 8 ] ，并返回包装后的结果:  soup  =  BeautifulSoup ( "<p>I wish I was bold.</p>" ,  'html.parser' )  soup .
new_tag ( "div" ))  # <div><p><b>I wish I was bold.</b></p></div>    该方法在 Beautiful Soup 4.0.5 中添加。    unwrap() ¶  Tag.unwrap() 方法与 wrap() 方法相反。它将用 tag 内内容来替换 tag 本身，
该方法常被用来解包内容:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
unwrap ()  a_tag  # <a href="http://example.com/">I linked to example.com</a>    与 replace_with() 方法相同， unwrap() 方法会返回被移除的 tag。    smooth() ¶  调用了一堆修改文档树的方法后，可能剩下的是 2 个或更多个彼此衔接的 NavigableString 对象。
Beautiful Soup 处理起来没有问题，但在刚刚解析的文档树中，可能会出现非预期情况  soup  =  BeautifulSoup ( "<p>A one</p>" ,  'html.parser' )  soup .
prettify ())  # <p>  #  A one  #  , a two  # </p>    这时可以使用 Tag.smooth() 方法来清理文档树，把相邻的字符串平滑的链接到一起  soup .
prettify ())  # <p>  #  A one, a two  # </p>    该方法在 Beautiful Soup 4.8.0 中添加。     输出 ¶   格式化输出 ¶  prettify() 方法将 Beautiful Soup 的文档树格式化后以 Unicode 编码输出，
每个 XML/HTML 标签都独占一行  markup  =  '<html><head><body><a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
prettify ())  # <html>  #  <head>  #  </head>  #  <body>  #   <a href="http://example.com/">  #    I linked to  #    <i>  #     example.com  #    </i>  #   </a>  #  </body>  # </html>    BeautifulSoup 对象的根节点和它的所有 tag 节点都可以调用 prettify() 方法:  print ( soup .
prettify ())  # <a href="http://example.com/">  #  I linked to  #  <i>  #   example.com  #  </i>  # </a>    因为格式化会添加额外的空格（为了换行显示），因为 prettify() 会改变 HTML 文档的内容，
所以不要用来格式化文档。 prettify() 方法的设计目标是为了帮助更好的显示和理解文档。    压缩输出 ¶  如果只想得到结果字符串，不重视格式，那么可以对一个 BeautifulSoup 对象或 Tag 对象
使用 Python 的 unicode() 或 str() 方法:  str ( soup )  # '<html><head></head><body><a href="http://example.com/">I linked to <i>example.com</i></a></body></html>'  str ( soup .
a )  # '<a href="http://example.com/">I linked to <i>example.com</i></a>'    str() 方法返回 UTF-8 编码的字符串，查看定 编码 了解更多选项。  还可以调用 encode() 方法获得字节码或调用 decode() 方法获得Unicode。    输出格式 ¶  Beautiful Soup 输出是会将 HTML 中的特殊字符编码转换成 Unicode, 比如 “&lquot;”:  soup  =  BeautifulSoup ( "&ldquo;Dammit!&rdquo; he said."
如果将文档转换成字节编码，那么字节码 Unicode 会被编码成 UTF-8。并且无法再转换回 html 中的特殊字符编码:  soup .
默认情况下，只会转义 & 符号和尖角号。它们会被转义为 “&amp;”，”&lt;” 和 “&gt;”，因此 Beautiful Soup
不会无意间生成错误格式的的 HTML 或 XML  soup  =  BeautifulSoup ( "<p>The law firm of Dewey, Cheatem, & Howe</p>" ,  'html.parser' )  soup .
a  # <a href="http://example.com/?foo=val1&amp;bar=val2">A link</a>    修改默认转义规则的方法是，设置 prettify() , encode() , 或 decode() 方法的 formatter 参数。Beautiful Soup 可以识别 5 种 formatter 值。  默认的设置是 formatter="minimal" 。处置字符串时 Beautiful Soup 会确保生成合法的 HTML/XML  french  =  "<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>"  soup  =  BeautifulSoup ( french ,  'html.parser' )  print ( soup .
prettify ( formatter = "minimal" ))  # <p>  #  Il a dit &lt;&lt;Sacré bleu!&gt;&gt;  # </p>    设置为 formatter="html" 时，Beautiful Soup 会尽可能把 Unicode 字符转换为 HTML 实体  print ( soup .
prettify ( formatter = "html" ))  # <p>  #  Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;  # </p>    设置为 formatter="html5" 时，结果与 formatter="html" 相似，区别是 Beautiful Soup
会忽略 HTML 标签种空标签里的斜杠符号，比如 “br” 标签  br  =  BeautifulSoup ( "<br>" ,  'html.parser' ) .
encode ( formatter = "html5" ))  # b'<br>'    另外，如果属性的值为空字符串的，它会变为 HTML 风格的 boolean 属性  option  =  BeautifulSoup ( '<option selected=""></option>' ) .
encode ( formatter = "html5" ))  # b'<option selected></option>'    这种机制在 Beautiful Soup 4.10.0 中添加。  设置为 formatter=None 时，Beautiful Soup 在输出时不会修改任何字符串内容。这是效率最高的选项，
但可能导致输出非法的 HTML/XML，比如下面例子  print ( soup .
encode ( formatter = None ))  # b'<a href="http://example.com/?foo=val1&bar=val2">A link</a>'      格式化对象 ¶  如果需要更复杂的机制来控制输出内容，可以实例化 Beautiful Soup 的 formatter 实例，
然后用作 formatter 参数。    class  bs4.
HTMLFormatter ¶   可以用来自定义 HTML 文档的格式化规则。  下面的 formatter 例子，可以将字符串全部转化为大写，不论是文字节点中的字符还是属性值  from  bs4.formatter  import  HTMLFormatter  def  uppercase ( str ):  return  str .
prettify ( formatter = formatter ))  # <a href="HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2">  #  A LINK  # </a>    下面的 formatter 例子，在美化文档时增加缩进长度  formatter  =  HTMLFormatter ( indent = 8 )  print ( link_soup .
XMLFormatter ¶   可以用来自定义 XML 文档的格式化规则。    编写自定义 formatter ¶  HTMLFormatter or XMLFormatter 的子类可以控制更多的输出过程。
例如，Beautiful Soup 默认情况下会对属性中的 tag 进行排序  attr_soup  =  BeautifulSoup ( b '<p z="1" m="2" a="3"></p>' ,  'html.parser' )  print ( attr_soup .
encode ())  # <p a="3" m="2" z="1"></p>    若想关闭这个功能，可以使用子类的 Formatter.attributes() 方法，该方法可以控制输出那些属性
以及这些属性的输出顺序。下面的例子会过滤掉文档中的 “m” 属性  class  UnsortedAttributes ( HTMLFormatter ):  def  attributes ( self ,  tag ):  for  k ,  v  in  tag .
encode ( formatter = UnsortedAttributes ()))  # <p z="1" a="3"></p>    危险提示：如果创建了 CData 对象，对象中的字符串对象始终表示原始内容，不会被格式化方法影响。
Beautiful Soup 输出时依然会调用自定义格式化方法，以防自定义方法中包含自定义的字符串计数方法，
但调用后不会使用返回结果，不影响原来的返回值。  from  bs4.element  import  CData  soup  =  BeautifulSoup ( "<a></a>" ,  'html.parser' )  soup .
[CDATA[one < three]]>  # </a>      get_text() ¶  如果只想得到 tag 中包含的文本内容，那么可以调用 get_text() 方法，这个方法获取到 tag
包含的所有文本内容，包括子孙 tag 中的可读内容，并将结果作为单独的一个 Unicode 编码字符串返回:  markup  =  '<a href="http://example.com/"> \n I linked to <i>example.com</i> \n </a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
get_text ()  'example.com''    可以通过参数指定 tag 的文本内容的连接符:  # soup.get_text("|")  ' \n I linked to |example.com| \n '    还可以去除每一个文本片段内容的前后空白:  # soup.get_text("|", strip=True)  'I linked to|example.com'    但这种情况，你可能应该使用 .stripped_strings 生成器，
获得文本列表后手动处理内容:  [ text  for  text  in  soup .
stripped_strings ]  # ['I linked to', 'example.com']    因为 Beautiful Soup 4.9.0 版本开始使用 lxml 或 html.parser，<script>，<style> 和
<template> 标签中的内容不会被当做普通的 ‘文本’ 来处理，因此这些标签中的内容不会算作页面中的
可读内容的一部分。  Beautiful Soup 4.10.0 版本以后，可以在 NavigableString 对象上调用 get_text()，.strings
或 .stripped_strings 属性，结果会返回对象本身或空，这种用法只有在对混合类型列表迭代时才会用到。     指定文档解析器 ¶  如果仅是想要解析HTML文档，只需要创建 BeautifulSoup 对象时传入文档就可以了。Beautiful Soup
会自动选择一个解析器来解析文档。同时还可以使用额外参数，来指定文档解析器。  BeautifulSoup 第一个参数应该是要被解析的文档字符串或是文件句柄 – 待解析文件的句柄，
第二个参数用来标识怎样解析文档。  如果不指定解析器，默认使用已安装的 最佳 HTML 解析器。Beautiful Soup 把 lxml 解析器排在第一,
然后是 html5lib, 然后是 Python 标准库。在下面两种条件下解析器优先顺序会变化:    要解析的文档是什么类型: 目前支持， “html”，“xml”，和 “html5”  指定使用哪种解析器: 目前支持，“lxml”，“html5lib”，和 “html.parser”（Python 标准库）    安装解析器 章节介绍了可以使用哪种解析器，以及如何安装。  如果指定的解析器没有安装，Beautiful Soup会自动选择其它方案。目前只有 lxml 解析器支持XML文档的解析，
在没有安装 lxml 库的情况下，无法自动选择 XML 文档解析器，手动指定 lxml 也不行。   解析器之间的区别 ¶  Beautiful Soup 为不同的解析器提供了相同的接口，但解析器本身时有区别的。同一篇文档被不同的解析器解析后
可能会生成不同结构的文档。区别最大的是 HTML 解析器和 XML 解析器，看下面片段被解析成 HTML 结构:  BeautifulSoup ( "<a><b/></a>" ,  "html.parser" )  # <a><b></b></a>    因为空标签 <b /> 不符合 HTML 标准，html.parser 解析器把它解析成一对儿 <b></b>。  同样的文档使用 XML 解析结果如下(解析 XML 需要安装 lxml 库)。注意，空标签 <b /> 依然被保留，
并且文档前添加了 XML 头，而不是被包含在 <html> 标签内:  print ( BeautifulSoup ( "<a><b/></a>" ,  "xml" ))  # <?xml version="1.0" encoding="utf-8"?>  # <a><b/></a>    HTML 解析器之间也有区别，如果被解析的HTML文档是标准格式，那么解析器之间没有任何差别。
只是解析速度不同，结果都会返回正确的文档树。  但是如果被解析文档不是标准格式，那么不同的解析器返回结果可能不同。下面例子中，使用 lxml
解析错误格式的文档，结果 </p> 标签被直接忽略掉了:  BeautifulSoup ( "<a></p>" ,  "lxml" )  # <html><body><a></a></body></html>    使用 html5lib 库解析相同文档会得到不同的结果:  BeautifulSoup ( "<a></p>" ,  "html5lib" )  # <html><head></head><body><a><p></p></a></body></html>    html5lib 库没有忽略掉 </p> 标签，而是自动补全了标签，还给文档树添加了 <head> 标签。  使用 pyhton 内置库解析结果如下:  BeautifulSoup ( "<a></p>" ,  "html.parser" )  # <a></a>    与 lxml [ 7 ] 库类似的，Python 内置库忽略掉了 </p> 标签，与 html5lib 库不同的是标准库没有
尝试创建符合标准的文档格式或将文档片段包含在 <body> 标签内，与lxml不同的是标准库甚至连 <html>
标签都没有尝试去添加。  因为文档片段 “<a></p>” 是错误格式，所以以上解析方式都能算作 “正确”，html5lib 库使用的是 HTML5
的部分标准，所以最接近”正确”。不过所有解析器的结构都能够被认为是”正常”的。  不同的解析器可能影响代码执行结果，如果在分发给别人的代码中使用了 BeautifulSoup ,
那么最好注明使用了哪种解析器，以减少不必要的麻烦。     编码 ¶  任何 HTML 或 XML 文档都有自己的编码方式，比如ASCII 或 UTF-8。但是使用 Beautiful Soup 解析后，
文档都被转换成了 Unicode:  markup  =  "<h1>Sacr \xc3\xa9 bleu!</h1>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
这不是魔术(但很神奇)，Beautiful Soup 用了 编码自动检测 子库来识别当前
文档编码并转换成 Unicode 编码。 BeautifulSoup 对象的 .original_encoding 属性记录了
自动识别编码的结果:  soup .
original_encoding  'utf-8'    编码自动检测 功能大部分时候都能猜对编码格式，但有时候也会出错。有时候即使
猜测正确，也是在逐个 字节的遍历整个文档后才猜对的，这样很慢。如果预先知道文档编码，可以设置编码参数
来减少自动检查编码 出错的概率并且提高文档解析速度。在创建 BeautifulSoup 对象的时候设置 from_encoding 参数。  下面一段文档用了 ISO-8859-8 编码方式，这段文档太短，结果 Beautiful Soup 以为文档是用 ISO-8859-7 编码:  markup  =  b "<h1> \xed\xe5\xec\xf9 </h1>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  print ( soup .
original_encoding )  # iso-8859-7    通过传入 from_encoding 参数来指定编码方式:  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  from_encoding = "iso-8859-8" )  print ( soup .
original_encoding )  # iso8859-8    如果仅知道文档采用了 Unicode 编码，但不知道具体编码。可以先自己猜测，猜测错误(依旧是乱码)时，
可以把错误编码作为 exclude_encodings 参数，这样文档就不会尝试使用这种编码了解码了。  译者备注: 在没有指定编码的情况下，BS会自己猜测编码，把不正确的编码排除掉，BS就更容易猜到正确编码。  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  exclude_encodings = [ "iso-8859-7" ])  print ( soup .
original_encoding )  # WINDOWS-1255    猜测的结果 Windows-1255 可能不是 100% 准确，但是 Windows-1255 编码是 ISO-8859-8 的扩展集，
所以猜测结果已经十分接近了，并不影响使用。( exclude_encodings 参数是 4.4.0版本的新功能)  少数情况下(通常是UTF-8编码的文档中包含了其它编码格式的文件)，想获得正确的 Unicode 编码就不得不将
文档中少数特殊编码字符替换成特殊 Unicode 编码，“REPLACEMENT CHARACTER” (U+FFFD, �) [ 9 ] 。
如果 Beautifu Soup 猜测文档编码时作了特殊字符的替换，那么 Beautiful Soup 会把 UnicodeDammit 或 BeautifulSoup 对象的 .contains_replacement_characters 属性标记为 True 。
这样就可以知道当前文档进行 Unicode 编码后丢失了一部分特殊内容字符。如果文档中包含 � 而 .contains_replacement_characters 属性是 False ,则表示 � 就是文档中原来的字符，
不是转码失败。   输出编码 ¶  通过 Beautiful Soup 输出文档时，不管输入文档是什么编码方式，输出编码均为UTF-8编码，
下面例子输入文档是 Latin-1 编码:  markup  =  b '''  <html>  <head>  <meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" />  </head>  <body>  <p>Sacr \xe9 bleu!</p>  </body>  </html>  '''  soup  =  BeautifulSoup ( markup ,  'html.parser' )  print ( soup .
#   </p>  #  </body>  # </html>    注意，输出文档中的 <meta> 标签内容中的编码信息已经修改成了与输出编码一致的 UTF-8。  如果不想用 UTF-8 编码输出，可以将编码方式传入 prettify() 方法:  print ( soup .
prettify ( "latin-1" ))  # <html>  #  <head>  #   <meta content="text/html; charset=latin-1" http-equiv="Content-type" />  # ...
   还可以调用 BeautifulSoup 对象或任意节点的 encode() 方法，就像 Python 的字符串
调用 encode() 方法一样:  soup .
encode ( "utf-8" )  # b'<p>Sacr\xc3\xa9 bleu!</p>'    如果文档中包含当前编码不支持的字符，那么这些字符将被转换成一系列 XML 特殊字符引用，下面例子中
包含了 Unicode 编码字符 SNOWMAN:  markup  =  u "<b> \N{SNOWMAN} </b>"  snowman_soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  snowman_soup .
b    SNOWMAN 字符在 UTF-8 编码中可以正常显示(看上去是 ☃)，但有些编码不支持 SNOWMAN 字符，比如
ISO-Latin-1 或 ASCII，那么在这些编码中 SNOWMAN 字符会被转换成 “&#9731”:  print ( tag .
encode ( "ascii" ))  # b'<b>&#9731;</b>'      Unicode, Dammit ¶  译者备注: Unicode Dammit 是 Beautiful Soup 内置库，主要用来猜测文档编码。  编码自动检测 功能可以在 Beautiful Soup 以外使用。当遇到一段未知编码
的文档时，可以通过下面方法把它转换为 Unicode 编码  from  bs4  import  UnicodeDammit  dammit  =  UnicodeDammit ( b " \xc2\xab Sacr \xc3\xa9 bleu!
original_encoding  # 'utf-8'    如果安装了 Python 的 chardet 或 cchardet 库，那么编码检测功能的准确率将大大提高。
输入的字符越多，检测结果越准确，如果事先猜测到一些可能编码，那么可以将猜测的编码作为参数，
这样将优先检测这些编码:  dammit  =  UnicodeDammit ( "Sacr \xe9 bleu!"
original_encoding  # 'latin-1'    编码自动检测 功能中有 2 项功能是 Beautiful Soup 库中用不到的   智能引号 ¶  使用 Unicode 时，Beautiful Soup 还会智能的把引号 [ 10 ] 转换成 HTML 或 XML 中的特殊字符:  markup  =  b "<p>I just \x93 love \x94 Microsoft Word \x92 s smart quotes</p>"  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "html" ) .
unicode_markup  # '<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'    也可以把引号转换为 ASCII 码:  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "ascii" ) .
unicode_markup  # '<p>I just "love" Microsoft Word\'s smart quotes</p>'    虽然这个功能很有用，但是 Beautiful Soup 没有使用这种方式。默认情况下，Beautiful Soup
把引号转换成 Unicode:  UnicodeDammit ( markup ,  [ "windows-1252" ]) .
unicode_markup  # '<p>I just “love” Microsoft Word’s smart quotes</p>'      矛盾的编码 ¶  有时文档的大部分都是用 UTF-8，但同时还包含了 Windows-1252 编码的字符，就像微软的智能引号 [ 10 ] 一样。
一些包含多个信息的来源网站容易出现这种情况。 UnicodeDammit.detwingle() 方法可以把这类文档转换成纯
UTF-8 编码格式，看个简单的例子:  snowmen  =  ( u " \N{SNOWMAN} "  *  3 )  quote  =  ( u " \N{LEFT DOUBLE QUOTATION MARK} I like snowmen!
encode ( "windows_1252" )    这段文档很杂乱，snowmen 是 UTF-8 编码，引号是 Windows-1252 编码，直接输出时不能同时显示
snowmen 和引号，因为它们编码不同:  print ( doc )  # ☃☃☃�I like snowmen!�  print ( doc .
decode ( "windows-1252" ))  # â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”    如果对这段文档用 UTF-8 解码就会产生 UnicodeDecodeError 异常，如果用 Windows-1252
解码就会得到一堆乱码。幸好， UnicodeDammit.detwingle() 方法会把这段字符串转换成 UTF-8
编码，允许我们同时显示出文档中的 snowmen 和引号:  new_doc  =  UnicodeDammit .
decode ( "utf8" ))  # ☃☃☃“I like snowmen!”    UnicodeDammit.detwingle() 方法只能解码包含在 UTF-8 编码中的 Windows-1252 编码内容，
（反过来的话，大概也可以）但这是最常见的用法。  在创建 BeautifulSoup 或 UnicodeDammit 对象前一定要先对文档调用 UnicodeDammit.detwingle() 确保文档的编码方式正确。Beautiful Soup
会假设文档只包含一种编码，如果尝试去解析一段同时包含 UTF-8 和 Windows-1252 编码的文档，
就有可能被误判成整个文档都是 Windows-1252 编码，解析结果就会得到一堆乱码，
比如: â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”。  UnicodeDammit.detwingle() 方法在 Beautiful Soup 4.1.0 版本中新增。      行编号 ¶  html.parser 和 html5lib 解析器可以跟踪原始文档中发现的每个 Tag。查看原始信息可以
使用 Tag.sourceline （行号）和 Tag.sourcepos （标签所在行的起始位置）  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  for  tag  in  soup .
string )))  # (1, 0, 'Paragraph 1')  # (3, 4, 'Paragraph 2')    注意，这两个解析器的 sourceline 和 sourcepos 会有些许的不同。html.parser 将
标签开始的 小于号作为标签起始符号，而 html5lib 将标签开始的大于号作为标签起始符号  soup  =  BeautifulSoup ( markup ,  'html5lib' )  for  tag  in  soup .
string )))  # (2, 0, 'Paragraph 1')  # (3, 6, 'Paragraph 2')    可以在 BeautifulSoup 构造函数中配置 store_line_numbers=False 来关闭这个功能  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  store_line_numbers = False )  print ( soup .
sourceline )  # None    这个功能在 4.8.1 版本中引入，lxml 解析器不支持这个功能。    比较对象是否相同 ¶  两个 NavigableString 或 Tag 对象具有相同的 HTML 或 XML 结构时，
Beautiful Soup就判断这两个对象相同。这个例子中，2个 <b> 标签在 BS 中是相同的，
尽管他们在文档树的不同位置，但是具有相同的表象: “<b>pizza</b>”  markup  =  "<p>I want <b>pizza</b> and more <b>pizza</b>!</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  first_b ,  second_b  =  soup .
previous_element )  # False    如果想判断两个对象是否严格的指向同一个对象可以通过 is 来判断  print  first_b  is  second_b  # False      复制Beautiful Soup对象 ¶  copy.copy() 方法可以复制任意 Tag 或 NavigableString 对象  import  copy  p_copy  =  copy .
p )  print ( p_copy )  # <p>I want <b>pizza</b> and more <b>pizza</b>!</p>    复制后的对象跟与对象是相等的，但指向不同的内存地址  print  soup .
p  ==  p_copy  # True  print  soup .
p  is  p_copy  # False    源对象和复制对象的区别是源对象在文档树中，而复制后的对象是独立的还没有添加到文档树中。
复制后对象的效果跟调用了 extract() 方法相同。  print  p_copy .
parent  # None    这是因为相等的对象不能同时插入相同的位置    高级自定义解析 ¶  Beautiful Soup 提供多种途径自定义解析器如果解析 HTML 和 XML。本章覆盖了最常用的自定义方法。   解析部分文档 ¶  如果仅仅因为想要查找文档中的 <a> 标签而将整片文档进行解析，实在是浪费内存和时间。最快的方法
是从一开始 就把 <a> 标签以外的东西都忽略掉。 SoupStrainer 类可以选择解析哪部分文档内容，
创建一个 SoupStrainer 对象并作为 parse_only 参数给 BeautifulSoup 的构造
方法即可。  (注意， 这个功能在 html5lib 解析器中无法使用 。如果使用 html5lib 解析器，整篇文档都会被解析，
这是因为 html5lib 会重新排列文档树的结构，如果部分节点不在文档树中，会导致崩溃。为了避免混淆，
下面的例子中 Beautiful Soup 都强制指定使用了 Python 内置解析器。)    SoupStrainer ¶  SoupStrainer 类接受与典型搜索方法相同的参数： name , attrs , recursive , string , **kwargs 。
下面举例说明三种 SoupStrainer 对象：  from  bs4  import  SoupStrainer  only_a_tags  =  SoupStrainer ( "a" )  only_tags_with_id_link2  =  SoupStrainer ( id = "link2" )  def  is_short_string ( string ):  return  string  is  not  None  and  len ( string )  <  10  only_short_strings  =  SoupStrainer ( string = is_short_string )    再拿“爱丽丝”文档来举例，来看看使用三种 SoupStrainer 对象做参数会有什么不同:  html_doc  =  """<html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_a_tags ) .
prettify ())  # Elsie  # ,  # Lacie  # and  # Tillie  # ...
 #    还可以将 SoupStrainer 作为参数传入 搜索文档树 中提到的方法。虽然不常用，但还是提一下:  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )  soup .
find_all ( only_short_strings )  # ['\n\n', '\n\n', 'Elsie', ',\n', 'Lacie', ' and\n', 'Tillie',  #  '\n\n', '...', '\n']      自定义包含多个值的属性 ¶  在 HTML 文档中，像 class 这样的属性的值是一个列表，像 id 这样的属性的值是一个单一字符串，
因为 HTML 标准定义了这些属性的不同行为  markup  =  '<a class="cls1 cls2" id="id1 id2">'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
a [ 'id' ]  # 'id1 id2'    设置 multi_valued_attributes=None 可以禁用多值的自动识别，然后全部属性的值都变成一个字符串  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  multi_valued_attributes = None )  soup .
a [ 'id' ]  # 'id1 id2'    如果给 multi_valued_attributes 参数传入一个字典，可以实现一点点解析自定义。如果需要这么做，
查看 HTMLTreeBuilder.DEFAULT_CDATA_LIST_ATTRIBUTES 了解 Beautiful Soup 的默认配置，
这些均是基于 HTML 标准配置的。  (这个功能添加于 Beautiful Soup 4.8.0)    处理重复属性 ¶  使用 html.parser 解析器时，可以通过设置 on_duplicate_attribute 参数，来定义当
Beautiful Soup 在 tag 中发现重复的属性名字时如何处理  markup  =  '<a href="http://url1/" href="http://url2/">'    默认行为是，重名属性会使用最后出现的值  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
a [ 'href' ]  # http://url2/    当 on_duplicate_attribute='ignore' 时，Beautiful Soup 会使用第一个出现的值，然后忽略
后出现的值  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  on_duplicate_attribute = 'ignore' )  soup .
a [ 'href' ]  # http://url1/    （lxml 和 html5lib 总是采用这种处理方式，它们的默认行为不能通过 Beautiful Soup 配置。）  如果需要复杂的控制，可以传入一个方法，当属性值重复时会被调用  def  accumulate ( attributes_so_far ,  key ,  value ):  if  not  isinstance ( attributes_so_far [ key ],  list ):  attributes_so_far [ key ]  =  [ attributes_so_far [ key ]]  attributes_so_far [ key ] .
a [ 'href' ]  # ["http://url1/", "http://url2/"]    这个特性新增于 Beautiful Soup 4.9.1。    实例化自定义子类 ¶  当解析器传递给 Beautiful Soup 一个标签或一个字符串后，Beautiful Soup 会实例化为 Tag 或 NavigableString 对象，并包含相关信息。如果想修改默认行为，可以让 Beautiful Soup 实例化 Tag 或 NavigableString 的子类，子类中可以自定义行为  from  bs4  import  Tag ,  NavigableString  class  MyTag ( Tag ):  pass  class  MyString ( NavigableString ):  pass  markup  =  "<div>some text</div>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  isinstance ( soup .
string ,  MyString )  # True    这种用法可用在于 Beautiful Soup 与测试框架集成。  这个特性新增于 Beautiful Soup 4.8.1。     常见问题 ¶   代码诊断 ¶  如果想知道 Beautiful Soup 到底怎样处理一份文档，可以将文档传入 diagnose() 方法(Beautiful Soup 4.2.0中新增)， Beautiful Soup 会输出一份报告，
说明不同的解析器会怎样处理这段文档，并标出当前的解析过程会使用哪种解析器:  from  bs4.diagnose  import  diagnose  with  open ( "bad.html" )  as  fp :  data  =  fp .
# Found lxml version 2.3.2.0  #  # Trying to parse your data with html.parser  # Here's what html.parser did with the document:  # ...
   diagnose() 方法的输出结果可能帮助你找到问题的原因，如果不行，还可以把结果复制出来以
便寻求他人的帮助。    文档解析错误 ¶  文档解析错误有两种。一种是崩溃，Beautiful Soup 尝试解析一段文档结果却抛除了异常，通常是 HTMLParser.HTMLParseError 。还有一种异常情况，是Beautiful Soup 解析后的文档树
看起来与原来的内容相差很多。  这些错误几乎都不是 Beautiful Soup 的原因，这不是因为 Beautiful Soup 的代码写得多优秀，
而是因为 Beautiful Soup 没有包含任何文档解析代码。异常产生自被依赖的解析器，如果解析器不能
很好的解析出当前的文档，那么最好的办法是换一个解析器。更多细节查看 安装解析器 章节。  最常见的解析错误是 HTMLParser.HTMLParseError:  malformed  start  tag 和 HTMLParser.HTMLParseError:  bad  end  tag 。这都是由Python内置的解析器引起的，
解决方法是 安装 lxml 或 html5lib 。  最常见的非预期行为是发现不了一个确定存在稳当中的 Tag。光是用眼睛就能轻易发现，但用 find_all() 方法返回 [] ，用 find() 方法返回 None 。这是 Python 内置解析器的又一个问题: 解析器会跳过那些
它不知道的 tag。解决方法还是 安装 lxml 或 html5lib    版本错误 ¶   SyntaxError:  Invalid  syntax (异常位置在代码行: ROOT_TAG_NAME  =  u'[document]' )，
原因是用 Python2 版本的 Beautiful Soup 未经过代码转换，直接在 Python3 中运行。  ImportError:  No  module  named  HTMLParser 因为在 Python3 中执行 Python2 版本的 Beautiful Soup。  ImportError:  No  module  named  html.parser 因为在 Python2 中执行 Python3 版本的 Beautiful Soup  ImportError:  No  module  named  BeautifulSoup 因为在没有安装 Beautiful Soup3 库的 Python 环境下执行代码，
或忘记了 Beautiful Soup4 的代码需要从 bs4 包中引入。  ImportError:  No  module  named  bs4 因为当前 Python 环境下还没有安装 Beautiful Soup4。     解析成XML ¶  默认情况下，Beautiful Soup 会将当前文档作为 HTML 格式解析，如果要解析 XML 文档，要在 BeautifulSoup 构造方法中加入第二个参数 “xml”:  soup  =  BeautifulSoup ( markup ,  "xml" )    当然，还需要 安装 lxml    其它解析器的错误 ¶   如果同样的代码在不同环境下结果不同，可能是因为两个环境下使用不同的解析器造成的。
例如这个环境中安装了 lxml，而另一个环境中只有 html5lib, 解析器之间的区别 中说明了原因。
修复方法是在 BeautifulSoup 的构造方法中中指定解析器。  因为HTML标签是 大小写敏感 的，
所以解析器会将 tag 和属性都转换成小写。例如文档中的 <TAG></TAG> 会被转换为 <tag></tag> 。
如果想要保留 tag 的大写的话，那么应该将文档 解析成XML 。     杂项错误 ¶   UnicodeEncodeError:  'charmap'  codec  can't  encode  character  '\xfoo'  in  position  bar (或其它类型的 UnicodeEncodeError )的错误，
主要是两方面的原因，第一种是正在使用的终端(console)无法显示部分Unicode，参考 Python wiki ，第二种是向文件
写入时，被写入文件不支持部分 Unicode，这时需要用 u.encode("utf8") 方法将
编码转换为UTF-8。  KeyError:  [attr] 因为调用 tag['attr'] 方法而引起，因为这个 tag 没有定义
该属性。出错最多的是 KeyError:  'href' 和 KeyError:  'class' 。如果不确定
某个属性是否存在时，用 tag.get('attr') 方法去获取它，跟获取 Python 字典的 key 一样。  AttributeError:  'ResultSet'  object  has  no  attribute  'foo' 错误通常是
因为把 find_all() 的返回结果当作一个 tag 或文本节点使用，实际上返回结果是一个
列表或 ResultSet 对象的字符串，需要对结果进行循环才能得到每个节点的 .foo 属性。 或者使用 find() 方法仅获取到一个节点。  AttributeError:  'NoneType'  object  has  no  attribute  'foo' 这个错误通常是
在调用了 find() 方法后直节点取某个属性 foo。但是 find() 方法并没有找到任何
结果，所以它的返回值是 None 。需要找出为什么 find() 的返回值是 None 。  AttributeError:  'NavigableString'  object  has  no  attribute  'foo' 这种问题
通常是因为吧一个字符串当做一个 tag 来处理。可能在迭代一个列表时，期望其中都是 tag，但实际上
列表里既包含 tag 也包含字符串。     如何提高效率 ¶  Beautiful Soup对文档的解析速度不会比它所依赖的解析器更快，如果对计算时间要求很高或者
计算机的时间比程序员的时间更值钱，那么就应该直接使用 lxml 。  换句话说，还有提高 Beautiful Soup 效率的办法，使用lxml作为解析器。Beautiful Soup
用 lxml 做解析器比用 html5lib 或 Python 内置解析器速度快很多。  安装 cchardet 后文档的解码的编码检测
速度会更快。  解析部分文档 不会节省多少解析时间，但是会节省很多内存，并且搜索时也会变得更快。     翻译这篇文档 ¶  非常感谢欢迎翻译 Beautiful Soup 的文档。翻译内容应当基于 MIT 协议，就像 Beautiful Soup 和
英文文档一样。  有两种方式将翻译内容添加到 Beautiful Soup 的网站上：   在 Beautiful Soup 代码库上创建一个分支，添加翻译，然后合并到主分支。就像修改源代码一样。  向 Beautiful Soup 讨论组里发送一个消息，带上翻译的链接，或翻译内容的附件。   使用中文或葡萄牙语的翻译作为模型。尤其注意，请翻译源文件 doc/source/index.rst ，
而不是 HTML 版本的文档。这样才能将文档发布为多种格式，而不局限于 HTML。    Beautiful Soup 3 ¶  Beautiful Soup 3 是上一个发布版本，目前已经停止维护。Beautiful Soup 3 库目前已经被
几个主要的 linux 发行版添加到源里:  $  apt-get  install  Python-beautifulsoup  在 PyPi 中分发的包名字是 BeautifulSoup :  $  easy_install  BeautifulSoup  $  pip  install  BeautifulSoup  或通过 Beautiful Soup 3.2.0源码包 安装。  如果是通过 easy_install  beautifulsoup 或 easy_install  BeautifulSoup 安装，然后代码无法运行，那么可能是安装了错误的 Beautiful Soup 3 版本。
应该这样安装 easy_install  beautifulsoup4 。  Beautiful Soup 3 的在线文档查看 这里 .
迁移到 BS4 ¶  大部分使用 Beautiful Soup 3 编写的代码都可以在 Beautiful Soup 4 上运行，
只有一个小变动。只要把引用包的名字从 BeautifulSoup 改为 bs4 ，比如:  from  BeautifulSoup  import  BeautifulSoup    修改为:  from  bs4  import  BeautifulSoup     如果代码抛出 ImportError 异常 “No module named BeautifulSoup”，
原因可能是尝试执行 Beautiful Soup 3，但环境中只安装了 Beautiful Soup 4。  如果代码抛出 ImportError 异常 “No module named bs4”，原因可能是尝试
运行 Beautiful Soup 4 的代码，但环境中只安装了Beautiful Soup 3。   尽管 BS4 兼容绝大部分 BS3 的功能，但 BS3 中的大部分方法已经不推荐使用了，旧方法标记废弃，
并按照 PEP8标准 重新命名了新方法。
虽然有大量的重命名和修改，但只有少数几个方法没有向下兼容。  下面内容就是 BS3 迁移到 BS4 的注意事项:   解析器的变化 ¶  Beautiful Soup 3 曾使用 Python 的 SGMLParser 解析器，这个模块在 Python3
中已经被移除了。Beautiful Soup 4 默认使用系统的 html.parser , 也可以使用
lxml 或 html5lib 扩展库代替。查看 安装解析器 章节。  因为解析器 html.parser 与 SGMLParser 不同。BS4 和 BS3 处理相同的文档会
产生不同的对象结构。使用 lxml 或 html5lib 解析文档的时候，如果添加了 html.parser 参数，解析的对象又会发生变化。如果发生了这种情况，只能修改对应的文档处理代码了。    方法名的变化 ¶   renderContents -> encode_contents  replaceWith -> replace_with  replaceWithChildren -> unwrap  findAll -> find_all  findAllNext -> find_all_next  findAllPrevious -> find_all_previous  findNext -> find_next  findNextSibling -> find_next_sibling  findNextSiblings -> find_next_siblings  findParent -> find_parent  findParents -> find_parents  findPrevious -> find_previous  findPreviousSibling -> find_previous_sibling  findPreviousSiblings -> find_previous_siblings  nextSibling -> next_sibling  previousSibling -> previous_sibling   Beautiful Soup 构造方法的参数部分也有名字变化:   BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...)  BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...)   为了适配 Python3，修改了一个方法名:   Tag.has_key() -> Tag.has_attr()   修改了一个属性名，让它看起来更专业点:   Tag.isSelfClosing -> Tag.is_empty_element   修改了下面 3 个属性的名字，以免与 Python 保留字冲突。这些变动不是向下兼容的，如果在 BS3
中使用了这些属性，那么在 BS4 中这些代码无法执行。   UnicodeDammit.Unicode  ->  UnicodeDammit.Unicode_markup  Tag.next -> Tag.next_element  Tag.previous -> Tag.previous_element     生成器 ¶  将下列生成器按照 PEP8 标准重新命名，并转换成对象的属性:   childGenerator() -> children  nextGenerator() -> next_elements  nextSiblingGenerator() -> next_siblings  previousGenerator() -> previous_elements  previousSiblingGenerator() -> previous_siblings  recursiveChildGenerator() -> descendants  parentGenerator() -> parents   所以要把这样的代码:  for  parent  in  tag .
parentGenerator ():  ...
   替换为:  for  parent  in  tag .
parents :  ...
   (其实老方法也可以继续使用)  有的生成器循环结束后会返回 None 然后结束。这是个 bug。新版生成器不再返回 None 。  BS4 中增加了 2 个新的生成器， .strings 和 stripped_strings 。 .strings 生成器
返回 NavigableString 对象， .stripped_strings 方法返回去除前后空白的 Python 的
string 对象。    XML ¶  BS4 中移除了解析 XML 的 BeautifulStoneSoup 类。如果要解析一段 XML 文档，使用 BeautifulSoup 构造方法并在第二个参数设置为“xml”。同时 BeautifulSoup 构造
方法也不再识别 isHTML 参数。  Beautiful Soup 处理 XML 空标签的方法升级了。旧版本中解析 XML 时必须指明哪个标签是空标签。
构造方法的 selfClosingTags 参数已经不再使用。新版 Beautiful Soup 将所有空标签解析
为空元素，如果向空元素中添加子节点，那么这个元素就不再是空元素了。    实体 ¶  输入的 HTML 或 XML 实体都会被解析成 Unicode 字符。Beautiful Soup 3 版本中有很多相似处理
实体的方法，在新版中都被移除了。 BeautifulSoup 构造方法也不再接受 smartQuotesTo 或 convertEntities 参数。 编码自动检测 方法依然有 smart_quotes_to 参数，但是默认会将引号转换成 Unicode。内容配置项 HTML_ENTITIES , XML_ENTITIES 和 XHTML_ENTITIES 在新版中被移除。因为它们代表的特性（转换部分而不是全部实体到 Unicode 字符）
已经不再支持。  如果在输出文档时想把 Unicode 字符转回 HTML 实体，而不是输出成 UTF-8 编码，那就需要用到 输出格式 的方法。    迁移杂项 ¶  Tag.string 属性现在是一个递归操作。如果 A 标签只包含了一个 B 标签，那么 A 标签的。
string 属性值与 B 标签的 string 属性值相同。  多值属性 比如 class 属性包含一个他们的值的列表，而不是一个字符串。这可能会影响到如何按照
CSS 类名哦搜索 tag。  Tag 对象实现了一个 __hash__ 方法，这样当两个 Tag 对象生成相同的摘要时会被认为相等。这可能会
改变你的脚本行为，如果你吧 Tag 对象放到字典或集合中。  如果使用 find* 方法时同时传入了 string 参数 和一个指定 tag 的参数比如 name 参数 。
Beautiful Soup 会搜索符合指定参数的 tag，并且这个 tag 的 Tag.string 属性包含 string 参数 参数的内容。结果中 不会 包含字符串本身。旧版本中 Beautiful Soup 会忽略掉指定
tag 的参数，只搜索符合 string 的内容。  BeautifulSoup 构造方法不再支持 markupMassage 参数。现在由解析器负责文档的解析正确性。  很少被用到的几个解析器方法在新版中被移除，比如 ICantBelieveItsBeautifulSoup 和 BeautifulSOAP 。
现在由解析器完全负责如何解释模糊不清的文档标记。  prettify() 方法在新版中返回 Unicode 字符串，不再返回字节串。      附录 ¶    [ 1 ]  BeautifulSoup 的 google 讨论组不是很活跃，可能是因为库已经比较完善了吧，但是作者还是会很热心的尽量帮你解决问题的。    [ 2 ]  ( 1 , 2 )  文档被解析成树形结构，所以下一步解析过程应该是当前节点的子节点    [ 3 ]  过滤器只能作为搜索文档的参数，或者说应该叫参数类型更为贴切，原文中用了 filter 因此翻译为过滤器    [ 4 ]  元素参数，HTML 文档中的一个 tag 节点，不能是文本节点    [ 5 ]  ( 1 , 2 , 3 , 4 , 5 )  采用先序遍历方式    [ 6 ]  CSS选择器是一种单独的文档搜索语法，参考 http://www.w3school.com.cn/css/css_selector_type.asp    [ 7 ]  原文写的是 html5lib, 译者觉得这是原文档的一个笔误    [ 8 ]  wrap含有包装，打包的意思，但是这里的包装不是在外部包装而是将当前tag的内部内容包装在一个tag里。
包装原来内容的新tag依然在执行 wrap() 方法的tag内    [ 9 ]  文档中特殊编码字符被替换成特殊字符(通常是�)的过程是Beautful Soup自动实现的，
如果想要多种编码格式的文档被完全转换正确，那么，只好，预先手动处理，统一编码格式    [ 10 ]  ( 1 , 2 )  智能引号，常出现在 microsoft 的 word 软件中，即在某一段落中按引号出现的顺序每个引号都被自动转换为左引号，或右引号。            Table of Contents   Beautiful Soup 4.12.0 文档  寻求帮助    快速开始  安装 Beautiful Soup  安装解析器    如何使用  对象的种类  name  attrs  多值属性  NavigableString    可遍历的字符串  BeautifulSoup    注释及特殊字符串  针对 HTML 文档  Stylesheet  Script  Template    针对 XML 文档  Declaration  Doctype  CData  ProcessingInstruction        遍历文档树  子节点  Tag 的名字  .contents 和 .children  .descendants  .string  .strings 和 stripped_strings    父节点  .parent  .parents    兄弟节点  .next_sibling 和 .previous_sibling  .next_siblings 和 .previous_siblings    回退和前进  .next_element 和 .previous_element  .next_elements 和 .previous_elements      搜索文档树  过滤器类型  字符串  正则表达式  列表  True  函数    find_all()  name 参数  keyword 参数  按CSS搜索  string 参数  limit 参数  recursive 参数    像调用 find_all() 一样调用tag  find()  find_parents() 和 find_parent()  find_next_siblings() 和 find_next_sibling()  find_previous_siblings() 和 find_previous_sibling()  find_all_next() 和 find_next()  find_all_previous() 和 find_previous()  CSS 选择器  Soup Sieve 高级特性  CSS 筛选器中的命名空间  支持 CSS 筛选器的历史版本    修改文档树  修改 tag 的名称和属性  修改 .string  append()  extend()  NavigableString() 和 .new_tag()  insert()  insert_before() 和 insert_after()  clear()  extract()  decompose()  replace_with()  wrap()  unwrap()  smooth()    输出  格式化输出  压缩输出  输出格式  格式化对象  HTMLFormatter  XMLFormatter    编写自定义 formatter  get_text()    指定文档解析器  解析器之间的区别    编码  输出编码  Unicode, Dammit  智能引号  矛盾的编码      行编号  比较对象是否相同  复制Beautiful Soup对象  高级自定义解析  解析部分文档  SoupStrainer  自定义包含多个值的属性  处理重复属性  实例化自定义子类    常见问题  代码诊断  文档解析错误  版本错误  解析成XML  其它解析器的错误  杂项错误  如何提高效率    翻译这篇文档  Beautiful Soup 3  迁移到 BS4  解析器的变化  方法名的变化  生成器  XML  实体  迁移杂项      附录     This Page   Show Source     Quick search               Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup 4.12.0 文档    © Copyright 2012, Leonard Richardson.
Navigation    index  Beautiful Soup 4.2.0 Doc.
日本語訳 (2013-11-19最終更新) »         Beautiful Soup ¶   Beautiful Soup はHTMLやXMLファイルからデータを取得するPythonのライブラリです。あなたの好きなパーサー(構文解析器)を使って、パースツリー(構文木)の探索、検索、修正を行います。
これはプログラマーの作業時間を大幅に短縮してくれます。   (訳注)石鹸は食べられない ¶  この文書は Beautiful Soup 4.2.0 Documentation の日本語訳です。”Beautiful Soup”を”ビューティフルソープ”と読んでしまう英語が苦手でちょっぴりHな後輩のために翻訳しました。  2013年10月29日からこの文書の翻訳をはじめました。11月1日現在まだ全てを訳し終えていませんが、スクレイピングに使う主な部分はとりあえず訳したので、一旦これで公開して、あとは年内を目処にまったりと翻訳をすすめ、あわせて質を高めていこうと思っています。今のところ、 パースツリーを修正 以降は、ざっくり訳のためにおかしな表現が多々あることにご注意ください。  誤訳やわかりづらいところを見つけたり、なにかご意見があるときには、近藤茂徳( )までご連絡ください。こういった翻訳をするのははじめてなので、つっこみ大歓迎です。よろしくお願いします。  2013年10月現在、Beautiful Soupについての日本語Webページは、Beautiful Soup 3とBeautiful Soup 4（以下、BS3,BS4)の情報が混在しています。とくに、”Beautiful Soup”で日本語ページを対象にググると、最初に表示される10件中9件がBS3による情報であるために、初心者はそのままBS3を使って混乱しがちです。ご注意ください。  混乱しないように初心者が知っておくべきこと   2012年5月にBS3の開発が終了し、現在ではBS4の利用が推奨されています  BS3はPython3に対応していません  ただし、BS3のスクリプトのほとんどはimport文を変えるだけでBS4でも動きます  そのため、BS3による情報も問題解決の役に立ちます  詳しくは Beautiful Soup 3 を読んでください  この文書の クイックスタート と find_all() を読めば、それなりに用は足りると思います     この文書について ¶  この文書は、Beautiful Soup 4 (訳注:以下BS4)の主要機能について、例を挙げて説明します。どのライブラリがよいか、どのように動くか、どのように使うか、どのようにあなたの望むことを達成するか、予想外の動きをしたときは何をすればよいかといったことを示します。  この文書で挙げられる例は、Python2.7と3.2のどちらでも同じように動きます。  あなたは Beautiful Soup 3 (訳注:以下BS3)の文書 を探しているのかもしれません。もしそうなら、BS3はすでに開発を終えていて、BS4が全てのプロジェクト対して推奨されていることを知っていてください。BS3とBS4の違いを知りたいときは、 BS4への移行 を見てください。  この文書は、ユーザーにより他の言語にも翻訳されています。   이 문서는 한국어 번역도 가능합니다.
( 외부 링크 )     助けてほしいときは ¶  Beautiful Soup について疑問が生じたり、問題に直面したときは、 ディスカッショングループにメールしてください。 もし問題がHTMLのパースのことであれば、そのHTMLについて diagnose() 関数の返す内容 を必ず書くようにしてください。     クイックスタート ¶  以下のHTMLドキュメントは、このあと何回も例として用いられます。 ふしぎの国のアリス からの引用です。:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """    この”three sisters”ドキュメントを Beautiful Soup にかけると、 Beautiful  Soup オブジェクトが得られます。これは入れ子データ構造でドキュメントを表現します。:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc )  print ( soup .
#   </p>  #   <p class="story">  #    ...
 #   </p>  #  </body>  # </html>    以下は、データ構造を探索するいくつかの方法です。:  soup .
find ( id = "link3" )  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>    よくある処理として、ページの<a>タグ内にあるURLを全て抽出するというものがあります。:  for  link  in  soup .
get ( 'href' ))  # http://example.com/elsie  # http://example.com/lacie  # http://example.com/tillie    また、ページからタグを除去して全テキストを抽出するという処理もあります。:  print ( soup .
#  # ...
   必要な情報は得られましたか？つづきをどうぞ。    インストール ¶  DebianかUbuntuの最近のバージョンを使っていれば、Beautiful Soupはシステムのパッケージマネージャでインストールできます。:  $  apt-get  install  python-bs4  Beautiful Soup 4 は PyPiを通して公開されています。そのため、もしシステムパッケージで Beautiful Soup をインストールできないときは、 easy_install か pip でインストールできます。  $  easy_install  beautifulsoup4  $  pip  install  beautifulsoup4  ( BeautifulSoup パッケージはおそらくあなたが探しているものでは ありません 。これは、一つ前のメジャーリリース Beautiful Soup 3 です。多くのソフトウェアがBS3を使っていて、今でもBS3は利用できます。しかし、新しくコードを書く場合は、 beautifulsoup4 をインストールすべきです。)  もし、 easy_install や pip をインストールしてないときは、 download the Beautiful Soup 4 source tarball でソースをダウンロードし setup.py を用いてインストールできます。  $  python  setup.py  install  もしどの方法も失敗するのなら、あなたのアプリケーションにライブラリをそのままパッケージングするという手もあります。Beautiful Soupのライセンスはそれを認めています。.tar.gz形式でダウンロードし、アプリケーションのソースコード内に bs4 ディレクトリをコピーしてください。そうすれば、Beautiful Soupをインストールすることなしに使うことができます。  私は、Python 2.7とPython 3.2でBeautiful Soupを開発しましたが、他の最近のバージョンでも動くはずです。   インストール後の問題 ¶  Beautiful SoupはPython 2のコードとしてパッケージされています。
Beautiful SoupをPython 3環境で使おうとしてインストールすると、それは自動的にPython 3のコードとして変換されます。
もし、Beautiful Soupパッケージをインストールしないと、コードは変換されません。
Windowsでは、間違ったバージョンが入っていると、それが報告されます。  ImportError “No module named HTMLParser” というエラーが表示されたら、それはPython 3環境でPython 2で書かれたコードを実行しようとしたためです。  ImportError “No module named html.parser” というエラーが表示されたら、それはPython 2環境でPython 3ので書かれたコードを実行しようとしたためです。  どちらの場合もとるべき対応は、Beautiful Soupを(tarballを解凍したときディレクトリを含め)
完全にアンインストールして、再インストールをすることです。  ROOT_TAG_NAME  =  u'[document]' 行で SyntaxError “Invalid syntax” のエラーが表示されたら、
Python 2で書かれたBeautiful SoupのコードをPython 3に変換しなければいけません。  そのためには、次のようにパッケージをインストールするか、:  $  python3  setup.py  install  もしくは、手動で 2to3 変換スクリプトを bs4 ディレクトリで実行すればできます。:  $  2to3-3.2  -w  bs4    パーサーのインストール ¶  Beautiful SoupはPythonの標準ライブラリに入っているHTMLパーサーをサポートすると同時に、多くのサードパーティーのPythonパーサーもサポートしています。一つには、 lxml parser .
があります。環境に依りますが、以下のコマンドのどれかでlxmlをインストールできるでしょう。:  $  apt-get  install  python-lxml  $  easy_install  lxml  $  pip  install  lxml  別の選択肢として、Python純正の html5lib parser が挙げられます。これは HTMLをwebブラウザがするようにパースします。これも環境に依りますが、以下のコマンドのどれかでhtml5libをインストールできるでしょう。:  $  apt-get  install  python-html5lib  $  easy_install  html5lib  $  pip  install  html5lib  以下の表は、各パーサーのライブラリの強みと弱みをまとめてあります。        パーサー  使用例  強み  弱み   Python’s html.parser  BeautifulSoup(markup,  "html.parser")   標準ライブラリ  まずまずのスピード  Python2.7.3/3.2.2以降に対応     Python2.7.3/3.2.2未満は非対応     lxml’s HTML parser  BeautifulSoup(markup,  "lxml")   爆速  対応(?)
外部Cライブラリに依存     lxml’s XML parser  BeautifulSoup(markup,  ["lxml",  "xml"])  BeautifulSoup(markup,  "xml")   爆速  唯一の対応XMLパーサー     外部Cライブラリに依存     html5lib  BeautifulSoup(markup,  "html5lib")   対応度高  WEBブラウザと同じようにパース  正しいHTML5を生成     とても遅い  外部Pythonライブラリに依存      できれば、速度のためにlxmlをインストールして使うことをお薦めします。
とくに、あなたがPython2.7.3のPython2系か、Python3.2.2より前のPython3系を使っているばあいは、lxmlかhtml5libをインストールすることは とても大事です 。
なぜなら、Pythonにはじめから組み込まれているHTMLパーサーは、古いバージョンのPythonではそこまで良く動かないからです。  構文が不正確なドキュメントのときは、パーサーが違うと生成されるパースツリーが異なってくることに注意してください。
詳しくは、 パーサーの違い を参照のこと。     スープの作成 ¶  ドキュメントをパース(構文解析)するには、
そのドキュメントを Beautiful  Soup コンストラクタに渡します。
文字列でも開いたファイルハンドルでも渡せます。:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( open ( "index.html" ))  soup  =  BeautifulSoup ( "<html>data</html>" )    最初に、ドキュメントはUnicodeに変換され、HTMLエンティティはUnicode文字列に変換されます。:  BeautifulSoup("Sacr&eacute; bleu!")
<html><head></head><body>Sacré bleu!</body></html>   Beautiful Soupは、ドキュメントをもっとも適したパーサー(構文解析器)を使ってパースします。
XMLパーサーを使うように指定しなければ、HTMLパーサーが用いられます。( XMLのパース を参照)    4種類のオブジェクト ¶  Beautiful Soup は複雑なHTMLドキュメントを、Pythonオブジェクトの複雑なツリー構造に変換します。
しかし、あなたは Tag , NavigableString , BeautifulSoup , Comment の 4種類のオブジェクト だけを扱えばよいです。   Tag obj.
¶  Tag オブジェクトは、元のドキュメント内のXMLやHTMLのタグに対応しています。:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' )  tag  =  soup .
b  type ( tag )  # <class 'bs4.element.Tag'>    Tag オブジェクトは、多くの属性とメソッドを持っています。それらのほとんどは、 パースツリーを探索 と パースツリーを検索 で説明します。この節では Tag オブジェクトの重要な機能である、名前と属性について説明します。   名前 ¶  タグはそれぞれ名前を持っていますが、 .name でアクセスできます。:  tag .
name  # u'b'    タグの名前を変えると、その変更はBeautiful Soupが生成する全てのマークアップに反映されます。:  tag .
name  =  "blockquote"  tag  # <blockquote class="boldest">Extremely bold</blockquote>      属性 ¶  タグは多くの属性を持ちます。
<b class=”boldest”>は、”boldest”という値の’class’属性を持ちます。 Tag オブジェクトを辞書のように扱うことで、そのタグの属性にアクセスできます。:  tag [ 'class' ]  # u'boldest'    .attrs で辞書に直接アクセスできます。:  tag .
attrs  # {u'class': u'boldest'}    繰り返しになりますが、辞書のように Tag オブジェクトを扱うことにより、タグの属性に対して追加, 削除, 修正も行うことができます。:  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>  tag [ 'class' ]  # KeyError: 'class'  print ( tag .
get ( 'class' ))  # None     値が複数のとき ¶  HTML4は、値を複数もてる2,3の属性を定義しています。
HTML5で、それらはなくなりましたが、別の同様の属性が定義されました。
もっとも一般的な値を複数もつ属性は class です。(たとえば、HTMLタグは複数のCSSクラスを持つことができます)
また他の複数の値を持つ属性としては、 rel , rev , accept-charset , headers , accesskey があります。
Beautiful Soupは、これらの属性がもつ複数の値をリストとして示します。:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' )  css_soup .
p [ 'class' ]  # ["body", "strikeout"]  css_soup  =  BeautifulSoup ( '<p class="body"></p>' )  css_soup .
p [ 'class' ]  # ["body"]    ある属性が複数の値をもっているようでも、HTML標準の定義から外れている場合、Beautiful Soupはその属性をひとまとまりの値として扱います。:  id_soup  =  BeautifulSoup ( '<p id="my id"></p>' )  id_soup .
a [ 'rel' ]  # ['index']  rel_soup .
p [ 'class' ]  # u'body strikeout'        NavigableString obj.
¶  タグの組に挟まれる短い(ドキュメントの本文のテキスト)文字列があります。
Beautiful Soupは、これらの文字列を表すのに NavigableString クラスを用います。:  tag .
string  # u'Extremely bold'  type ( tag .
string )  # <class 'bs4.element.NavigableString'>    NavigableString オブジェクトは、PythonのUnicode型のように振るまいます。
また、 パースツリーを探索 と パースツリーを検索 に述べられている機能のいくつかもサポートします。 unicode() を用いて、 NavigableString オブジェクトをUnicode型に変換できます。:  unicode_string  =  unicode ( tag .
string )  unicode_string  # u'Extremely bold'  type ( unicode_string )  # <type 'unicode'>    NavigableString の文字列は編集できませんが、 replace_with() を使って、他の文字列に置換することはできます。:  tag .
replace_with ( "No longer bold" )  tag  # <blockquote>No longer bold</blockquote>    NavigableString は、 パースツリーを探索 と パースツリーを検索 で述べられている機能のほとんどをサポートします。
しかし、全てをサポートしているわけではありません。
とくに、 Tag オブジェクトが文字列や別の Tag を内に含むのに対して、 string オブジェクトは何も持たず、 .contents` 属性, .string 属性, find() メソッドをサポートしません。  NavigableString をBeautiful Soupの外で使いたい場合は、 unicode() を使ってPythonのUnicode文字列に変換するべきです。そうしないと、Beautiful Soupを使い終わった後も、Beautiful Soupのパースツリー全体へのリファレンスを持ち続けることになり、メモリを大量に浪費します。    BeautifulSoup obj.
¶  Beautiful  Soup オブジェクトは、それ自身で元のドキュメント全体を表しています。
たいていの場合、 Tag obj.
を扱うことで、用は足りるでしょう。
これは、 Tag obj.
が パースツリーを探索 と パースツリーを検索 .
で述べられているメソッドの多くをサポートしているということです。  BeautifulSoup オブジェクトは、実際のHTMLやXMLタグに対応していないので、名前や属性を持たない。
しかし、 .name をみるような便利なものはいくつかある。そして、それらは特別な .name “[document]”を得られる(?訳がおかしい。けど次回まわし?
):  soup .
name  # u'[document]'      Comments obj.
他 ¶  Tag , NavigableString , BeautifulSoup はHTMLやXMLファイルのほぼ全てをカバーします。しかし、少しだけ残ったものがあります。それはコメントについてです。:  markup  =  "<b><!--Hey, buddy.
Want to buy a used parser?--></b>"  soup  =  BeautifulSoup ( markup )  comment  =  soup .
string  type ( comment )  # <class 'bs4.element.Comment'>    Comment オブジェクトは、 NavigableString オブジェクトの特別なタイプです。:  comment  # u'Hey, buddy.
Want to buy a used parser'    コメントはHTMLの中にあらわれますが、 Comment は特別な書式で表示されます。:  print ( soup .
Want to buy a used parser?-->  # </b>    Beautiful Soupは、XMLドキュメントのなかの他の全ての要素をクラス定義しています。 CData , ProcessingInstruction , Declaration , Doctype .
Comment クラスのように、これらは文字に何かを加えた NavigableString のサブクラスです。
ここでは、コメントをCDDATAブロックに置換した例を示します。:  from  bs4  import  CData  cdata  =  CData ( "A CDATA block" )  comment .
replace_with ( cdata )  print ( soup .
prettify ())  # <b>  #  <!
[CDATA[A CDATA block]]>  # </b>       パースツリーを探索 ¶  ここで再び “Three sisters” のHTMLドキュメントです。:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc )    ドキュメントのある部分から他の部分へどのように移動するかを示すために、このドキュメントを例に使っていきます。   子要素へ下移動 ¶  タグはその間に(ドキュメント本文のテキスト)文字列や他のタグを挟んでいます。これらの要素は、 タグの 子要素 です。Beautiful Soupは、タグの子要素を探索し扱うための多くの属性を提供します。  Beautiful Soupの文字列は、これらの属性をサポートしません。なぜなら、文字列は子要素をもたないからです。   タグ名で探索 ¶  パースツリーを探索する一番簡単な方法は、あなたが取得したいタグの名前を使うことです。
もし、<head> タグを取得したければ、 soup.head と入力すればよいです。:  soup .
title  # <title>The Dormouse's story</title>    また、パースツリーのある部分から出発して、何度もズームインを繰り返す方法もあります。
このコードは、<body>タグ以下の最初の<b>タグを取得します。:  soup .
b  # <b>The Dormouse's story</b>    属性としてタグ名を使うと、その名前のタグのうち 最初 にあるものを取得できます。:  soup .
a  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    全ての <a>タグを取得したいときや、ある名前のタグのうち2番目以降のものをしたいときは、 パースツリーを検索 で述べられている find_all() のようなメソッドを使う必要があります。:  soup .
find_all ( 'a' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      .contents / .children ¶  タグの子要素は、 .contents で呼び出すと、リストで取得できます。:  head_tag = soup.head
head_tag
# <head><title>The Dormouse's story</title></head>

head_tag.contents
[<title>The Dormouse's story</title>]

title_tag = head_tag.contents[0]
title_tag
# <title>The Dormouse's story</title>
title_tag.contents
# [u'The Dormouse's story']   Beautiful  Soup オブジェクトは、それ自身が子要素を持ちます。この場合、<html>タグが Beautiful  Soup オブジェクトの子要素になります。:  len ( soup .
name  # u'html'    文字列は .contents を持ちません。なぜなら、文字列は何も挟まないからです。:  text  =  title_tag .
contents  # AttributeError: 'NavigableString' object has no attribute 'contents'    タグの子要素を、リストの代わりに、 .children ジェネレーターを用いてイテレーターで扱うこともできます。:  for  child  in  title_tag .
children :  print ( child )  # The Dormouse's story      .descendants ¶  .contents と .children 属性は、あるタグの 直下の 子要素のみを表します。
例えば、<head>タグは、ただ一つの直下の子要素である<title>タグを持ちます。:  head_tag .
contents  # [<title>The Dormouse's story</title>]    しかし、この<title>タグ自身も、子要素に”The Dormouse’s story”文字列を持ちます。
この文字列もまた、<head>タグの子要素であるという意味になります。
そこで、 .descendants (子孫) 属性を用いると、 あるタグの 全ての 子要素を再帰的に取り出すことができます。
再帰的というのは、直下の子要素、そのまた子要素、そしてさらにといったふうに繰り返してということです。  for  child  in  head_tag .
descendants :  print ( child )  # <title>The Dormouse's story</title>  # The Dormouse's story    このドキュメントの<head>タグはただ1つの子要素しか持ちませんが、
<title>タグと<title>タグの子要素という2つの子孫要素を持ちます。
また、このドキュメントの BeautifulSoup オブジェクトには、
直下の子要素は<html>タグ1つしかありませんが、子孫要素はたくさんあります。:  len ( list ( soup .
descendants ))  # 25      .string ¶  ある Tag オブジェクトが1つだけ子要素をもっていて、その子要素が NavigableString オブジェクトならば、 .string 属性で利用できます。:  title_tag .
string  # u'The Dormouse's story'    ある Tag オブジェクトのただ1つの子要素が、別の Tag オブジェクトであって .string 属性を持つならば、元の Tag オブジェクトも同じ .string 属性を持つと考えられます。:  head_tag .
string  # u'The Dormouse's story'    ある tag オブジェクトが複数の子要素を持ち、 .string 属性がどの子要素を参照しているかわからないとき、 .string 属性は None と定義されます。:  print ( soup .
string )  # None      .strings / .stripped_strings ¶  あるタグの中にあるドキュメント本文が要素が複数であっても、それらの文字列をみることができます。
その場合は、 .strings ジェネレーターを使用します。:  for  string  in  soup .
# u'\n\n'  # u'...'  # u'\n'    これらの文字列は、大量の余計な空白が入りがちである。
そこで、 .stripped_strings ジェネレーターを代わりに用いることで、それら空白を除くことができる。:  for  string  in  soup .
# u'...'    ここでは、文字列中に入る空白はそのままで、文字列の最初や最後に付く空白は削除されます。     親要素へ上移動 ¶  “家族ツリー”に例えると、全てのタグや文字列はそれぞれが一つの親要素を持ちます。   .parent ¶  .parent 属性で親要素にアクセスできます。
たとえば、”three sisters”ドキュメントでは、<head>タグは<title>タグの親要素です。:  title_tag  =  soup .
parent  # <head><title>The Dormouse's story</title></head>    タイトル文字列はそれ自身が親要素を持ち、<title>タグはタイトル文字列を子要素に持ちます。:  title_tag .
parent  # <title>The Dormouse's story</title>    <html>タグの様なトップレベルのタグは、 BeautifulSoup オブジェクトそれ自身になります。:  html_tag  =  soup .
parent )  # <class 'bs4.BeautifulSoup'>    そして、 BeautifulSoup オブジェクトの .parent 属性は、Noneになります。:  print ( soup .
parent )  # None      .parents ¶  あるタグに対する祖先要素全てを .parents で取得することができます。
以下は、HTMLドキュメントの深いところにある<a>タグからスタートして、最上層まで辿っています。:  link  =  soup .
name )  # p  # body  # html  # [document]  # None       兄弟要素へ横移動 ¶  以下のようなシンプルなHTMLドキュメントを考えてみましょう。:  sibling_soup  =  BeautifulSoup ( "<a><b>text1</b><c>text2</c></b></a>" )  print ( sibling_soup .
prettify ())  # <html>  #  <body>  #   <a>  #    <b>  #     text1  #    </b>  #    <c>  #     text2  #    </c>  #   </a>  #  </body>  # </html>    <b>タグは<c>タグと同じレベルにあります。つまり、2つはともに同じタグの直下の子要素ということです。
こういった関係にあるタグを siblings (兄弟)といいます。
HTMLドキュメントをきれいに出力(?
)したとき、siblingsは同じインデントレベルになります。
こういったタグの関係をコードで利用することができます。   .next_sibling / .previous_sibling ¶  .next_sibling と .previous_sibling を用いて、パースツリーの同じレベルの要素間を辿ることができます。:  sibling_soup .
previous_sibling  # <b>text1</b>    この<b>タグは .next_sibling は持ちますが、 .previous_sibling は持ちません。
なぜなら、<b>タグの前にはパースツリーで同レベルの要素がないからです。
同様に、<c>タグは .previous_sibling を持ちますが、 .next_sibling は持ちません。:  print ( sibling_soup .
next_sibling )  # None    “text1”と”text”は兄弟ではありません。なぜなら、2つは同じ親をもたないからです。:  sibling_soup .
next_sibling )  # None    実際のHTMLドキュメントをパースすると、 .next_sibling や .previous_sibling は前後に空白を持ちます。
“three sisters”ドキュメントで見てみましょう。:  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a>
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>   すなおに考えれば、最初の<a>タグの .next_sibling は2番目の<a>タグとなるはずですが、実際は違います。
それは、最初の<a>タグと2番目を分ける”コンマと改行コード”という文字列になります。:  link  =  soup .
next_sibling  # u',\n'    2番目の<a>タグは、そのコンマと改行コードの .next_sibling になります。:  link .
next_sibling  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>      .next_siblings / .previous_siblings ¶  複数の兄弟要素を .next_siblings や .previous_siblings をイテレーターとして使って、まとめて扱えます。:  for  sibling  in  soup .
next_siblings :  print ( repr ( sibling ))  # u',\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # u' and\n'  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>  # u'; and they lived at the bottom of a well.'
# None  for  sibling  in  soup .
previous_siblings :  print ( repr ( sibling ))  # ' and\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # u',\n'  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  # u'Once upon a time there were three little sisters; and their names were\n'  # None       前後の要素へ移動 ¶  “three sisters”ドキュメントのはじめの部分を見てみましょう。:  <html><head><title>The Dormouse's story</title></head>
<p class="title"><b>The Dormouse's story</b></p>   HTMLパーサーは、この文字列を読み込み、イベントの連なりとして理解します。”open an <html> tag”, “open a <head> tag”, “open a <title> tag”, “add a string”, “close the <title> tag”, “open a <p>”...
といったかんじです。Beautiful Soupはこのイベントの連なりを、さらに再構成して扱います。   .next_element / .previous_element ¶  文字列やHTMLタグの .next_element 属性は、それの直後の要素を指し示します。 .next_string と同じようですが、決定的に違います。  “three sisters”ドキュメントの最後の<a>タグについて考えてみましょう。
それの .next_string はその<a>タグによって分割された文の後ろの部分の文字列です。(?
):  last_a_tag  =  soup .
一方、 .next_element は、<a>タグのすぐ後ろの要素である”Tillie”という単語を指し示します。文の残りの部分ではありません。:  last_a_tag .
next_element  # u'Tillie'    これは元の文章で”Tillie”という単語がセミコロンの前に現れるからです。
パーサーは<a>タグに出会い、次に”Tillie”という単語、そして</a>という閉じるタグがきます。
そのあとは、セミコロンがあって、文の残りの部分です。
セミコロンは<a>タグと同じレベルにありますが、”Tillie”という単語が最初に出会います。  .previous_element 属性は、 .next_element とは逆です。
その要素の一つ前の要素を指し示します。:  last_a_tag .
next_element  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>      .next_elements / .previous_elements ¶  パースされたドキュメントの要素を、前後方向に取得していくイテレーターを使うこともできます。:  for  element  in  last_a_tag .
# u'\n\n'  # <p class="story">...</p>  # u'...'  # u'\n'  # None        パースツリーを検索 ¶  Beautiful Soupはパースパースツリーを検索する多くのメソッドを定義しています。
しかし、それらはどれもとても似通っています。
この章では、 find() と find_all() という2つの人気のメソッドの説明に、多くのスペースを費やします。
それ以外のメソッドは、ほとんど同じ引数を持つので、簡単な説明にとどめることにします。  ここでは再び、”three sisters”ドキュメントを例に使っていきます。:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc )    find_all() のようなフィルターを通すことにより、
興味のあるドキュメントのある一部分にズームすることができます。   フィルターの種類 ¶  find_all() 等のメソッドの詳細を説明するまえに、これらのメソッドに渡すフィルターの例を示します。
検索APIの使い方をマスターする上で、フィルターは何度もでてきます。
これにより、タグ名, タグの属性, ドキュメントの文字列やそれを組み合わせた条件を指定して、フィルターをかけます   文字列 ¶  もっともシンプルなフィルターは文字列です。
検索メソッドに文字列を渡すと、Beautiful Soupは厳格に文字列を一致させます。
以下のコードは、ドキュメント内の<b>タグを全て見つけます。:  soup .
find_all ( 'b' )  # [<b>The Dormouse's story</b>]    バイト文字列を渡すと、Beautiful SoupはそれをUTF-8にエンコードされた文字列として扱います。
これを避けるには、代わりにUnicode文字列を渡します。    正規表現 ¶  正規表現オブジェクトを渡すと、Beautiful Soupはそれの match() メソッドを用いて、その正規表現に一致するものをマッチさせます。
以下のコードは、全ての”b”ではじまるつづりの名前のタグを見つけます。
“three sisters”ドキュメントでは、<body>タグと<b>タグにマッチします。:  import  re  for  tag  in  soup .
name )  # body  # b    以下のコードでは、タグ名に”t”のつづりを含むもの全てを見つけます。:  for  tag  in  soup .
name )  # html  # title      リスト ¶  フィルターにリストで引数をわたすと、Beautiful Soupはそのリストの内のいずれかにマッチした要素を返します。
以下のコードは、全ての<a>タグと<b>タグを見つけます。:  soup .
find_all ([ "a" ,  "b" ])  # [<b>The Dormouse's story</b>,  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      True値 ¶  True値 は全ての要素にマッチします。
以下のコードは、ドキュメント内の 全て のタグをみつけます。
ただし、ドキュメント本文のテキスト文字列はマッチされません。:  for  tag  in  soup .
name )  # html  # head  # title  # body  # p  # b  # p  # a  # a  # a  # p      関数 ¶  以上のフィルターで機能が足りないときは、自分で引数に要素をとる関数を定義することもできます。
その関数は、引数がマッチしたときは True を、そうでないときは False を返します。  以下の関数では、HTMLタグが “class” 属性を持ち、”id”属性を持たない場合に True を返します。:  def  has_class_but_no_id ( tag ):  return  tag .
has_attr ( 'id' )    この関数を find_all() に渡すと、”three sisters”ドキュメントから全ての<p>タグを取得できます。:  soup .
find_all ( has_class_but_no_id )  # [<p class="title"><b>The Dormouse's story</b></p>,  #  <p class="story">Once upon a time there were...</p>,  #  <p class="story">...</p>]    この関数は<p>タグだけを抽出します。
<a>タグは”class”と”id”の両方の属性を定義しているので抽出できません。
<html>や<title>のようなタグは、”class”を定義してないので、同様に抽出できません。  以下の関数は、HTMLタグがstringオブジェクトに囲まれているときは、 True を返します。(?
):  from  bs4  import  NavigableString  def  surrounded_by_strings ( tag ):  return  ( isinstance ( tag .
name  # p  # a  # a  # a  # p    これで検索メソッドの詳細をみていくことの準備ができました。     find_all() ¶  使い方: find_all( name , attrs , recursive , text , limit , **kwargs )  find_all() メソッドは、 Tag オブジェクトが持つ子孫要素のうち、引数に一致する 全ての 要素を見つけます。 フィルターの種類 でいくつかの例を挙げましたが、ここでもう少し説明します。:  soup .
find ( text = re .
compile ( "sisters" ))  # u'Once upon a time there were three little sisters; and their names were\n'    これらの使い方は、すでに説明してるものもあれば、初出のものもあります。 text や id に値を渡すのはどういう意味でしょうか？
なぜ、 find_all("p",  "title") は、CSSの”title”タグをもつ<p>タグを発見したのでしょうか？ find_all() の引数をみていきましょう。   name引数 ¶  find_all() の name 引数に値を渡すと、タグの名前だけを対象に検索が行われます。
名前がマッチしないタグと同じように、テキスト文字列は無視されます。  以下の例は、もっともシンプルな使い方です。:  soup .
find_all ( "title" )  # [<title>The Dormouse's story</title>]    フィルターの種類 で述べたように、 name 引数は文字列, 正規表現, リスト, 関数, True値をとることができます。    キーワード引数 ¶  どのような理解できない引数でも、タグの属性の一つとして解釈されます。
キーワード引数 id に値を渡すと、Beautiful Soupはタグの’id’属性に対してフィルタリングを行います。:  soup .
find_all ( id = 'link2' )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    キーワード引数 href に値を渡すと、Beautiful SoupはHTMLタグの’href’属性に対してフィルタリングを行います。:  soup .
compile ( "elsie" ))  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    キーワード引数の値もまた、 文字列 , 正規表現 , リスト , 関数 , True値 をとることができます。  次のコードは、 id 属性に値が入っている全てのタグを見つけます。このとき、値は何でもあっても構いません。:  soup .
find_all ( id = True )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    複数のキーワード引数を一度に渡すことによって、複数の属性についてフィルタリングできます。:  soup .
compile ( "elsie" ),  id = 'link1' )  # [<a class="sister" href="http://example.com/elsie" id="link1">three</a>]    HTML5の ‘data-*’ 属性など、いくつかの属性についてはキーワード引数として用いることができません。:  data_soup  =  BeautifulSoup ( '<div data-foo="value">foo!</div>' )  data_soup .
find_all ( data - foo = "value" )  # SyntaxError: keyword can't be an expression    しかし、これらの属性を辞書にして、キーワード引数 attrs として値を渡せばフィルタリングすることができます。:  data_soup .
find_all ( attrs = { "data-foo" :  "value" })  # [<div data-foo="value">foo!</div>]      CSSのクラスで検索 ¶  HTMLタグが持つCSSのクラスで検索をかけるのはとても便利です。
しかし”class”はPythonの予約語のため、 class をキーワード引数として用いると文法エラーになります。
そこで、Beautiful Soup 4.1.2からは、 class_ というキーワード引数でCSSのクラスを検索できるようになりました。:  soup .
find_all ( "a" ,  class_ = "sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    他のキーワード引数と同様、 class_ には文字列, 正規表現, 関数, True値を渡せます。:  soup .
find_all ( class_ = has_six_characters )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Tag オブジェクトの属性の 値が複数のとき を思い出してください。
それと同様に、あるCSSクラスを検索するときは、複数のCSSクラスに対してマッチさせられます。:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' )  css_soup .
find_all ( "p" ,  class_ = "body" )  # [<p class="body strikeout"></p>]    class 属性の値は、文字列としても検索できます。:  css_soup .
find_all ( "p" ,  class_ = "body strikeout" )  # [<p class="body strikeout"></p>]    しかし、文字列の値としての変数を検索することはできません。:  css_soup .
find_all ( "p" ,  class_ = "strikeout body" )  # []    もしあなたが2つ以上のクラスをまっちさせたいなら、CSSセレクトを使ってください。:  css_soup .
select ( "p.strikeout.body" )  # [<p class="body strikeout"></p>]    Beautiful Soupの古いバージョンでは、 class_ 引数は使えません。
そこで、以下に述べる attrs トリックを使うことができます。
これは”class”をkeyに持つ辞書を attrs 引数に渡して、検索することができます。
この辞書のvalueには、文字列, 正規表現などが使えます。:  soup .
find_all ( "a" ,  attrs = { "class" :  "sister" })  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      text引数 ¶  text 引数で、タグに挟まれている文字列を対象に検索することができます。 name 引数やキーワード引数のように、 文字列 , 正規表現 , リスト , 関数 , True値 が使えます。
以下の例をごらんください。:  soup .
find_all ( text = "Elsie" )  # [u'Elsie']  soup .
find_all ( text = [ "Tillie" ,  "Elsie" ,  "Lacie" ])  # [u'Elsie', u'Lacie', u'Tillie']  soup .
find_all ( text = re .
compile ( "Dormouse" ))  [ u"The Dormouse's story" ,  u"The Dormouse's story" ]  def  is_the_only_string_within_a_tag ( s ):  """Return True if this string is the only child of its parent tag."""
find_all ( text = is_the_only_string_within_a_tag )  # [u"The Dormouse's story", u"The Dormouse's story", u'Elsie', u'Lacie', u'Tillie', u'...']    text 引数はテキスト文字列の検索ですが、これにタグの検索を組みわせることもできます。
Beautiful Soupは、 text 引数で指定した文字列を .string にもつタグ全てを見つけます。
次のコードは、 .string に “Elsie”を持つ<a>タグを見つけます。:  soup .
find_all ( "a" ,  text = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]      limit引数 ¶  find_all() メソッドは、指定したフィルターにマッチした全てのタグと文字列を返します。
これはドキュメントが大きいときは時間がかかります。
もし、 全ての 結果を必要としなければ、 limit 引数で取得する数を指定することができます。  “three siters”ドキュメントには3つのリンクがある、しかし以下のコードははじめの2つしか見つけない。:  soup .
find_all ( "a" ,  limit = 2 )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]      recursive引数 ¶  mytag.find_all() を実行すると、Beautiful Soupは、 mytag の全ての子孫要素を調べます。
(子要素、子要素の子要素、そのまた子要素というかんじで、、)
もし、直下の子要素しか調べたくなければ、 recursive=False という引数を渡せばよいです。
以下で違いをみてみましょう。:  soup .
find_all ( "title" ,  recursive = False )  # []    これはドキュメントの一部です。:  <html>
 <head>
  <title>
   The Dormouse's story
  </title>
 </head>
...
  このドキュメントにおいて、<title>タグは<html>の下にはあるが、 直下 にあるわけではありません。
Beautiful Soupが<title>タグを見つけることができるのは、<html>タグ以下の全ての子孫要素を探してよいときだけです。
もし、 find_all() の引数に recurive=False という<html>タグの直下のみを検索するという制限がかかっていたら、<title>タグを見つけることはできません。  Beautiful Soupは、多くのパースツリーを検索するメソッドを提供しています。
それら多くは共通する引数を持ちます。 find_all() の name , attrs , text , limit , キーワード引数は、他の多くのメソッドにも対応しています。
しかし、 recursive 引数は、 find_all() , find() の2つのメソッドしか対応していません。 find_parents() のようなメソッドに、引数 recursive=False を渡しても意味がありません。    ショートカット ¶  find_all() はBeautiful Soupの検索APIの中で、一番使われるものなので、ショートカットがあります。 Beautiful  Soup オブジェクトや Tag オブジェクトを関数のように扱って、 find_all() メソッドを呼び出すことができます。
以下の2行は等価です。:  soup .
find_all ( "a" )  soup ( "a" )    以下の2行もまた等価です。:  soup .
find_all ( text = True )  soup .
title ( text = True )       find() ¶  使い方: find( name , attrs , recursive , text , **kwargs )  find_all() メソッドは、検索結果を得るためにHTMLドキュメント全部をスキャンします。
しかし、1つだけの検索結果が必要なときがあります。
もし、HTMLドキュメントに<body>タグが1つだけなら、HTMLドキュメント全体をスキャンするのは時間の無駄です。
その場合は find_all() メソッドに limit=1 という引数を渡さずに、 find() メソッドを使うことができます。
以下の2行は、ほぼ等価です。:  soup .
find ( 'title' )  # <title>The Dormouse's story</title>    ただ1つ違う点は、 find_all() は要素1のリストを返し、 find() は要素をそのまま返すことです。  find_all() が何もみつけられないときは空リストを返します。 find() が何もみつけられないときは、 None を返します。:  print ( soup .
find ( "nosuchtag" ))  # None    タグ名で探索 で出てきた soup.head.title で探索する方法を覚えていますか？ おｋれは、 find() についても適用できます。:  soup .
find ( "title" )  # <title>The Dormouse's story</title>      find_parents() / find_parent() ¶  使い方: find_parents( name , attrs , text , limit , **kwargs )  使い方: find_parent( name , attrs , text , **kwargs )  ここまで find_all() と find() について述べてきました。
Beautiful Soup APIにはパースツリーを検索するためのメソッドが、あと10あります。
しかし、おそれる必要はありません。
そのうち5つは、 find_all() と基本的に同じです。
そして、のこりの5つは find() と基本的に同じです。
違いは、ツリーのどの部分を検索対象にするのかという点のみです。  最初に、 find_parents() と find_parent() を見てみましょう。 find_all() と find() がタグの子孫を見て、ツリーを下りていったことを思い出してください。 find_parents() と find_parent() は逆です。
これらはタグや文字列の親をみて、ツリーを’上に’検索していきます。
以下の”three daughters”ドキュメントの例で、深いレベルにある文字列から検索していく様子を見てください。:  a_string = soup.find(text="Lacie")
a_string
# u'Lacie'

a_string.find_parents("a")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]

a_string.find_parent("p")
# <p class="story">Once upon a time there were three little sisters; and their names were
#  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;
#  and they lived at the bottom of a well.</p>

a_string.find_parents("p", class="title")
# []   3つの<a>タグのうちの1つは、検索の起点になる文字列の直接の親要素なので、それが返されました。
3つの<p>タグのうちの1つは、起点の文字列の直接の親ではありませんが、やはりそれも返されました。
CSSクラス”title”をもつ<p>タグは、”three daughers”ドキュメント中にはあるのですが、起点の文字列の親要素ではないので、 find_parents() では見つけることができませんでした。  find_parent() と find_parents() のつながりはわかったでしょうか。 .parent と .parents 属性については、以前に述べてあります。
そのつながりはとても強いです。
これらの検索メソッドは実際には .parents で、全ての親要素の連なりをイテレートして扱います。
そして、要素それぞれについてフィルターにマッチするかどうかをチェックします。    find_next_siblings() / find_next_sibling() ¶  使い方: find_next_siblings( name , attrs , text , limit , **kwargs )  使い方: find_next_sibling( name , attrs , text , **kwargs )  これらのメソッドは、後方にある兄弟要素を扱うのに、 .next_siblings を使います。 find_next_siblings() メソッドはマッチする兄弟要素を全て返し、 find_next_sibling() は最初の一つを返します。:  first_link  =  soup .
find_next_sibling ( "p" )  # <p class="story">...</p>      find_previous_siblings() / find_previous_sibling() ¶  使い方: find_previous_siblings( name , attrs , text , limit , **kwargs )  使い方: find_previous_sibling( name , attrs , text , **kwargs )  これらのメソッドは、HTMLドキュメントの前方にあった兄弟要素を扱うのに .previous_siblings を使います。 find_previous_siblings() メソッドはマッチする兄弟要素を全て返し、 find_previous_sibling() は最初の一つを返します。:  last_link  =  soup .
find_previous_sibling ( "p" )  # <p class="title"><b>The Dormouse's story</b></p>      find_all_next() / find_next() ¶  使い方: find_all_next( name , attrs , text , limit , **kwargs )  使い方: find_next( name , attrs , text , **kwargs )  これらのメソッドは、HTMLドキュメントのその後にあらわれるタグと文字列の要素全てイテレートして扱うために、 .next_elements メソッドを使います。 find_all_next() メソッドはマッチするもの全てを返し、 find_next() は最初にマッチしたものを返します。(!要改善):  first_link  =  soup .
find_all_next ( text = True )  # [u'Elsie', u',\n', u'Lacie', u' and\n', u'Tillie',  #  u';\nand they lived at the bottom of a well.
find_next ( "p" )  # <p class="story">...</p>    最初の例では、起点となった<a>タグに挟まれている、文字列”Elsie”が返されています。
２番めの例では、起点となった<a>タグと同じパートじゃないにも関わらず、最後の<p>タグが示されています。
これらのメソッドでは、問題はフィルターにマッチするか否かと、スタートした要素よりも後にでてきたかということが問われます。(!要改善)    find_all_previous() / find_previous() ¶  使い方: find_all_previous( name , attrs , text , limit , **kwargs )  使い方: find_previous( name , attrs , text , **kwargs )  これらのメソッドは、ドキュメントの起点のタグの前にあらわれるタグと文字列の要素全てをイテレートして扱うために、 .previous_elements メソッドを使います。(!要改善):  first_link  =  soup .
find_previous ( "title" )  # <title>The Dormouse's story</title>    find_all_previous("p") は”three sisters”ドキュメントの最初の段落を見つけます。(class=”title”のやつです)
しかし、第２段落でも見つけます。<p>タグは内に起点にした<a>タグを含んでいます。
驚きすぎないでください。
我々は、起点のタグより前方に現れた全てのタグを見ているのです。<a>タグを挟んでいる<p>タグは、<a>タグよりも前に示されねばなりません。(!要改善)    CSSセレクタ ¶  Beautiful Soupは、よく使われるCSSセレクタをほとんどサポートしています。 Tag オブジェクトや BeautifulSoup オブジェクトに .select() メソッドで文字列を渡すだけで使えます。  タグを見つけるには次のようにします。:  soup .
select ( "p nth-of-type(3)" )  # [<p class="story">...</p>]    あるタグより後ろの指定されたタグを見つけます。:  soup .
select ( "html head title" )  # [<title>The Dormouse's story</title>]    あるタグの直後の指定されたタグを見つけます。:  soup .
select ( "body > a" )  # []    タグの兄弟要素を見つけます。:  soup .
select ( "#link1 + .sister" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    CSSクラスによってタグを見つけます。:  soup .
select ( "[class~=sister]" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    CSSのIDによってタグを見つけます。:  soup .
select ( "a#link2" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    指定の属性の有無でタグを見つけます。:  soup .
select ( 'a[href]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    属性が持つ値によってタグを見つけます。:  soup .
select ( 'a[href*=".com/el"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    languageコードで、マッチさせます。:  multilingual_markup  =  """  <p lang="en">Hello</p>  <p lang="en-us">Howdy, y'all</p>  <p lang="en-gb">Pip-pip, old fruit</p>  <p lang="fr">Bonjour mes amis</p>  """  multilingual_soup  =  BeautifulSoup ( multilingual_markup )  multilingual_soup .
select ( 'p[lang|=en]' )  # [<p lang="en">Hello</p>,  #  <p lang="en-us">Howdy, y'all</p>,  #  <p lang="en-gb">Pip-pip, old fruit</p>]    このやり方は、CSSセレクタの文法を知っているユーザにとっては、とても便利です。
これでBeautiful Soup APIの全てのキモを使えるようになりました。
もしCSSセレクタを使いこなしたいなら、lxmlを使ってみるのもよいでしょう。
lxmlは処理がとても速く、さらに多くのCSSセレクタをサポートしています。
しかし、ここではBeautiful Soup APIを使って、シンプルなCSSセレクタの組み合わせるによる方法を説明しました。     パースツリーを修正 ¶  Beautiful Soupの主な強みは、パースツリーの検索するところにあります。
しかしまた、Beautiful Soupは、ツリーを修正したり、変更したツリーを新しいHTMLやXMLのドキュメントに出力することもできます。   名前や属性の変更 ¶  属性 の節でも述べましたが、タグの名前変更、属性値の変更、追加、削除ができます。:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' )  tag  =  soup .
name  =  "blockquote"  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>      .string の修正 ¶  Tag オブジェクトの .string を変更すると、そのタグが挟む文字列がその値に変更されます。:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
tag  # <a href="http://example.com/">New link text.</a>    注意点: 変更したタグが他のタグを挟んでいると、それらのタグ全てが破壊されます。    append() ¶  Tag.append() により、タグが挟んでいる文字列に追加をすることができます。
まるでPythonのリストの .append() のように作用します。:  soup  =  BeautifulSoup ( "<a>Foo</a>" )  soup .
append ( "Bar" )  soup  # <html><head></head><body><a>FooBar</a></body></html>  soup .
contents  # [u'Foo', u'Bar']      BeautifulSoup.new_string() / .new_tag() ¶  ドキュメントに文字列を加えたいときは、Pythonの文字列を append() に渡してください。
もしくは、 factory method の BeautifulSoup.new_string() を呼出してください。:  soup  =  BeautifulSoup ( "<b></b>" )  tag  =  soup .
append ( "Hello" )  new_string  =  soup .
new_string ( " there" )  tag .
contents  # [u'Hello', u' there']    新しいコメントや 他の NavigableString のサブクラスを生成したいときは、 new_string() の第２引数にそのクラスを渡してください。:  from  bs4  import  Comment  new_comment  =  soup .
new_string ( "Nice to see you."
,  Comment )  tag .
contents  # [u'Hello', u' there', u'Nice to see you.']
(これはBeautiful Soup 4.2.1 の新機能です)  完全に新しいタグを生成したいときは、factory methodの BeautifulSoup.new_tag() を呼び出してください。:  soup  =  BeautifulSoup ( "<b></b>" )  original_tag  =  soup .
original_tag  # <b><a href="http://www.example.com">Link text.</a></b>    第１引数のタグ名だけは必須です。    insert() ¶  Tag.insert() は Tag.append() に似ています。
違うのは、タグの .contents の最後以外にも、要素を挿入できるという点です。:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
contents  # [u'I linked to ', u'but did not endorse', <i>example.com</i>]      insert_before() / insert_after() ¶  insert_before() メソッドは、あるタグの直前に、別のタグや文字列を挿入します。:  soup  =  BeautifulSoup ( "<b>stop</b>" )  tag  =  soup .
b  # <b><i>Don't</i>stop</b>    insert_after() メソッドは、あるタグの直後に、別のタグや文字列を挿入します。:  soup .
insert_after ( soup .
new_string ( " ever " ))  soup .
b  # <b><i>Don't</i> ever stop</b>  soup .
contents  # [<i>Don't</i>, u' ever ', u'stop']      clear() ¶  Tag.clear() は、タグが挟んでいるcontentsを削除します。(訳注:要チェック?
):  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
clear ()  tag  # <a href="http://example.com/"></a>      extract() ¶  PageElement.extract() はツリーからタグや文字列を除去します。
返値は、その抽出されたタグや文字列です。:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
parent )  None    このとき、2つのパースツリーがあります。1つは BeautifulSoup オブジェクトを根ノードとしたあなたがパースしたドキュメントです。もう1つは、抽出したタグを根ノードとするものです。抽出した要素の子要素を extract でコールできます。:  my_string  =  i_tag .
extract ()  my_string  # u'example.com'  print ( my_string .
parent )  # None  i_tag  # <i></i>      decompose() ¶  Tag.decompose() はパースツリーからタグを除去します。 そのタグと挟んでいるcontentsを完全に削除します  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
a  soup .
decompose ()  a_tag  # <a href="http://example.com/">I linked to</a>      replace_with() ¶  PageElement.replace_with() はツリーからタグと文字列を除去し、
引数に与えたタグや文字をその代わりに置き換えます。:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
string  =  "example.net"  a_tag .
replace_with ( new_tag )  a_tag  # <a href="http://example.com/">I linked to <b>example.net</b></a>    replace_with() は置き換えられたタグや文字列を返します。
それを、調査したり、ツリーの他の部分に加えることができます。    wrap() ¶  PageElement.wrap() は、その要素を引数で指定したタグを挟みます。
新しく挟まれたものを返します。  soup = BeautifulSoup("<p>I wish I was bold.</p>")
soup.p.string.wrap(soup.new_tag("b"))
# <b>I wish I was bold.</b>

soup.p.wrap(soup.new_tag("div")
# <div><p><b>I wish I was bold.</b></p></div>   このメソッドは、Beautiful Soup 4.0.5 からの新機能です。    unwrap() ¶  Tag.unwrap() は wrap() の反対です。
それは、タグの中身がなんであれ、それとタグを置き換えます。
マークアップをはずすのに便利です。(訳注：やりなおし??
):  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
unwrap ()  a_tag  # <a href="http://example.com/">I linked to example.com</a>    replace_with() のように、 unwrap() は置き換えられたタグを返します。     出力 ¶   きれいに出力 ¶  prettify() メソッドは、BeautifulSoupパースツリーを、1行に1タグのきれいにフォーマットされたUnicode文字列に変換します。:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  soup .
prettify ())  # <html>  #  <head>  #  </head>  #  <body>  #   <a href="http://example.com/">  #    I linked to  #    <i>  #     example.com  #    </i>  #   </a>  #  </body>  # </html>    prettify() メソッドは、 トップレベルの BeautifulSoup オブジェクトでも、それ以外の Tag オブジェクトでも呼び出すことができます。:  print ( soup .
prettify ())  # <a href="http://example.com/">  #  I linked to  #  <i>  #   example.com  #  </i>  # </a>      一行に出力 ¶  フォーマットされたテキストではなく単なる文字列がほしければ、 BeautifulSoup や Tag オブジェクトの unicode() や str() を呼び出せます。:  str ( soup )  # '<html><head></head><body><a href="http://example.com/">I linked to <i>example.com</i></a></body></html>'  unicode ( soup .
a )  # u'<a href="http://example.com/">I linked to <i>example.com</i></a>'    str() 関数は、UTF-8にエンコードされた文字列を返します。
他のオプションを知りたければ、 エンコード をみてください。  バイト文字列を得るのに、 encode() を用いることもできます。 decode() を用いると、Unicodeを得ることができます。    フォーマットを指定 ¶  Beautiful Soupに、”&lquot;”のようなHTMLエンティティを含んだドキュメントを渡すと、それらはUnicodeキャラクタに変換されます。:  soup  =  BeautifulSoup ( "&ldquo;Dammit!&rdquo; he said."
)  unicode ( soup )  # u'<html><head></head><body>\u201cDammit!\u201d he said.</body></html>'    そのドキュメントを文字列に変換すると、Unicode文字列はUTF-8キャラクタとしてエンコードされます。
それを、HTMLエンティティに戻すことはできません。:  str ( soup )  # '<html><head></head><body>\xe2\x80\x9cDammit!\xe2\x80\x9d he said.</body></html>'    デフォルトでは、出力するときエスケープされるのは、裸の&と角かっこのみです。
これらは、”&amp;”,”&lt;”,”&gt”に変換されます。
そのためBeautifulSoupはうっかり不正確なHTMLやXMLを生成することはありません。:  soup  =  BeautifulSoup ( "<p>The law firm of Dewey, Cheatem, & Howe</p>" )  soup .
p  # <p>The law firm of Dewey, Cheatem, &amp; Howe</p>  soup  =  BeautifulSoup ( '<a href="http://example.com/?foo=val1&bar=val2">A link</a>' )  soup .
a  # <a href="http://example.com/?foo=val1&amp;bar=val2">A link</a>    prettify() , encode() , decode() の formatter 属性に値を与えると、出力を変更することができます。 formatter は、4種類の値をとり得ます。  デフォルトでは、 formatter="minimal" です。
文字列は、Beautiful Soupが正しいHTML/XMLを生成することを十分に保証するように、加工されるだけです。:  french  =  "<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>"  soup  =  BeautifulSoup ( french )  print ( soup .
prettify ( formatter = "minimal" ))  # <html>  #  <body>  #   <p>  #    Il a dit &lt;&lt;Sacrﾃｩ bleu!&gt;&gt;  #   </p>  #  </body>  # </html>    もし、 formatter="html" を渡せば、BSは 可能なときはいつでも、Unicode文字列を HTMLエンティティに変換します。:  print ( soup .
prettify ( formatter = "html" ))  # <html>  #  <body>  #   <p>  #    Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;  #   </p>  #  </body>  # </html>    もし、 formatter=None を渡せば、BSは出力においてまったく文字列を修正しません。
これは、最速のオプションですが、BSが正しくないHTML/XMLを生成することになります。
次の例をご覧ください。:  print ( soup .
prettify ( formatter = None ))  # <html>  #  <body>  #   <p>  #    Il a dit <<Sacrﾃｩ bleu!>>  #   </p>  #  </body>  # </html>  link_soup  =  BeautifulSoup ( '<a href="http://example.com/?foo=val1&bar=val2">A link</a>' )  print ( link_soup .
encode ( formatter = None ))  # <a href="http://example.com/?foo=val1&bar=val2">A link</a>    formatter に関数を渡すと、ドキュメントの文字列や属性値のたびに、BSはその関数をコールします。
関数内で望むことはなんであれできます。
以下では、formatterは文字列を大文字にコンバートし、他には何もしません。:  def  uppercase ( str ):  return  str .
upper ()  print ( soup .
prettify ( formatter = uppercase ))  # <html>  #  <body>  #   <p>  #    IL A DIT <<SACRﾃ BLEU!>>  #   </p>  #  </body>  # </html>  print ( link_soup .
prettify ( formatter = uppercase ))  # <a href="HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2">  #  A LINK  # </a>    もしあなたがあなたの関数を書いたなら、あなたは bs4.dammit の EntitySubstitution クラスについて知るべきです。
このクラスは、BSの標準的なformatter をクラスメソッドとして内包します。
“html” formatterは EntitySubstitution.substitute_html ,
“minimal” formatterは EntitySubstitution.substitute_xml です。
あなたは、これらの関数を、 formatter==html や formatter==minimal をシュミレーションします。
しかし、それに加えて他のこともします。  これは例です。UnicodeキャラクタをHTMLエンティティに置換します。可能なときはいつでも。
しかし、 また 全ての文字列を大文字に変換します。:  from  bs4.dammit  import  EntitySubstitution  def  uppercase_and_substitute_html_entities ( str ):  return  EntitySubstitution .
substitute_html ( str .
upper ())  print ( soup .
prettify ( formatter = uppercase_and_substitute_html_entities ))  # <html>  #  <body>  #   <p>  #    IL A DIT &lt;&lt;SACR&Eacute; BLEU!&gt;&gt;  #   </p>  #  </body>  # </html>    最後に一点(最終通告?
): もし CData オブジェクトを生成したときは、そのオブジェクト内のテキストは 正確にあるがまま、フォーマットされることなく いつも表されます。
BSは formatterメソッドを呼出します。あなたがカスタムメソッドを書いた場合にのみ。どういうカスタムメソッド化というと、全てのドキュメント内の文字列やなにかをcountする。しかし、それは返り値を無視します。:  from  bs4.element  import  CData  soup  =  BeautifulSoup ( "<a></a>" )  soup .
prettify ( formatter = "xml" ))  # <a>  #  <!
[CDATA[one < three]]>  # </a>      get_text() ¶  ドキュメントやタグのテキスト部分だけが取得したいときは、 get_text() メソッドを使います。
それは、全ドキュメントや下層のタグを、ユニコードの単一文字列として返します。:  markup  =  '<a href="http://example.com/"> \n I linked to <i>example.com</i> \n </a>'  soup  =  BeautifulSoup ( markup )  soup .
get_text ()  u' \n I linked to example.com \n '  soup .
get_text ()  u'example.com'    テキストをまとめる際の区切り文字を指定することができます。:  # soup.get_text("|")  u' \n I linked to |example.com| \n '    各文字列パーツの最初と最後の空白を除去することもできます。:  # soup.get_text("|", strip=True)  u'I linked to|example.com'    空白を除去するのに、 stripped_strings ジェネレーターを使って処理することもできます。:  [ text  for  text  in  soup .
stripped_strings ]  # [u'I linked to', u'example.com']       パーサーの指定 ¶  もし貴方がいくつかのhtmlをパースしたいなら、あなたは、 Beautiful  Soup コンストラクタに、マークアップをダンプできる。
それはたぶんうまくいきます。
Beautiful Soupはパーサーを選んで、データをパースします。
しかし、どのパーサーが使われるか変更するために、コンストラクタに渡すいくつかの引数があります  １つ目の BeautifulSoup コンストラクタの引数は、 あなたがパースしたいマークアップの、文字列または開いているファイルハンドルです。
２つ目の引数は、 どのように マークアップをぱーすするかについてです。  もし何も指定しなかった場合は、インストールされているなかで最高のHTMLパーサーを使います。
Beautiful Soupは、lxmlのパーサーを最高のものとしています。そして、html5libとPythonの組み込みパーサー。
あなたは次のうちの一つを指定することで、これを上書きできます。   パースしたいマークアップの種類: サポートしているのは、”html”, “xml”, “html5”です。    パーサーライブラリの名前: オプションとしてサポートしているのは、”lxml”, “html5lib”, (Pythonの組み込みHTMLパーサーである) “html.parser”。   この パーサーのインストール の章は、サポートしているパーサーを比較します。  もし適切なパーサーをインストールしていないときは、Beautiful Soupはあなたのリクエストを無視し、違うパーサーを選びます。
現在、ただ一つサポートされているXMLパーサーは、lxmlです。
もし、lxmlをインストールしてないとき、XMLの要求はあなたに何も与えませんし、”lxml”へのリクエストも動きません。(要改善!)
パーサーの違い ¶  Beautiful Soupは多くの異なるパーサーに同じインターフェースを提供しています。
しかし、パーサーはそれぞれは異なります。
パーサーが異なると、同じドキュメントでも、生成されるパースツリーは異なってきます。
HTMLパーサーとXMLパーサーには大きな違いがあります。
以下は、短いドキュメントをHTMLとしてパースしたものです。:  BeautifulSoup ( "<a><b /></a>" )  # <html><head></head><body><a><b></b></a></body></html>    空の<b />タグは、正式なHTMLではないため、パーサーはそれを<b></b>のタグの組に変換します。  以下は、同じドキュメントをXMLとしてパースしたものです。
(これを実行するにはlxmlをインストールしておく必要があります)
<b />タグはそのまま残っており、ドキュメントはXML宣言が<html>タグの代わりに加えられたことに気づいてください。:  BeautifulSoup ( "<a><b /></a>" ,  "xml" )  # <?xml version="1.0" encoding="utf-8"?>  # <a><b/></a>    HTMLパーサー同士でも、違いはあります。
完全な形のHTMLドキュメントをBeautiful Soupに与えたときは、その違いは問題になりません。
あるパーサーは、他のパーサーよりも速いでしょう。
しかし、それらは全て元のHTMLドキュメントを正確に反映したデータ構造を与えるでしょう。  しかし、不完全な形のHTMLドキュメントのときは、異なるパーサーは異なる結果を出力します。
以下は、lxmlのHTMLパーサーによってパースされた短く不正なドキュメントです。
ぶらさがっている</p>タグは、単に無視されていることに気づいてください。:  BeautifulSoup ( "<a></p>" ,  "lxml" )  # <html><body><a></a></body></html>    以下は、html5libによってパースされた同じドキュメントです。:  BeautifulSoup ( "<a></p>" ,  "html5lib" )  # <html><head></head><body><a><p></p></a></body></html>    ぶらさがっている</p>タグを無視する代わりに、html5libは、それを開始の<p>タグと組にしました。
このパーサーはまた、ドキュメントに空の<head>タグも加えました。  以下は、Python組み込みのHTMLパーサーで同じドキュメントをパースしたものです。:  BeautifulSoup ( "<a></p>" ,  "html.parser" )  # <a></a>    html5libのように、このパーサーは終わりの</p>タグを無視します。
html5libとは違い、このパーサーは<body>タグを加えて正しい書式のHTMLドキュメントを作成しようとはしません。
lxmlとは違い、なんとかして<html>タグを加えようとはしません。  “<a></p>”というドキュメントは不正なので、これについての”正しい”処理方法はありません。
html5libパーサーはhtml5標準のいち部分のテクニックを使います。
それは、ただしい主張を正しい方法についてします。しかし、これらの3つの方法全て、道理に合っています。(?あとで再チェック)  パーサー間の違いは、あなたのスクリプトにも影響するでしょう。
もし、スクリプトを他の人に配布したり、複数の計算機で実行しようとするならば、 Beautiful  Soup コンストラクタについてパーサーを指定するべきです。
そうすることによって、あなたがパースした方法と違うやりかたでドキュメントをパースする可能性を減らすでしょう。     エンコード ¶  HTMLやXMLドキュメントは全て、ASCIIやUTF-8のような特定の文字コードで書かれています。
しかし、BeautifulSoupにドキュメントをロードすると、それらはUnicode型に変換されます。:  markup  =  "<h1>Sacr \xc3\xa9 bleu!</h1>"  soup  =  BeautifulSoup ( markup )  soup .
string  # u'Sacr\xe9 bleu!'
これは魔法ではありません。Beautiful Soupは Unicode, Dammit を内部でライブラリとして呼び出し、文字コードを判別してUnicodeに変換するのに使っています。
自動判別された文字コードは、 BeautifulSoup オブジェクトの .original_encoding 属性で参照することができます。:  soup .
original_encoding  'utf-8'    Unicode, Dammit はほとんどの場合正しく判別しますが、たまに失敗します。
たいてい適切に判別しますが、バイト毎の検索の場合は、とてもながい時間がかかります。
もし、ドキュメントの文字コードが分かっているのなら、失敗や遅延を避けるために、 BeautifulSoup コンストラクタに from_encoding として渡すとよいです。  次の例は、ISO-8859-8(訳注:ラテン文字等の文字コード)で書かれたドキュメントです。
このドキュメントは短いために、Unicode, Dammitはそれが何か判別できず、それをISO-8859-7(訳注:ギリシア文字等の文字コード)と誤認します。:  markup = b"<h1>\xed\xe5\xec\xf9</h1>"
soup = BeautifulSoup(markup)
soup.h1
<h1>νεμω</h1>
soup.original_encoding
'ISO-8859-7'   正しい from_encoding を渡すことで、これを正すことができます。:  soup = BeautifulSoup(markup, from_encoding="iso-8859-8")
soup.h1
<h1>ם ו ל ש</h1>
soup.original_encoding
'iso8859-8'   (通常、UTF-8のドキュメントは複数の文字コードを含むことができますが、) ごくまれに、変換できない文字をユニコードの特殊文字”REPLACEMENT CHARACTER” (U+FFFD,�) に置き換えることがあります。
Unicode, Dammit がこれを行うときは、同時に、 UnicodeDammit か BeautifulSoup オブジェクトの .contains_replacement_characters 属性にTrueをセットします。
これにより、変換後のユニコードの文字列は、元の文字コードの文字列を正確に表現しておらず、いくつかのデータが損なわれているということがわかります。
もし、 .contains_replacement_characters が False のときは、ドキュメント内に特殊文字�があっても、それは(この段落の�のように)もともとあり、データは損なわれていないということです。   出力のエンコード ¶  Beautiful Soupでドキュメントを出力すると、元のドキュメントがUTF-8でなくても、UTF-8で出力されます。
次の例は、Latin-1で書かれたドキュメントについてです。:  markup  =  b '''  <html>  <head>  <meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" />  </head>  <body>  <p>Sacr \xe9 bleu!</p>  </body>  </html>  '''  soup  =  BeautifulSoup ( markup )  print ( soup .
prettify ())  # <html>  #  <head>  #   <meta content="text/html; charset=utf-8" http-equiv="Content-type" />  #  </head>  #  <body>  #   <p>  #    Sacrﾃｩ bleu!
#   </p>  #  </body>  # </html>    <meta>タグは書き換えられ、ドキュメントが現在UTF-8であることを示しています。:  ..
If you don't want UTF-8, you can pass an encoding into ``prettify()``::   UTF-8以外で出力したいときは、 prettify() にその文字コードを渡してください。:  print ( soup .
prettify ( "latin-1" ))  # <html>  #  <head>  #   <meta content="text/html; charset=latin-1" http-equiv="Content-type" />  # ...
   Pythonのstr型であるかのように、 BeautifulSoup オブジェクトや、その要素のencode()をコールすることができます。:  soup .
encode ( "latin-1" )  # '<p>Sacr\xe9 bleu!</p>'  soup .
encode ( "utf-8" )  # '<p>Sacr\xc3\xa9 bleu!</p>'    あなたが選んだ文字コードでは表せない文字は、XMLエンティティリファレンスの数字に変換されます。
次の例は、スノーマンのユニコード文字を含んだドキュメントです。:  markup  =  u"<b> \N{SNOWMAN} </b>"  snowman_soup  =  BeautifulSoup ( markup )  tag  =  snowman_soup .
b    スノーマンの文字はUTF-8のドキュメントに組み込めます。(それは☃と表示されます。しかし、ISO-Latin-1やASCIIにはその文字がありません。そこで、これらの文字コードでは”&#9731”に変換されます。):  print ( tag .
encode ( "utf-8" ))  # <b>☃</b>  print  tag .
encode ( "latin-1" )  # <b>&#9731;</b>  print  tag .
encode ( "ascii" )  # <b>&#9731;</b>      Unicode, Dammit ¶  Beautiful Soup 抜きで、Unicode, Dammitを使えます。
文字コードがわからないデータを持つときや、Unicodeにそのデータを変換したいときは、それは便利です。:  from  bs4  import  UnicodeDammit  dammit  =  UnicodeDammit ( "Sacr \xc3\xa9 bleu!"
)  print ( dammit .
original_encoding  # 'utf-8'    Pythonライブラリ chardet か cchardet をインストールしていれば、Unicode, Dammitはさらに正確に文字コードを推測できます。:  dammit  =  UnicodeDammit ( "Sacr \xe9 bleu!"
original_encoding  # 'latin-1'    Unicode, Dammitには、Beautiful Soupが使わない2つの機能があります。   スマート引用符 ¶  (訳注: スマート引用符とは、引用符’で左右の向き(open/close)が区別されているもののことです。
ASCIIコードやシフトJISの引用符は区別されていません。
[ 参考リンク ])  Unicode, Dammitは Microsoftスマート引用符を、HTMLやXMLのエンティティに変換します。:  markup  =  b "<p>I just \x93 love \x94 Microsoft Word \x92 s smart quotes</p>"  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "html" ) .
unicode_markup  # u'<p>I just &ldquo;love&rdquo; Microsoft Word&rsquo;s smart quotes</p>'  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "xml" ) .
unicode_markup  # u'<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'    Microsoftスマート引用符をASCII引用符に変換できます。:  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "ascii" ) .
unicode_markup  # u'<p>I just "love" Microsoft Word\'s smart quotes</p>'    できればこの機能を便利に使ってほしいですが、Beautiful Soupはそれを使いません。
Beautiful Soupは、他の文字と同じように、Microsoftスマート引用符をUnicodeキャラクタに変換するという、デフォルトの振るまいを選びます。:  UnicodeDammit ( markup ,  [ "windows-1252" ]) .
unicode_markup  # u'<p>I just \u201clove\u201d Microsoft Word\u2019s smart quotes</p>'      複数の文字コード ¶  ときどき、ほぼUTF-8で書かれているが、一部Microsoftスマート引用符のような文字コードがWindows-1252の文字を含むドキュメントがあります。:  snowmen  =  ( u" \N{SNOWMAN} "  *  3 )  quote  =  ( u" \N{LEFT DOUBLE QUOTATION MARK} I like snowmen!
encode ( "windows_1252" )    このドキュメントは扱いに困ります。
スノーマンはUTF-8ですが、スマート引用符はWindows-1252です。
スノーマンか引用符のどちらかしか表示できません。:  print ( doc )  # ☃☃☃�I like snowmen!�  print ( doc .
decode ( "windows-1252" ))  # â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”    ドキュメントをUTF-8としてデコードすると、 UnicodeDecodeError が発生し、Windows-1252でデコードすると意味不明(gibberish?
)なことになります。
幸いなことに、 UnicodeDammit.detwingle() はその文字をpure UTF-8に変換し、それをUnicodeにデコードし、スノーマンと引用符を並べて表示することを許可します。:  new_doc  =  UnicodeDammit .
decode ( "utf8" ))  # ☃☃☃“I like snowmen!”    UnicodeDammit.detwingle() UTF-8に埋め込まれたWindows-1252の文字を扱う方法(とその逆)のみを知っています。しかしこれは、よくあるケースではありません。  データを BeautifulSoup や UnicodeDammit コンストラクタに渡す前に、 UnicodeDammit.detwingle() をコールしなければならないことに注意してください。
Beautiful Soupは 何らかの単一の文字コードでドキュメントが記されていると想定しています。
もし、UTF-8とWindows-1252の両方を含むドキュメントを渡したら、ドキュメント全体がWindows-1252と判断しがちです。そして、そしてそのドキュメントの出力は、 ` ﾃ｢ﾋ愴津｢ﾋ愴津｢ﾋ愴停廬 like snowmen!窶拜` のようになります。  UnicodeDammit.detwingle() はBeautiful Soup 4.1.0からの機能です。      ドキュメントの一部をパース ¶  あるドキュメントの<a>タグに対してBeautiful Soupを使いたい場合、ドキュメント全部をパースして、その中から<a>タグを探すのは、時間とメモリの無駄です。
最初にでてくる<a>タグ以外を全て無視すれば、処理は速くなります。 SoupStrainer クラスは、与えられたドキュメントのどの部分をパースするかを選ぶことができます。
そのためには、 SoupStrainer を作成し、それを BeautifulSoup コンストラクタに parse_only 属性として渡すだけです。  (この機能はhtml5libパーサーを使っているときは、使えないことにご注意ください。
もしhtml5libを使うときはどんなときでも、ドキュメント全体がパースされます。
これは、html5libがパースツリーをそのように継続的に再構築するためです。
もし、ドキュメントの一部がパースツリーに組み込まれてない場合は、それは裏ッシュします。
それをさけるためには、例において、Beautiful SoupがPythonの組み込みパーサーを利用させてください)   SoupStrainer ¶  SoupStrainer (スープ漉し器)クラスは、 パースツリーを検索 : するときの典型的なメソッドである name , attrs , text , and **kwargs をもちます。
以下は、 SoupStrainer オブジェクトの3通りの例です。:  from  bs4  import  SoupStrainer  only_a_tags  =  SoupStrainer ( "a" )  only_tags_with_id_link2  =  SoupStrainer ( id = "link2" )  def  is_short_string ( string ):  return  len ( string )  <  10  only_short_strings  =  SoupStrainer ( text = is_short_string )    ここで、”three sisters”ドキュメントをもう一回とりあげます。
ドキュメントを SoupStrainer オブジェクトで3通りにパースするので、どうなるかを見てみましょう。:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_a_tags ) .
prettify ())  # Elsie  # ,  # Lacie  # and  # Tillie  # ...
 #    SoupStrainer  パースツリーを検索 のメソッドに渡すことができます。
これは、とても便利です。少しだけ説明します。:  soup  =  BeautifulSoup ( html_doc )  soup .
find_all ( only_short_strings )  # [u'\n\n', u'\n\n', u'Elsie', u',\n', u'Lacie', u' and\n', u'Tillie',  #  u'\n\n', u'...', u'\n']       トラブルシューティング ¶   diagnose() ¶  もし、Beautiful Soupがドキュメントに何かをしてトラブルになっているときは、どのドキュメントを diagnose() 関数に渡してみてください。(これはBeautiful Soup 4.2.0の新機能です)
Beautiful Soupは、どのようにパーサーがそのドキュメントを扱ったかというレポートを出力し、BeautifulSoupが使っているパーサーが失っているかどうかを教えてくれます。:  from  bs4.diagnose  import  diagnose  data  =  open ( "bad.html" ) .
# Found lxml version 2.3.2.0  #  # Trying to parse your data with html.parser  # Here's what html.parser did with the document:  # ...
   diagnose()の出力をみてみると、どのように問題を解決すればよいかわかるでしょう。もし、わからなくても、助けをもとめるときに、 diagnose() の出力を貼り付けることができます。    パース時に出るエラー ¶  パースエラーには2種類あります。
1つは、クラッシュです。Beautifuls Soupにドキュメントを読み込ませたときに、例外が発生します。たいていそれは HTMLParser.HTMPParserError です。
もう1つは、想定外の動作です。Beautiful Soupのパースツリーが、元のドキュメントのパースツリーとかなり違うことがあります。  これらのエラーは、たいていBeautiful Soupが原因ではありません。そのように言えるのは、Beautiful Soupがよくできたソフトウェアだからではなく、Beautiful Soupがパース処理のコードを含んでいないためです。
代わりに、Beautiful Soupは外部のパーサーに頼っています。もしあるパーサーが正しいドキュメントをパースできないときは、他のパーサーを試してみるというのが一番良い対処です。 パーサーのインストール に、これについての詳細とパーサーの比較が載っています。  一番よくみるパースエラーは、 HTMLParser.HTMLParseError:  malformed  start  tag と HTMLParser.HTMLPraseError:  bad  end  tag でしょう。
これらはともに、Python組み込みのHTMLパーサーライブラリが返します。
この場合は、 lxml か html5lib をインストール するとよいです。  想定外の動作のエラーで最も多いのは、あると思っていたタグを見つけられないときです。
見たことあると思いますが、そのとき find_all() は [] を返し、 find() は None を返します。
これも、Python組み込みHTMLパーサーにとっては、よくある問題です。やはり、一番よい対処は、 lxml か html5lib をインストール することです。    バージョン違いの問題 ¶   SyntaxError:  Invalid  syntax (on the line ROOT_TAG_NAME  =  u'[document]' ): Python 2バージョンのBeautiful Soupを、変換しないでPython 3で実行したためです。    ImportError:  No  module  named  HTMLParser - Python 2バージョンのBeautiful Soupを、Python 3で実行したためです。    ImportError:  No  module  named  html.parser - Python 3バージョンのBeautiful Soupを、Python 2で実行したためです。    ImportError:  No  module  named  BeautifulSoup - Beautiful Soup 3のコードを、BS3がインストールされてない環境で実行したため、またはBeautiful Soup 4のコードをパッケージ名を bs4 に変えずに実行したためです。    ImportError:  No  module  named  bs4 - Beautiful Soup 4のコードを、BS4がインストールされてない環境で実行したためです。     XMLのパース ¶  デフォルトでは、Beautiful SoupはドキュメントをHTMLとしてパースします。XMLとしてパースするには、 BeautifulSoup コンストラクタの第二引数に、 “xml” を渡します。:  soup  =  BeautifulSoup ( markup ,  "xml" )    このためには、 lxml をインストール している必要があります。    その他のパーサーの問題 ¶   If your script works on one computer but not another, it’s probably
because the two computers have different parser libraries
available.
For example, you may have developed the script on a
computer that has lxml installed, and then tried to run it on a
computer that only has html5lib installed.
See パーサーの違い for why this matters, and fix the problem by mentioning a
specific parser library in the BeautifulSoup constructor.
その他 ¶   UnicodeEncodeError:  'charmap'  codec  can't  encode  character  u'\xfoo'  in  position  bar (or just about any other UnicodeEncodeError ) - This is not a problem with Beautiful Soup.
This problem shows up in two main situations.
First, when you try to
print a Unicode character that your console doesn’t know how to
display.
(See this page on the Python wiki for help.)
Second, when
you’re writing to a file and you pass in a Unicode character that’s
not supported by your default encoding.
In this case, the simplest
solution is to explicitly encode the Unicode string into UTF-8 with u.encode("utf8") .
But find_all() returns a _list_ of tags
and strings–a ResultSet object.
パフォーマンス改善 ¶  Beautiful Soup will never be as fast as the parsers it sits on top
of.
ドキュメントの一部をパース won’t save you much time parsing
the document, but it can save a lot of memory, and it’ll make searching the document much faster.
Beautiful Soup 3 ¶  Beautiful Soup 3は一つ前のリリースで、すでに開発は停止しています。
現在でも、全ての主なLinuxディストリビューションに含まれています。:  $  apt-get  install  python-beautifulsoup  Pypiでも BeautifulSoup として利用できます。  $  easy_install  BeautifulSoup  $  pip  install  BeautifulSoup  次のリンクからダウンロードできます。 tarball of Beautiful Soup 3.2.0 .
easy_install  beautifulsoup , easy_install  BeautifulSoup というコマンドでBeautiful Soupをインストールすると、あなたのコードは動きません。 easy_install  beautifulsoup4 と入力しましょう。  Beautiful Soup 3 のドキュメントはアーカイブされています。  日本語版は次のリンクから参照できます。 Beautiful Soup ドキュメント Beautiful Soup 4での変更点が理解するために、これらのドキュメントを読んでみてください。   BS4への移行 ¶  多くのBS3で書かれたコードは、一か所変更するだけでBS4で動きます。パッケージ名を BeautifulSoup から bs4 に変更するだけです。これを、、:  from  BeautifulSoup  import  BeautifulSoup    以下のようにします。:  from  bs4  import  BeautifulSoup     ImportError “No module named BeautifulSoup” が表示された場合、BS4しかインストールされていないのに、BS3のコードを実行しようとしたのが問題です。    ImportError “No module named bs4” が表示された場合、BS3しかインストールされていないのに、BS4のコードを実行しようとしたのが問題です。   BS4はBS3の大部分について後方互換性がありますが、それらのメソッドのほとんどは変更され`PEP 8 規約 < http://www.python.org/dev/peps/pep-0008/ >`_ に沿った新しい名前になっています。多くの名前等の変更により、後方互換性の一部が損なわれています。  以下は、BS3のコードをBS4に変換するのに知っておくべき事項です。:   パーサー ¶  Beautiful Soup 3 used Python’s SGMLParser , a module that was
deprecated and removed in Python 3.0.
See パーサーのインストール for a comparison.
Since html.parser is not the same parser as SGMLParser , it
will treat invalid markup differently.
Usually the “difference” is
that html.parser crashes.
In that case, you’ll need to install
another parser.
But sometimes html.parser just creates a different
parse tree than SGMLParser would.
If this happens, you may need to
update your BS3 scraping code to deal with the new tree.
メソッド名 ¶   renderContents -> encode_contents  replaceWith -> replace_with  replaceWithChildren -> unwrap  findAll -> find_all  findAllNext -> find_all_next  findAllPrevious -> find_all_previous  findNext -> find_next  findNextSibling -> find_next_sibling  findNextSiblings -> find_next_siblings  findParent -> find_parent  findParents -> find_parents  findPrevious -> find_previous  findPreviousSibling -> find_previous_sibling  findPreviousSiblings -> find_previous_siblings  nextSibling -> next_sibling  previousSibling -> previous_sibling   Some arguments to the Beautiful Soup constructor were renamed for the
same reasons:   BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...)  BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...)   I renamed one method for compatibility with Python 3:   Tag.has_key() -> Tag.has_attr()   I renamed one attribute to use more accurate terminology:   Tag.isSelfClosing -> Tag.is_empty_element   I renamed three attributes to avoid using words that have special
meaning to Python.
If you used these attributes in BS3, your code will break
on BS4 until you change them.
UnicodeDammit.unicode -> UnicodeDammit.unicode_markup  Tag.next -> Tag.next_element  Tag.previous -> Tag.previous_element     ジェネレーター ¶  I gave the generators PEP 8-compliant names, and transformed them into
properties:   childGenerator() -> children  nextGenerator() -> next_elements  nextSiblingGenerator() -> next_siblings  previousGenerator() -> previous_elements  previousSiblingGenerator() -> previous_siblings  recursiveChildGenerator() -> descendants  parentGenerator() -> parents   So instead of this:  for  parent  in  tag .
エンティティ ¶  An incoming HTML or XML entity is always converted into the
corresponding Unicode character.
その他 ¶  Tag.string now operates recursively.
値が複数のとき like class have lists of strings as
their values, not strings.
If you pass one of the find* methods both text  and a tag-specific argument like name , Beautiful Soup will
search for tags that match your tag-specific criteria and whose Tag.string matches your value for text .
Table Of Contents   Beautiful Soup  (訳注)石鹸は食べられない  この文書について  助けてほしいときは    クイックスタート  インストール  インストール後の問題  パーサーのインストール    スープの作成  4種類のオブジェクト  Tag obj.
名前  属性  値が複数のとき      NavigableString obj.
BeautifulSoup obj.
Comments obj.
他    パースツリーを探索  子要素へ下移動  タグ名で探索  .contents / .children  .descendants  .string  .strings / .stripped_strings    親要素へ上移動  .parent  .parents    兄弟要素へ横移動  .next_sibling / .previous_sibling  .next_siblings / .previous_siblings    前後の要素へ移動  .next_element / .previous_element  .next_elements / .previous_elements      パースツリーを検索  フィルターの種類  文字列  正規表現  リスト  True値  関数    find_all()  name引数  キーワード引数  CSSのクラスで検索  text引数  limit引数  recursive引数  ショートカット    find()  find_parents() / find_parent()  find_next_siblings() / find_next_sibling()  find_previous_siblings() / find_previous_sibling()  find_all_next() / find_next()  find_all_previous() / find_previous()  CSSセレクタ    パースツリーを修正  名前や属性の変更  .string の修正  append()  BeautifulSoup.new_string() / .new_tag()  insert()  insert_before() / insert_after()  clear()  extract()  decompose()  replace_with()  wrap()  unwrap()    出力  きれいに出力  一行に出力  フォーマットを指定  get_text()    パーサーの指定  パーサーの違い    エンコード  出力のエンコード  Unicode, Dammit  スマート引用符  複数の文字コード      ドキュメントの一部をパース  SoupStrainer    トラブルシューティング  diagnose()  パース時に出るエラー  バージョン違いの問題  XMLのパース  その他のパーサーの問題  その他  パフォーマンス改善    Beautiful Soup 3  BS4への移行  パーサー  メソッド名  ジェネレーター  XML  エンティティ  その他       This Page   Show Source    Quick search        Enter search terms or a module, class or function name.
日本語訳 (2013-11-19最終更新) »    © Copyright 2013, Leonard Richardson.
Created using Sphinx 1.2.
목차    인덱스  뷰티플수프 4.0.0 문서 »         뷰티플수프 문서 ¶ 한글판 johnsonj 2012.11.08 원문 위치   뷰티플수프 는 HTML과 XML 파일로부터 데이터를 뽑아내기 위한 파이썬 라이브러리이다.
여러분이 선호하는 해석기와 함께 사용하여 일반적인 방식으로 해석 트리를 항해, 검색, 변경할 수 있다.
주로 프로그래머의 수고를 덜어준다.
이 지도서에서는 뷰티플수프 4의 중요한 특징들을 예제와 함께 모두 보여준다.
이 라이브러리가 어느 곳에 유용한지, 어떻게 작동하는지, 또 어떻게 사용하는지, 어떻게 원하는대로 바꿀 수 있는지, 예상을 빗나갔을 때 어떻게 해야 하는지를 보여준다.
이 문서의 예제들은 파이썬 2.7과 Python 3.2에서 똑 같이 작동한다.
혹시 뷰티플수프 3 에 관한 문서를 찾고 계신다면 뷰티플수프 3는 더 이상 개발되지 않는다는 사실을 꼭 아셔야겠다.
새로 프로젝트를 시작한다면 뷰티플수프 4를 적극 추천한다.
뷰티플수프 3와 뷰티플수프 4의 차이점은 BS4 코드 이식하기 를 참조하자.
도움 얻기 ¶  뷰피플수프에 의문이 있거나, 문제에 봉착하면 토론 그룹에 메일을 보내자 .
바로 시작 ¶  다음은 이 문서에서 예제로 사용할 HTML 문서이다.
이상한 나라의 앨리스 이야기의 일부이다:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """    “three sisters” 문서를 뷰피플수프에 넣으면 BeautifulSoup 객체가 나오는데, 이 객체는 문서를 내포된 데이터 구조로 나타낸다:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc )  print ( soup .
#   </p>  #   <p class="story">  #    ...
 #   </p>  #  </body>  # </html>    다음은 간단하게 데이터 구조를 항해하는 몇 가지 방법이다:  soup .
find ( id = "link3" )  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>    일반적인 과업으로 한 페이지에서 <a> 태그에 존재하는 모든 URL을 뽑아 낼 일이 많다:  for  link  in  soup .
get ( 'href' ))  # http://example.com/elsie  # http://example.com/lacie  # http://example.com/tillie    또 다른 과업으로 페이지에서 텍스트를 모두 뽑아낼 일이 많다:  print ( soup .
#  # ...
   이것이 여러분이 필요한 것인가?
그렇다면, 계속 읽어 보자.
뷰티플 수프 설치하기 ¶  데비안이나 우분투 리눅스 최신 버전을 사용중이라면, 시스템 꾸러미 관리자로 뷰티플수프를 설치하자:  $  apt-get  install  python-bs4  뷰티블수프 4는 PyPi를 통하여도 출간되어 있으므로, 시스템 꾸러미 관리자로 설치할 수 없을 경우, easy_install 로 설치하거나 pip 로 설치할 수 있다.
꾸러미 이름은 beautifulsoup4 이며, 같은 꾸러미로 파이썬 2 그리고 파이썬 3에 작동한다.
$  easy_install  beautifulsoup4  $  pip  install  beautifulsoup4  (이 BeautifulSoup 꾸러미가 혹시 원하는 것이 아니라면 .
이전 버전으로 뷰티플수프 3 가 있다.
많은 소프트웨에서 BS3를 사용하고 있으므로, 여전히 사용할 수 있다.
그러나 새로 코드를 작성할 생각이라면 beautifulsoup4 를 설치하시기 바란다.)
easy_install 도 pip 도 설치되어 있지 않다면, 뷰티플수프 4 소스 를 내려 받아 setup.py 로 설치하실 수 있다.
$  python  setup.py  install  다른 모든 것이 실패하더라도, 뷰티플수프 라이센스는 여러분의 어플리케이션에 통채로 꾸려 넣는 것을 허용하므로 전혀 설치할 필요없이 소스를 내려받아 bs4 디렉토리를 통채로 코드베이스에 복사해서 사용하셔도 된다.
본인은 파이썬 2.7과 파이썬 3.2에서 뷰티플수프를 개발하였지만, 다른 최신 버전에도 작동하리라 믿는 바이다.
설치 이후의 문제 ¶  뷰티플 수프는 파이썬 2 코드로 꾸려 넣어져 있다.
파이썬 3에 사용하기 위해 설치하면, 파이썬 3 코드로 자동으로 변환된다.
꾸러미가 설치되어 있지 않다면, 당연히 변환되지 않는다.
또한 윈도우즈 머신이라면 잘못된 버전이 설치되어 있다고 보고된다.
“No module named HTMLParser”와 같은 ImportError 에러가 일어나면, 파이썬 3 아래에서 파이썬 2 버전의 코드를 실행하고 있기 때문이다.
“No module named html.parser”와 같은 ImportError 에러라면, 파이썬 3 버전의 코드를 파이썬 2 아래에서 실행하고 있기 때문이다.
두 경우 모두 최선의 선택은 시스템에서 (압축파일을 풀 때 만들어진 디렉토리를 모두 포함하여) 뷰티플수프를 제거하고 다시 설치하는 것이다.
다음 ROOT_TAG_NAME  =  u'[document]' 줄에서 SyntaxError “Invalid syntax”를 맞이한다면, 파이썬 2 코드를 파이썬 3 코드로 변환할 필요가 있다.
이렇게 하려면 다음과 같이 패키지를 설치하거나:  $  python3  setup.py  install  아니면 직접 파이썬의 2to3 변환 스크립트를 bs4 디렉토리에 실행하면 된다:  $  2to3-3.2  -w  bs4    해석기 설치하기 ¶  뷰티플수프는 파이썬 표준 라이브러리에 포함된 HTML 해석기를 지원하지만, 또 수 많은 제-삼자 파이썬 해석기도 지원한다.
그 중 하나는 lxml 해석기 이다.
설정에 따라, 다음 명령어들 중 하나로 lxml을 설치하는 편이 좋을 경우가 있다:  $  apt-get  install  python-lxml  $  easy_install  lxml  $  pip  install  lxml  파이썬 2를 사용중이라면, 또다른 대안은 순수-파이썬 html5lib 해석기 를 사용하는 것인데, 이 해석기는 HTML을 웹 브라우저가 해석하는 방식으로 해석한다.
설정에 따라 다음 명령어중 하나로 html5lib를 설치하는 것이 좋을 때가 있다:  $  apt-get  install  python-html5lib  $  easy_install  html5lib  $  pip  install  html5lib  다음 표에 각 해석 라이브러리의 장점과 단점을 요약해 놓았다:        해석기  전형적 사용방법  장점  단점   파이썬의 html.parser  BeautifulSoup(markup,  "html.parser")   각종 기능 완비  적절한 속도  관대함 (파이썬 2.7.3과 3.2에서.)
별로 관대하지 않음
(파이썬 2.7.3이나 3.2.2 이전 버전에서)     lxml의 HTML 해석기  BeautifulSoup(markup,  "lxml")   아주 빠름  관대함     외부 C 라이브러리 의존     lxml의 XML 해석기  BeautifulSoup(markup,  ["lxml",  "xml"])  BeautifulSoup(markup,  "xml")   아주 빠름  유일하게 XML 해석기 지원     외부 C 라이브러리 의존     html5lib  BeautifulSoup(markup,  html5lib)   아주 관대함  웹 브라우저의 방식으로 페이지를 해석함  유효한 HTML5를 생성함     아주 느림  외부 파이썬 라이브러리 의존  파이썬 2 전용      가능하다면, 속도를 위해 lxml을 설치해 사용하시기를 권장한다.
2.7.3 이전의 파이썬2, 또는3.2.2 이전의 파이썬 3 버전을 사용한다면, lxml을 사용하는 것이 필수이다 .
그렇지 않고 구형 버전의 파이썬 내장 HTML 해석기 html5lib는 별로 좋지 않다.
문서가 유효하지 않을 경우 해석기마다 다른 뷰티플수프 트리를 생산한다는 사실을 주목하자.
자세한 것은 해석기들 사이의 차이점들 을 살펴보자.
수프 만들기 ¶  문서를 해석하려면, 문서를 BeautifulSoup 구성자에 건네주자.
문자열 혹은 열린 파일 핸들을 건네면 된다:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( open ( "index.html" ))  soup  =  BeautifulSoup ( "<html>data</html>" )    먼저, 문서는 유니코드로 변환되고 HTML 개체는 유니코드 문자로 변환된다:  BeautifulSoup("Sacr&eacute; bleu!")
<html><head></head><body>Sacré bleu!</body></html>   다음 뷰티플수프는 문서를 가장 적당한 해석기를 사용하여 해석한다.
특별히 XML 해석기를 사용하라고 지정해 주지 않으면 HTML 해석기를 사용한다.
( XML 해석하기 참조.)
객체의 종류 ¶  뷰티플수프는 복합적인 HTML 문서를 파이썬 객체로 구성된 복합적인 문서로 변환한다.
그러나 객체의 종류 를 다루는 법만 알면 된다.
태그 ¶   Tag 객체는 원래 문서의 XML 태그 또는 HTML 태그에 상응한다:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' )  tag  =  soup .
b  type ( tag )  # <class 'bs4.element.Tag'>    태그는 많은 속성과 메쏘드가 있지만, 그 대부분을 나중에 트리 항해하기 그리고 트리 검색하기 에서 다룰 생각이다.
지금은 태그의 가장 중요한 특징인 이름과 속성을 설명한다.
이름 ¶  태그마다 이름이 있고, 다음 .name 과 같이 접근할 수 있다:  tag .
name  # u'b'    태그의 이름을 바꾸면, 그 변화는 뷰티블수프가 생산한 HTML 조판에 반영된다:  tag .
name  =  "blockquote"  tag  # <blockquote class="boldest">Extremely bold</blockquote>      속성 ¶  태그는 속성을 여러개 가질 수 있다.
<b  class="boldest"> 태그는 속성으로 “class”가 있는데 그 값은
“boldest”이다.
태그의 속성에는 사전처럼 태그를 반복해 접근하면 된다:  tag [ 'class' ]  # u'boldest'    사전에 .attrs 와 같이 바로 접근할 수 있다:  tag .
attrs  # {u'class': u'boldest'}    태그의 속성을 추가, 제거, 변경할 수 있다.
역시 태그를 사전처럼 취급해서 처리한다:  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>  tag [ 'class' ]  # KeyError: 'class'  print ( tag .
get ( 'class' ))  # None     값이-여럿인 속성 ¶  HTML 4에서 몇몇 속성은 값을 여러 개 가질 수 있도록 정의된다.
HTML 5에서 그 중 2개는 제거되었지만, 몇 가지가 더 정의되었다.
가장 흔한 다중값 속성은 class 이다 (다시 말해, 태그가 하나 이상의 CSS 클래스를 가질 수 있다).
다른 것으로는 rel , rev , accept-charset , headers , 그리고 accesskey 가 포함된다.
뷰티플수프는 다중-값 속성의 값들을 리스트로 나타낸다:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' )  css_soup .
p [ 'class' ]  # ["body"]    속성에 하나 이상의 값이 있는 것처럼 보이지만 , HTML 표준에 정의된 다중-값 속성이 아니라면, 뷰티플수프는 그 속성을 그대로 둔다:  id_soup  =  BeautifulSoup ( '<p id="my id"></p>' )  id_soup .
p [ 'class' ]  # u'body strikeout'        NavigableString ¶  문자열은 태그 안에 있는 일군의 텍스트에 상응한다.
뷰티플수프는 NavigableString 클래스 안에다 이런 텍스트를 보관한다:  tag .
string )  # <class 'bs4.element.NavigableString'>     NavigableString 은 파이썬의 유니코드 문자열과 똑 같은데, 단 트리 항해하기 와 트리 탐색하기 에 기술된 특징들도 지원한다는 점이 다르다.
NavigableString 을 유니코드 문자열로 변환하려면 unicode() 를 사용한다:  unicode_string  =  unicode ( tag .
string )  unicode_string  # u'Extremely bold'  type ( unicode_string )  # <type 'unicode'>    문자열을 바로바로 편집할 수는 없지만, replace_with() 을 사용하면 한 문자열을 또다른 문자열로 바꿀 수 있다:  tag .
replace_with ( "No longer bold" )  tag  # <blockquote>No longer bold</blockquote>    NavigableString 은 트리 항해하기 와 트리 탐색하기 에 기술된 특징들을 모두는 아니지만, 대부분 지원한다.
특히, (태그에는 다른 문자열이나 또다른 태그가 담길 수 있지만) 문자열에는 다른 어떤 것도 담길 수 없기 때문에, 문자열은 .contents 나 .string 속성, 또는 find() 메쏘드를 지원하지 않는다.
BeautifulSoup ¶   BeautifulSoup 객체 자신은 문서 전체를 대표한다.
대부분의 목적에, 그것을 Tag 객체로 취급해도 좋다.
이것은 곧 트리 항해하기 와 트리 검색하기 에 기술된 메쏘드들을 지원한다는 뜻이다.
BeautifulSoup 객체는 실제 HTML 태그나 XML 태그에 상응하지 않기 때문에, 이름도 속성도 없다.
그러나 가끔 그의 이름 .name 을 살펴보는 것이 유용할 경우가 있다.
그래서 특별히 .name 에 “[document]”라는 이름이 주어졌다:  soup .
name  # u'[document]'      주석과 기타 특수 문자열들 ¶  Tag , NavigableString , 그리고 BeautifulSoup 정도면 HTML이나 XML 파일에서 보게될 거의 모든 것들을 망라한다.
그러나 몇 가지 남은 것들이 있다.
아마도 신경쓸 필요가 있는 것이 유일하게 있다면 바로 주석이다:  markup  =  "<b><!--Hey, buddy.
string  type ( comment )  # <class 'bs4.element.Comment'>     Comment 객체는 그냥 특별한 유형의 NavigableString 이다:  comment  # u'Hey, buddy.
Want to buy a used parser'    그러나 HTML 문서의 일부에 나타나면, Comment 는 특별한 형태로 화면에 표시된다:  print ( soup .
Want to buy a used parser?-->  # </b>    뷰티플수프는 XML 문서에 나올만한 것들을 모두 클래스에다 정의한다: CData , ProcessingInstruction , Declaration , 그리고 Doctype 이 그것이다.
Comment 와 똑같이, 이런 클래스들은 NavigableString 의 하위클래스로서 자신의 문자열에 다른 어떤것들을 추가한다.
다음은 주석을 CDATA 블록으로 교체하는 예이다:  from  bs4  import  CData  cdata  =  CData ( "A CDATA block" )  comment .
[CDATA[A CDATA block]]>  # </b>       트리 항해하기 ¶  다시 또 “Three sisters” HTML 문서를 보자:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc )    이 예제로 한 문서에서 일부를 다른 곳으로 이동하는 법을 보여주겠다.
내려가기 ¶  태그에는 또다른 태그가 담길 수 있다.
이런 요소들은 그 태그의 자손( children )이라고 부른다.
뷰티플수프는 한 태그의 자손을 항해하고 반복하기 위한 속성을 다양하게 제공한다.
뷰티플수프의 문자열은 이런 속성들을 제공하지 않음에 유의하자.
왜냐하면 문자열은 자손을 가질 수 없기 때문이다.
태그 이름을 사용하여 항해하기 ¶  가장 단순하게 해석 트리를 항해하는 방법은 원하는 태그의 이름을 지정해 주는 것이다.
<head> 태그를 원한다면, 그냥 soup.head 라고 지정하면 된다:  soup .
title  # <title>The Dormouse's story</title>    이 트릭을 반복해 사용하면 해석 트리의 특정 부분을 확대해 볼 수 있다.
다음 코드는 <body> 태그 아래에서 첫 번째 <b> 태그를 얻는다:  soup .
b  # <b>The Dormouse's story</b>    태그 이름을 속성으로 사용하면 오직 그 이름으로 첫 번째 태그만 얻는다:  soup .
a  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    <a> 태그를 모두 얻거나, 특정이름으로 첫 번째 태그 말고 좀 더 복잡한 어떤 것을 얻고 싶다면, 트리 탐색하기 에 기술된 메쏘드들을 사용해야 한다.
예를 들어, find_all() 과 같은 메쏘드를 사용하면 된다:  soup .
find_all ( 'a' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      .contents 그리고 .children ¶  태그의 자손은 .contents 라고 부르는 리스트로 얻을 수 있다:  head_tag = soup.head
head_tag
# <head><title>The Dormouse's story</title></head>

head_tag.contents
[<title>The Dormouse's story</title>]

title_tag = head_tag.contents[0]
title_tag
# <title>The Dormouse's story</title>
title_tag.contents
# [u'The Dormouse's story']   BeautifulSoup 객체 자체에 자손이 있다.
이 경우, <html> 태그가 바로 BeautifulSoup 객체의 자손이다.
name  # u'html'    문자열은 .contents 를 가질 수 없는데, 왜냐하면 문자열 안에는 아무것도 담을 수 없기 때문이다:  text  =  title_tag .
contents  # AttributeError: 'NavigableString' object has no attribute 'contents'    자손을 리스트로 얻는 대신에, .children 발생자를 사용하면 태그의 자손을 반복할 수 있다:  for  child  in  title_tag .
children :  print ( child )  # The Dormouse's story      .descendants ¶  내용물( .contents )과 자손( .children ) 속성은 오직 한 태그의 직계( direct ) 자손만 고려한다.
예를 들면, <head> 태그는 오직 한 개의 직계 자손으로 <title> 태그가 있다:  head_tag .
contents  # [<title>The Dormouse's story</title>]    그러나 <title> 태그 자체에 자손이 하나 있다: 문자열 “The Dormouse’s
story”가 그것이다.
그 문자열도 역시 <head> 태그의 자손이다.
.descendants 속성은 한 태그의 자손들을 모두 재귀적으로, 반복할 수 있도록 해준다: 그의 직계 자손, 그 직계 자손의 자손, 등등:  for  child  in  head_tag .
descendants :  print ( child )  # <title>The Dormouse's story</title>  # The Dormouse's story    <head> 태그는 오직 자손이 하나이지만, 후손은 둘이다:
<title> 태그와 <title> 태그의 자손이 그것이다.
BeautifulSoup 객체는 오직 하나의 직계 자손(<html> 태그)만 있지만, 수 많은 후손을 가진다:  len ( list ( soup .
descendants ))  # 25      .string ¶  태그에 오직 자손이 하나라면, 그리고 그 자손이 NavigableString 이라면, 그 자손은 .string 으로 얻을 수 있다:  title_tag .
string  # u'The Dormouse's story'    태그의 유일한 자손이 또다른 태그라면, 그리고 그 태그가 .string 을 가진다면, 그 부모 태그는 같은 .string 을 그의 자손으로 가진다고 간주된다:  head_tag .
string  # u'The Dormouse's story'    태그에 하나 이상의 태그가 있다면, .string 이 무엇을 가리킬지 확실하지 않다.
그래서 그럴 경우 .string 은 None 으로 정의된다:  print ( soup .
string )  # None      .strings 그리고 stripped_strings ¶  한 태그 안에 여러개의 태그가 있더라도 여전히 문자열을 볼 수 있다.
.strings 발생자를 사용하자:  for  string  in  soup .
# u'\n\n'  # u'...'  # u'\n'    이런 문자열들은 공백이 쓸데 없이 많은 경향이 있으므로, 대신에 .stripped_strings 발생자를 사용해 제거해 버릴 수 있다:  for  string  in  soup .
# u'...'    여기에서, 전적으로 공백만으로 구성된 문자열은 무시되고 문자열 앞과 뒤의 공백은 제거된다.
올라가기 ¶  “가족 트리” 비유를 계속 사용해 보자.
태그마다 그리고 문자열마다 부모( parent )가 있다: 즉 자신을 담고 있는 태그가 있다.
.parent ¶  한 요소의 부모는 .parent 속성으로 접근한다.
예제 “three sisters”문서에서,  <head> 태그는  <title> 태그의 부모이다:  title_tag  =  soup .
parent  # <head><title>The Dormouse's story</title></head>    title 문자열 자체로 부모가 있다: 그 문자열을 담고 있는 <title> 태그가 그것이다:  title_tag .
parent  # <title>The Dormouse's story</title>    <html> 태그와 같은 최상위 태그의 부모는 BeautifulSoup 객체 자신이다:  html_tag  =  soup .
parent )  # <class 'bs4.BeautifulSoup'>    BeautifulSoup 객체의 .parent 는 None으로 정의된다:  print ( soup .
parent )  # None      .parents ¶   .parents 로 한 요소의 부모들을 모두 다 반복할 수 있다.
다음 예제는 .parents 를 사용하여 문서 깊숙히 묻힌 <a> 태그로부터 시작하여, 문서의 최상단까지 순회한다:  link  =  soup .
name )  # p  # body  # html  # [document]  # None       옆으로 가기 ¶  다음과 같은 간단한 문서를 생각해 보자:  sibling_soup  =  BeautifulSoup ( "<a><b>text1</b><c>text2</c></b></a>" )  print ( sibling_soup .
prettify ())  # <html>  #  <body>  #   <a>  #    <b>  #     text1  #    </b>  #    <c>  #     text2  #    </c>  #   </a>  #  </body>  # </html>    <b> 태그와 <c> 태그는 같은 수준에 있다: 둘 다 같은 태그의 직계 자손이다.
이를 형제들( siblings )이라고 부른다.
문서가 pretty-printed로 출력되면, 형제들은 같은 들여쓰기 수준에서 나타난다.
이런 관계를 코드 작성에도 이용할 수 있다.
.next_sibling 그리고 .previous_sibling ¶   .next_sibling 과 .previous_sibling 를 사용하면 해석 트리에서 같은 수준에 있는 페이지 요소들 사이를 항해할 수 있다:  sibling_soup .
previous_sibling  # <b>text1</b>    <b> 태그는 .next_sibling 이 있지만, .previous_sibling 은 없는데,
그 이유는  <b> 태그 앞에 트리에서 같은 수준에 아무것도 없기 때문이다.
같은 이유로, <c> 태그는 .previous_sibling 은 있지만 .next_sibling 은 없다:  print ( sibling_soup .
next_sibling )  # None    문자열“text1”과 “text2”는 형제 사이가 아니다 .
왜냐하면 부모가 같지 않기 때문이다:  sibling_soup .
next_sibling )  # None    실제 문서에서, 한 태그의 .next_sibling 이나 .previous_sibling 은 보통 공백이 포함된 문자열이다.
“three sisters” 문서로 되돌아 가보자:  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a>
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>   첫번째 <a> 태그의 .next_sibling 이 두 번째 <a> 태그가 될 것이라고 생각하실지 모르겠다.
그러나 실제로는 문자열이 다음 형제이다: 즉, 첫 번째 <a> 태그와 두 번째 태그를 가르는 쉼표와 새줄 문자가 그것이다:  link  =  soup .
next_sibling  # u',\n'    두 번째 <a> 태그는 실제로는 그 쉼표의 .next_sibling 이다:  link .
next_sibling  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>      .next_siblings 그리고 .previous_siblings ¶  태그의 형제들은 .next_siblings 이나 .previous_siblings 로 반복할 수 있다:  for  sibling  in  soup .
previous_siblings :  print ( repr ( sibling ))  # ' and\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # u',\n'  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  # u'Once upon a time there were three little sisters; and their names were\n'  # None       앞뒤로 가기 ¶  “three sisters” 문서의 앞부분을 살펴보자:  <html><head><title>The Dormouse's story</title></head>
<p class="title"><b>The Dormouse's story</b></p>   HTML 해석기는 이 문자열들을 취해서 일련의 이벤트로 변환한다: “<html> 태그 열기”, “<head> 태그 열기”, “
<title> 태그 열기”, “문자열 추가”, “<title> 태그 닫기”, “<p> 태그 열기”, 등등.
뷰티플수프는 문서의 최초 해석 상태를 재구성하는 도구들을 제공한다.
.next_element 그리고 .previous_element ¶  문자열이나 태그의 .next_element 속성은 바로 다음에 해석된 것을 가리킨다.
.next_sibling 과 같을 것 같지만, 보통 완전히 다르다.
다음은 “three sisters”문서에서 마지막 <a> 태그이다.
그의 .next_sibling 은 문자열이다: <a> 태그가 시작되어 중단되었던 문장의 끝부분이다:  last_a_tag  =  soup .
그러나 <a> 태그의 .next_element 는, 다시 말해  <a> 태그 바로 다음에 해석된 것은, 나머지 문장이 아니다 : 그것은 단어 “Tillie”이다:  last_a_tag .
next_element  # u'Tillie'    그 이유는 원래의 조판에서 단어“Tillie”가 쌍반점보다 먼저 나타나기 때문이다.
해석기는 <a> 태그를 맞이하고, 다음으로 단어 “Tillie”, 그 다음 닫는 </a> 태그, 그 다음에 쌍반점과 나머지 문장을 맞이한다.
쌍반점은  <a> 태그와 같은 수준에 있지만, 단어 “Tillie”를 먼저 만난다.
.previous_element 속성은 .next_element 와 정반대이다.
바로 앞에 해석된 요소를 가리킨다:  last_a_tag .
next_element  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>      .next_elements 그리고 .previous_elements ¶  이제 이해가 가셨으리라 믿는다.
이런 반복자들을 사용하면 문서에서 해석하는 동안 앞 뒤로 이동할 수 있다:  for  element  in  last_a_tag .
# u'\n\n'  # <p class="story">...</p>  # u'...'  # u'\n'  # None        트리 탐색하기 ¶  뷰티플수프에는 해석 트리를 탐색하기 위한 메쏘드들이 많이 정의되어 있지만, 모두 다 거의 비슷하다.
가장 많이 사용되는 두 가지 메쏘드를 설명하는데 시간을 많이 할애할 생각이다: find() 와 find_all() 이 그것이다.
다른 메쏘드는 거의 똑 같은 인자를 취한다.
그래서 그것들은 그냥 간략하게 다루겠다.
다시 또, “three sisters” 문서를 예제로 사용하자:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc )    find_all() 과 같이 인자에 여과기를 건네면, 얼마든지 문서에서 관심있는 부분을 뜯어낼 수 있다.
여과기의 종류 ¶  find_all() 과 유사 메쏘드들에 관하여 자세히 설명하기 전에 먼저, 이런 메쏘드들에 건넬 수 있는 다양한 여과기의 예제들을 보여주고 싶다.
이런 여과기들은
탐색 API 전체에 걸쳐서 나타나고 또 나타난다.
태그의 이름, 그의 속성, 문자열 텍스트, 또는 이런 것들을 조합하여 여과할 수 있다.
문자열 ¶  가장 단순한 여과기는 문자열이다.
문자열을 탐색 메쏘드에 건네면 뷰티플수프는 그 정확한 문자열에 맞게 부합을 수행한다.
다음 코드는 문서에서 <b> 태그를 모두 찾는다:  soup .
find_all ( 'b' )  # [<b>The Dormouse's story</b>]    바이트 문자열을 건네면, 뷰티플수프는 그 문자열이 UTF-8로 인코드되어 있다고 간주한다.
이를 피하려면 대신에 유니코드 문자열을 건네면 된다.
정규 표현식 ¶  정규 표현식 객체를 건네면, 뷰티플수프는 match() 메쏘드를 사용하여 그 정규 표현식에 맞게 여과한다.
다음 코드는 이름이 “b”로 시작하는 태그를 모두 찾는다; 이 경우, <body> 태그와 <b> 태그를 찾을 것이다:  import  re  for  tag  in  soup .
name )  # body  # b    다음 코드는 이름에 ‘t’가 포함된 태그를 모두 찾는다:  for  tag  in  soup .
name )  # html  # title      리스트 ¶  리스트를 건네면, 뷰티플수프는 그 리스트에 담긴 항목마다 문자열 부합을 수행한다.
다음 코드는 모든 <a> 태그 그리고 모든 <b> 태그를 찾는다:  soup .
find_all ([ "a" ,  "b" ])  # [<b>The Dormouse's story</b>,  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      True ¶   True 값은 참이면 모두 부합시킨다.
다음 코드는 문서에서 태그를 모두 찾지만, 텍스트 문자열은 전혀 찾지 않는다:  for  tag  in  soup .
name )  # html  # head  # title  # body  # p  # b  # p  # a  # a  # a  # p      함수 ¶  다른 어떤 부합 기준도 마음에 안든다면, 요소를 그의 유일한 인자로 취하는 함수를 정의하면 된다.
함수는 인자가 부합하면 True 를 돌려주고, 그렇지 않으면 False 를 돌려주어야 한다.
다음은 태그에 “class”속성이 정의되어 있지만 “id” 속성은 없으면 True 를 돌려주는 함수이다:  def  has_class_but_no_id ( tag ):  return  tag .
has_key ( 'class' )  and  not  tag .
has_key ( 'id' )    이 함수를 find_all() 에 건네면 <p> 태그를 모두 얻게 된다:  soup .
find_all ( has_class_but_no_id )  # [<p class="title"><b>The Dormouse's story</b></p>,  #  <p class="story">Once upon a time there were...</p>,  #  <p class="story">...</p>]    이 함수는 <p> 태그만 얻는다.
<a> 태그는 획득하지 않는데, 왜냐하면 “class”와 “id”가 모두 정의되어 있기 때문이다.
<html>과 <title>도 얻지 않는데, 왜냐하면 “class”가 정의되어 있지 않기 때문이다.
다음은 태그가 문자열 객체로 둘러 싸여 있으면 True 를 돌려주는 함수이다:  from  bs4  import  NavigableString  def  surrounded_by_strings ( tag ):  return  ( isinstance ( tag .
name  # p  # a  # a  # a  # p    이제 탐색 메쏘드들을 자세하게 살펴볼 준비가 되었다.
find_all() ¶  서명: find_all( name , attrs , recursive , text , limit , **kwargs )   find_all() 메쏘드는 태그의 후손들을 찾아서 지정한 여과기에 부합하면 모두 추출한다.
몇 가지 여과기 에서 예제들을 제시했지만, 여기에 몇 가지 더 보여주겠다:  soup .
compile ( "sisters" ))  # u'Once upon a time there were three little sisters; and their names were\n'    어떤 것은 익숙하지만, 다른 것들은 새로울 것이다.
text 혹은 id 에 값을 건넨다는 것이 무슨 뜻인가?
왜 다음 find_all("p",  "title") 은 CSS 클래스가 “title”인 <p> 태그를 찾는가?
find_all() 에 건넨 인자들을 살펴보자.
name 인자 ¶  인자를 name 에 건네면 뷰티플수프는 특정 이름을 가진 태그에만 관심을 가진다.
이름이 부합되지 않는 태그와 마찬가지로, 텍스트 문자열은 무시된다.
다음은 가장 단순한 사용법이다:  soup .
find_all ( "title" )  # [<title>The Dormouse's story</title>]    여과기의 종류 에서 보았듯이 name 에 건넨 값이 문자열 , 정규 표현식 , 리스트 , 함수 , 또는 True 값일 수 있다는 사실을 기억하자.
키워드 인자 ¶  인지되지 않는 인자는 한 태그의 속성중 하나에 대한 여과기로 변환된다.
id 라는 인자에 대하여 값을 하나 건네면, 뷰티플수프는 각 태그의 ‘id’속성에 대하여 걸러낸다:  soup .
find_all ( id = 'link2' )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    href 에 대하여 값을 건네면, 뷰티플수프는 각 태그의 ‘href’속성에 대하여 걸러낸다:  soup .
compile ( "elsie" ))  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    문자열 , 정규 표현식 , 리스트 , 함수 , 또는 True 값 에 기반하여 속성을 걸러낼 수 있다.
다음 코드는 그 값이 무엇이든 상관없이, id 속성을 가진 태그를 모두 찾는다:  soup .
find_all ( id = True )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    하나 이상의 키워드 인자를 건네면 한 번에 여러 값들을 걸러낼 수 있다:  soup .
compile ( "elsie" ),  id = 'link1' )  # [<a class="sister" href="http://example.com/elsie" id="link1">three</a>]      CSS 클래스로 탐색하기 ¶  특정 CSS 클래스를 가진 태그를 탐색하면 아주 유용하지만, CSS 속성의 이름인 “class”는 파이썬에서 예약어이다.
키워드 인자로 class 를 사용하면 구문 에러를 만나게 된다.
뷰티플 4.1.2 부터, CSS 클래스로 검색할 수 있는데 class_ 키워드 인자를 사용하면 된다:  soup .
find_all ( "a" ,  class_ = "sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    다른 키워드 인자와 마찬가지로, class_ 에 문자열, 정규 표현식, 함수, 또는 True 를 건넬 수 있다:  soup .
find_all ( class_ = has_six_characters )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    기억하자 .
하나의 태그에 그의 “class” 속성에 대하여 값이 여러개 있을 수 있다.
특정 CSS 클래스에 부합하는 태그를 탐색할 때, 그의 CSS  클래스들 모두 에 대하여 부합을 수행하는 것이다:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' )  css_soup .
find_all ( "p" ,  class_ = "body" )  # [<p class="body strikeout"></p>]     class 속성의 정확한 문자열 값을 탐색할 수도 있다:  css_soup .
find_all ( "p" ,  class_ = "body strikeout" )  # [<p class="body strikeout"></p>]    그러나 문자열 값을 변형해서 탐색하면 작동하지 않는다:  css_soup .
find_all ( "p" ,  class_ = "strikeout body" )  # []    class_ 를 위한 간편한 방법이 뷰티플수프 모든 버전에 존재한다.
find() -유형의 메쏘드에 건네는 두 번째 인자는 attrs 인데, 문자열을 attrs 에 건네면 그 문자열을 CSS 클래스처럼 탐색한다:  soup .
find_all ( "a" ,  "sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    정규 표현식, 함수 또는 사전을 제외하고 True–유형으로도 건넬 수 있다.
무엇을 건네든지 그 CSS 클래스를 탐색하는데 사용된다.
class_ 키워드 인자에 건넬 때와 똑같다:  soup .
find_all ( "p" ,  re .
compile ( "itl" ))  # [<p class="title"><b>The Dormouse's story</b></p>]    사전을 attrs 에 건네면, 단지 그 CSS 클래스만 아니라 한번에 많은 HTML 속성을 탐색할 수 있다.
다음 코드 두 줄은 동등하다:  soup .
compile ( "elsie" ),  id = 'link1' )  soup .
find_all ( attrs = { 'href'  :  re .
compile ( "elsie" ),  'id' :  'link1' })    이것은 별로 유용한 특징은 아니다.
왜냐하면 보통 키워드 인자를 사용하는 편이 더 쉽기 때문이다.
text 인자 ¶   text 인자로 태그 대신 문자열을 탐색할 수 있다.
name 과 키워드 인자에서처럼, 문자열 , 정규 표현식 , 리스트 , 함수 , 또는 True 값 을 건넬 수 있다.
다음은 몇 가지 예이다:  soup .
find_all ( text = is_the_only_string_within_a_tag )  # [u"The Dormouse's story", u"The Dormouse's story", u'Elsie', u'Lacie', u'Tillie', u'...']     text 가 문자열 찾기에 사용되지만, 태그를 찾는 인자와 결합해 사용할 수 있다: 뷰티플수프는 text 에 대한 값에 자신의 .string 이 부합하는 태그를 모두 찾는다.
다음 코드는 자신의 .string 이 “Elsie”인 <a> 태그를 찾는다:  soup .
find_all ( "a" ,  text = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]      limit 인자 ¶  find_all() 메쏘드는 여과기에 부합하는 문자열과 태그를 모두 돌려준다.
이런 방법은 문서가 방대하면 시간이 좀 걸릴 수 있다.
결과가 모조리 필요한 것은 아니라면, limit 에 숫자를 건넬 수 있다.
이 방법은 SQL에서의 LIMIT 키워드와 정확히 똑같이 작동한다.
뷰티플수프에게 특정 횟수를 넘어서면 결과 수집을 중지하라고 명령한다.
“three sisters” 문서에 링크가 세 개 있지만, 다음 코드는 앞의 두 링크만 찾는다:  soup .
find_all ( "a" ,  limit = 2 )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]       recursive 인자 ¶  mytag.find_all() 를 호출하면, 뷰티플수프는 mytag 의 후손을 모두 조사한다: 그의 자손, 그 자손의 자손, 그리고 등등.
뷰티플수프에게 직계 자손만 신경쓰라고 시키고 싶다면, recursive=False 를 건네면 된다.
다음에 차이점을 살펴보자:  soup .
find_all ( "title" ,  recursive = False )  # []    다음은 예제 문서의 일부이다:  <html>
 <head>
  <title>
   The Dormouse's story
  </title>
 </head>
...
  <title> 태그는 <html> 태그 아래에 있지만, <html> 태그 바로 아래에 있는 것은 아니다 : <head> 태그가 사이에 있다.
뷰티플수프는 <html> 태그의 모든 후손을 찾아 보도록 허용해야만 <title> 태그를 발견한다.
그러나 recursive=False 가 검색을
<html> 태그의 직접 자손으로 제한하기 때문에, 아무것도 찾지 못한다.
뷰티플수프는 트리-탐색 메쏘드들을 다양하게 제공한다 (아래에 다룸).
대부분 find_all() 과 같은 인자를 취한다: name , attrs , text , limit , 그리고 키워드 인자를 취한다.
그러나 recursive 인자는 다르다: find_all() 과 find() 만 유일하게 지원한다.
recursive=False 를 find_parents() 같은 인자에 건네면 별로 유용하지 않을 것이다.
태그를 호출하는 것은 find_all() 을 호출하는 것과 똑같다 ¶   find_all() 는 뷰티플수프 탐색 API에서 가장 많이 사용되므로, 그에 대한 간편 방법을 사용할 수 있다.
BeautifulSoup 객체나 Tag 객체를 마치 함수처럼 다루면, 그 객체에 대하여 find_all() 를 호출하는 것과 똑같다.
find_all ( "a" )  soup ( "a" )    다음 두 줄도 역시 동등하다:  soup .
title ( text = True )      find() ¶  서명: find( name , attrs , recursive , text , **kwargs )   find_all() 메쏘드는 전체 문서를 훓어서 결과를 찾지만, 어떤 경우는 결과 하나만 원할 수도 있다.
문서에 오직 <body> 태그가 하나 뿐임을 안다면, 전체 문서를 훓어 가면서 더 찾는 것은 시간 낭비이다.
find_all 메쏘드를 호출할 때마다, limit=1 을 건네기 보다는 find() 메쏘드를 사용하는 편이 좋다.
다음 코드 두 줄은 거의 동등하다 :  soup .
find ( 'title' )  # <title>The Dormouse's story</title>    유일한 차이점은 find_all() 메쏘드가 단 한개의 결과만 담고 있는 리스트를 돌려주고, find() 는 그냥 그 결과를 돌려준다는 점이다.
find_all() 이 아무것도 찾을 수 없다면, 빈 리스트를 돌려준다.
find() 가 아무것도 찾을 수 없다면, None 을 돌려준다:  print ( soup .
find ( "nosuchtag" ))  # None     태그 이름을 사용하여 항해하기 에서 soup.head.title 트릭을 기억하시는지?
그 트릭은 반복적으로 find() 를 호출해서 작동한다:  soup .
find ( "title" )  # <title>The Dormouse's story</title>      find_parents() 그리고 find_parent() ¶  서명: find_parents( name , attrs , text , limit , **kwargs )  서명: find_parent( name , attrs , text , **kwargs )  많은 시간을 할애해 find_all() 과 find() 를 다루었다.
뷰티플수프 API에는  트리 탐색을 위해 다른 메쏘드가 열가지 정의되어 있지만, 걱정하지 말자.
이런 메쏘드중 다섯가지는 기본적으로 find_all() 과 똑같고, 다른 다섯가지는 기본적으로 find() 와 똑같다.
유일한 차이점은 트리의 어떤 부분을 검색할 것인가에 있다.
먼저 find_parents() 와 find_parent() 를 살펴보자.
find_all() 과 find() 는 트리를 내려 오면서, 태그의 후손들을 찾음을 기억하자.
다음 메쏘드들은 정 반대로 일을 한다: 트리를 위로 올라가며, 한 태그의 (또는 문자열의) 부모를 찾는다.
시험해 보자.“three daughters” 문서 깊숙히 묻힌 문자열부터 시작해 보자:  a_string = soup.find(text="Lacie")
a_string
# u'Lacie'

a_string.find_parents("a")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]

a_string.find_parent("p")
# <p class="story">Once upon a time there were three little sisters; and their names were
#  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;
#  and they lived at the bottom of a well.</p>

a_string.find_parents("p", class_="title")
# []   세가지 <a> 태그 중 하나는 해당 문자열의 직계 부모이다.
그래서 탐색해서 그것을 찾는다.
세가지 <p> 태그 중 하나는 그 문자열의 방계 부모이고, 그것도 역시 잘 탐색한다.
CSS 클래스가“title”인 <p> 태그가 문서 어딘가에 존재하지만, 그것은 이 문자열의 부모가 아니므로, find_parents() 로 부모를 찾을 수 없다.
아마도 find_parent() 와 find_parents() , 그리고 앞서 언급한 .parent 와 .parents 속성 사이에 관련이 있으리라 짐작했을 것이다.
이 관련은 매우 강력하다.
이 탐색 메쏘드들은 실제로 .parents 로 부모들을 모두 찾아서, 제공된 여과기준에 부합하는지 하나씩 점검한다.
find_next_siblings() 그리고 find_next_sibling() ¶  서명: find_next_siblings( name , attrs , text , limit , **kwargs )  서명: find_next_sibling( name , attrs , text , **kwargs )  이 메쏘드들은 .next_siblings 을 사용하여 트리에서 한 요소의 나머지 형제들을 반복한다.
find_next_siblings() 메쏘드는 부합하는 형제들을 모두 돌려주고, find_next_sibling() 메쏘드는 그 중 첫 째만 돌려준다:  first_link  =  soup .
find_next_sibling ( "p" )  # <p class="story">...</p>      find_previous_siblings() 그리고 find_previous_sibling() ¶  서명: find_previous_siblings( name , attrs , text , limit , **kwargs )  서명: find_previous_sibling( name , attrs , text , **kwargs )  이 메쏘드들은 .previous_siblings 를 사용하여 트리에서 한 원소의 앞에 나오는 형제들을 반복한다.
find_previous_siblings() 메쏘는 부합하는 형제들을 모두 돌려주고, find_previous_sibling() 는 첫 째만 돌려준다:  last_link  =  soup .
find_previous_sibling ( "p" )  # <p class="title"><b>The Dormouse's story</b></p>      find_all_next() 그리고 find_next() ¶  서명: find_all_next( name , attrs , text , limit , **kwargs )  서명: find_next( name , attrs , text , **kwargs )  이 메쏘드들은 .next_elements 를 사용하여 문서에서 한 태그의 뒤에 오는 태그이든 문자열이든 무엇이든지 반복한다.
find_all_next() 메쏘드는 부합하는 것들을 모두 돌려주고, find_next() 는 첫 번째 부합하는 것만 돌려준다:  first_link  =  soup .
find_next ( "p" )  # <p class="story">...</p>    첫 예제에서, 문자열 “Elsie”가 나타났다.
물론 그 안에 우리가 시작했던 <a> 태그 안에 포함되어 있음에도 불구하고 말이다.
두 번째 예제를 보면, 문서의 마지막 <p> 태그가 나타났다.
물론 트리에서 우리가 시작했던 <a> 태그와 같은 부분에 있지 않음에도 불구하고 말이다.
이런 메쏘드들에게, 유일한 관심 사항은 원소가 여과 기준에 부합하는가 그리고 시작 원소 말고 나중에 문서에 나타나는가이다.
find_all_previous() 그리고 find_previous() ¶  서명: find_all_previous( name , attrs , text , limit , **kwargs )  서명: find_previous( name , attrs , text , **kwargs )  이 메쏘드들은 .previous_elements 를 사용하여 문서에서 앞에 오는 태그나 문자열들을 반복한다.
find_all_previous() 메쏘드는 부합하는 모든 것을 돌려주고, find_previous() 는 첫 번째 부합만 돌려준다:  first_link  =  soup .
find_previous ( "title" )  # <title>The Dormouse's story</title>    find_all_previous("p") 를 호출하면 문서에서 첫 번째 문단(class=”title”)을 찾지만, 두 번째 문단 <p> 태그도 찾는다.
이 안에 우리가 시작한 <a> 태그가 들어 있다.
이것은 그렇게 놀랄 일이 아니다: 시작한 위치보다 더 앞에 나타나는 태그들을 모두 찾고 있는 중이다.
<a> 태그가 포함된 <p> 태그는 자신 안에 든 <a> 태그보다 먼저 나타나는 것이 당연하다.
CSS 선택자 ¶  뷰티플수프는 CSS  선택자 표준 의 부분집합을 지원한다.
그냥 문자열로 선택자를 구성하고 그것을 Tag 의 .select() 메쏘드 또는 BeautifulSoup 객체 자체에 건네면 된다.
다음과 같이 태그를 검색할 수 있다:  soup .
select ( "title" )  # [<title>The Dormouse's story</title>]    다른 태그 아래의 태그를 찾을 수 있다:  soup .
select ( "html head title" )  # [<title>The Dormouse's story</title>]    다른 태그 바로 아래에 있는 태그를 찾을 수 있다:  soup .
select ( "body > a" )  # []    CSS 클래스로 태그를 찾는다:  soup .
select ( "[class~=sister]" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    ID로 태그를 찾는다:  soup .
select ( "a#link2" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    속성이 존재하는지 테스트 한다:  soup .
select ( 'a[href]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    속성 값으로 태그를 찾는다:  soup .
select ( 'a[href*=".com/el"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    언어 코덱을 일치 시킨다:  multilingual_markup  =  """  <p lang="en">Hello</p>  <p lang="en-us">Howdy, y'all</p>  <p lang="en-gb">Pip-pip, old fruit</p>  <p lang="fr">Bonjour mes amis</p>  """  multilingual_soup  =  BeautifulSoup ( multilingual_markup )  multilingual_soup .
select ( 'p[lang|=en]' )  # [<p lang="en">Hello</p>,  #  <p lang="en-us">Howdy, y'all</p>,  #  <p lang="en-gb">Pip-pip, old fruit</p>]    이것은 CSS 선택자 구문을 알고 있는 사용자에게 유용하다.
이 모든 일들을 뷰티플수프 API로 할 수 있다.
CSS 선택자만 필요하다면, lxml을 직접 사용하는 편이 좋을 것이다.
왜냐하면, 더 빠르기 때문이다.
그러나 이렇게 하면 간단한 CSS 선택자들을 뷰티플수프 API와 조합해 사용할 수 있다 .
트리 변형하기 ¶  뷰티플수프의 강점은 해석 트리를 검색 하는데에 있다.
그러나 또한 해석 트리를 변형해서 새로운 HTML 또는 XML 문서로 저장할 수도 있다.
태그 이름과 속성 바꾸기 ¶  이에 관해서는 속성 부분에서 다룬 바 있지만, 다시 반복할 가치가 있다.
태그 이름을 바꾸고 그의 속성 값들을 바꾸며, 속성을 새로 추가하고, 속성을 삭제할 수 있다:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' )  tag  =  soup .
name  =  "blockquote"  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>      .string 변경하기 ¶  태그의 .string 속성을 설정하면, 태그의 내용이 주어진 문자열로 교체된다:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
tag  # <a href="http://example.com/">New link text.</a>    주의하자: 태그에 또 다른 태그가 들어 있다면, 그 태그는 물론 모든 내용이 사라진다.
append() ¶  Tag.append() 로 태그에 내용을 추가할 수 있다.
파이썬 리스트에 .append() 를 호출한 것과 똑같이 작동한다:  soup  =  BeautifulSoup ( "<a>Foo</a>" )  soup .
contents  # [u'Foo', u'Bar']      BeautifulSoup.new_string() 그리고 .new_tag() ¶  문자열을 문서에 추가하고 싶다면, 파이썬 문자열을 append() 에 건네기만 하면 된다.
아니면 BeautifulSoup.new_string() 공장 메쏘드를 호출하면 된다:  soup  =  BeautifulSoup ( "<b></b>" )  tag  =  soup .
contents  # [u'Hello', u' there']    완전히 새로 태그를 만들어야 한다면 어떻게 할까?
최선의 해결책은 BeautifulSoup.new_tag() 공장 메쏘드를 호출하는 것이다:  soup  =  BeautifulSoup ( "<b></b>" )  original_tag  =  soup .
original_tag  # <b><a href="http://www.example.com">Link text.</a></b>    오직 첫 번째 인자, 즉 태그 이름만 있으면 된다.
insert() ¶  Tag.insert() 는 Tag.append() 와 거의 같은데, 단, 새 요소가 반드시 그의 부모의 .contents 끝에 갈 필요는 없다.
원하는 위치 어디든지 삽입된다.
파이썬 리스트의 .insert() 와 똑같이 작동한다:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
contents  # [u'I linked to ', u'but did not endorse', <i>example.com</i>]      insert_before() 그리고 insert_after() ¶   insert_before() 메쏘드는 태그나 문자열을 해석 트리에서 어떤 것 바로 앞에 삽입한다:  soup  =  BeautifulSoup ( "<b>stop</b>" )  tag  =  soup .
b  # <b><i>Don't</i>stop</b>     insert_after() 메쏘드는 해석 트리에서 다른 어떤 것 바로 뒤에 나오도록 태그나 문자열을 이동시킨다:  soup .
contents  # [<i>Don't</i>, u' ever ', u'stop']      clear() ¶  Tag.clear() 은 태그의 내용을 제거한다:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
clear ()  tag  # <a href="http://example.com/"></a>      extract() ¶  PageElement.extract() 는 해석 트리에서 태그나 문자열을 제거한다.
추출하고 남은 태그나 문자열을 돌려준다:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
parent )  None    이 시점에서 두 가지 해석 트리를 가지는 효과가 있다: 하나는 문서를 해석하는데 사용된 BeautifulSoup 객체에 뿌리를 두고, 또 하나는 추출된 그 태그에 뿌리를 둔다.
더 나아가 추출한 요소의 자손들에다 extract 를 호출할 수 있다:  my_string  =  i_tag .
parent )  # None  i_tag  # <i></i>      decompose() ¶  Tag.decompose() 는 태그를 트리에서 제거한 다음, 그와 그의 내용물을 완전히 파괴한다 :  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
decompose ()  a_tag  # <a href="http://example.com/">I linked to</a>      replace_with() ¶  PageElement.replace_with() 는 트리에서 태그나 문자열을 제거하고 그것을 지정한 태그나 문자열로 교체한다:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
replace_with ( new_tag )  a_tag  # <a href="http://example.com/">I linked to <b>example.net</b></a>    replace_with() 는 교체된 후의 태그나 문자열을 돌려준다.
그래서 검사해 보거나 다시 트리의 다른 부분에 추가할 수 있다.
wrap() ¶  PageElement.wrap() 는 지정한 태그에 요소를 둘러싸서 새로운 포장자를 돌려준다:  soup = BeautifulSoup("<p>I wish I was bold.</p>")
soup.p.string.wrap(soup.new_tag("b"))
# <b>I wish I was bold.</b>

soup.p.wrap(soup.new_tag("div")
# <div><p><b>I wish I was bold.</b></p></div>   다음 메쏘드는 뷰티플수프 4.0.5에 새로 추가되었다.
unwrap() ¶  Tag.unwrap() 은 wrap() 의 반대이다.
태그를 그 태그 안에 있는 것들로 교체한다.
조판을 걷어내 버릴 때 좋다:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
unwrap ()  a_tag  # <a href="http://example.com/">I linked to example.com</a>    replace_with() 처럼, unwrap() 은 교체된 후의 태그를 돌려준다.
(이전 뷰티플수프 버전에서, unwrap() 는 replace_with_children() 이라고 불리웠으며, 그 이름은 여전히 작동한다.)
출력 ¶   예쁘게-인쇄하기 ¶  prettify() 메쏘드는 뷰티플수프 해석 트리를 멋지게 모양을 낸 유니코드 문자열로 변환한다.
HTML/XML 태그마다 따로따로 한 줄에 표시된다:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  soup .
prettify ())  # <html>  #  <head>  #  </head>  #  <body>  #   <a href="http://example.com/">  #    I linked to  #    <i>  #     example.com  #    </i>  #   </a>  #  </body>  # </html>    최상위 BeautifulSoup 객체에 prettify() 를 호출할 수 있으며, 또는 Tag 객체에 얼마든지 호출할 수 있다:  print ( soup .
prettify ())  # <a href="http://example.com/">  #  I linked to  #  <i>  #   example.com  #  </i>  # </a>      있는-그대로 인쇄하기 ¶  멋진 모양 말고 그냥 문자열을 원한다면, BeautifulSoup 객체, 또는 그 안의 Tag 에 unicode() 또는 str() 을 호출하면 된다:  str ( soup )  # '<html><head></head><body><a href="http://example.com/">I linked to <i>example.com</i></a></body></html>'  unicode ( soup .
a )  # u'<a href="http://example.com/">I linked to <i>example.com</i></a>'    str() 함수는  UTF-8로 인코드된 문자열을 돌려준다.
다른 옵션은 인코딩 을 살펴보자.
또 encode() 를 호출하면 bytestring을 얻을 수 있고, decode() 로는 유니코드를 얻는다.
출력 포맷터 ¶  뷰티플수프 문서에 “&lquot;”와 같은 HTML 개체가 들어 있다면, 그 개체들은 유니코드 문자로 변환된다:  soup  =  BeautifulSoup ( "&ldquo;Dammit!&rdquo; he said."
)  unicode ( soup )  # u'<html><head></head><body>\u201cDammit!\u201d he said.</body></html>'    문서를 문자열로 변환하면, 유니코드 문자들은  UTF-8로 인코드된다.
HTML 개체는 다시 복구할 수 없다:  str ( soup )  # '<html><head></head><body>\xe2\x80\x9cDammit!\xe2\x80\x9d he said.</body></html>'    기본 값으로, 출력에서 피신 처리가 되는 유일한 문자들은 앰퍼센드와 옆꺽쇠 문자들이다.
이런 문자들은 “&amp;”, “&lt;”, 그리고 “&gt;”로 변환된다.
그래서 뷰티플수프는 무효한 HTML이나 XML을 생성하는 실수를 하지 않게 된다:  soup  =  BeautifulSoup ( "<p>The law firm of Dewey, Cheatem, & Howe</p>" )  soup .
a  # <a href="http://example.com/?foo=val1&amp;bar=val2">A link</a>    이 행위를 바꾸려면 formatter 인자용 값을 prettify() , encode() , 또는 decode() 에 제공하면 된다.
뷰티플수프는 formatter 에 대하여 가능한 네 가지 값을 인지한다.
기본값은 formatter="minimal" 이다.
문자열은 뷰티플수프가 유효한 HTML/XML을 생산한다고 확신할 만큼 처리된다:  french  =  "<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>"  soup  =  BeautifulSoup ( french )  print ( soup .
prettify ( formatter = "minimal" ))  # <html>  #  <body>  #   <p>  #    Il a dit &lt;&lt;Sacré bleu!&gt;&gt;  #   </p>  #  </body>  # </html>     formatter="html" 을 건네면, 뷰티플수프는 유니코드 문자를 가능한한 HTML 개체로 변환한다:  print ( soup .
prettify ( formatter = "html" ))  # <html>  #  <body>  #   <p>  #    Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;  #   </p>  #  </body>  # </html>    formatter=None 을 건네면, 뷰티플수프는 출력시 전혀 문자열을 건드리지 않는다.
이것이 가장 빠른 선택이지만, 다음 예제에서와 같이 잘못해서 뷰티플수프가 무효한 HTML/XML을 생산할 가능성이 있다:  print ( soup .
prettify ( formatter = None ))  # <html>  #  <body>  #   <p>  #    Il a dit <<Sacré bleu!>>  #   </p>  #  </body>  # </html>  link_soup  =  BeautifulSoup ( '<a href="http://example.com/?foo=val1&bar=val2">A link</a>' )  print ( link_soup .
encode ( formatter = None ))  # <a href="http://example.com/?foo=val1&bar=val2">A link</a>    마지막으로, formatter 에 함수를 건네면, 뷰티플수프는 문서에서 문자열과 속성 값에 대하여 하나하나 그 함수를 한 번 호출한다.
이 함수에서 무엇이든 할 수 있다.
다음은 문자열을 대문자로 바꾸고 다른 일은 절대로 하지 않는 포맷터이다:  def  uppercase ( str ):  return  str .
prettify ( formatter = uppercase ))  # <html>  #  <body>  #   <p>  #    IL A DIT <<SACRÉ BLEU!>>  #   </p>  #  </body>  # </html>  print ( link_soup .
prettify ( formatter = uppercase ))  # <a href="HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2">  #  A LINK  # </a>    따로 함수를 작성하고 있다면, bs4.dammit 모듈에 있는 EntitySubstitution 클래스에 관하여 알아야 한다.
이 클래스는 뷰티플수프의 표준 포맷터를 클래스 메쏘드로 구현한다:
“html”포맷터는 EntitySubstitution.substitute_html 이고, “minimal” 포맷터는 EntitySubstitution.substitute_xml 이다.
이 함수들을 사용하면 formatter=html 나 formatter==minimal 를 흉내낼 수 있지만, 더 처리해야할 일이 있다.
다음은 가능하면 유니코드 문자를 HTML 개체로 교체하는 예제이다.
그러나 또한 모든 문자열을 대문자로 바꾼다:  from  bs4.dammit  import  EntitySubstitution  def  uppercase_and_substitute_html_entities ( str ):  return  EntitySubstitution .
prettify ( formatter = uppercase_and_substitute_html_entities ))  # <html>  #  <body>  #   <p>  #    IL A DIT &lt;&lt;SACR&Eacute; BLEU!&gt;&gt;  #   </p>  #  </body>  # </html>    마지막 단점: CData 객체를 만들면, 그 객체 안의 텍스트는 언제나 포맷팅 없이도, 정확하게 똑같이 나타난다 .
문서에서 문자열 같은 것들을 세는 메쏘드를 손수 만들 경우, 뷰티플수프는 포맷터 메쏘드를 호출한다.
그러나 반환 값은 무시된다.
from bs4.element import CData
soup = BeautifulSoup(“<a></a>”)
soup.a.string = CData(“one < three”)
print(soup.a.prettify(formatter=”xml”))
# <a>
#  <!
[CDATA[one < three]]>
# </a>    get_text() ¶  문서나 태그에서 텍스트 부분만 추출하고 싶다면, get_text() 메쏘드를 사용할 수 있다.
이 메쏘드는 문서나 태그 아래의 텍스트를, 유니코드 문자열 하나로 모두 돌려준다:  markup  =  '<a href="http://example.com/"> \n I linked to <i>example.com</i> \n </a>'  soup  =  BeautifulSoup ( markup )  soup .
get_text ()  u'example.com'    텍스트를 합칠 때 사용될 문자열을 지정해 줄 수 있다:  # soup.get_text("|")  u' \n I linked to |example.com| \n '    뷰티플수프에게 각 테스트의 앞과 뒤에 있는 공백을 걷어내라고 알려줄 수 있다:  # soup.get_text("|", strip=True)  u'I linked to|example.com'    그러나 이 시점에서 대신에 .stripped_strings 발생자를 사용해서, 텍스트를 손수 처리하고 싶을 수 있겠다:  [ text  for  text  in  soup .
stripped_strings ]  # [u'I linked to', u'example.com']       사용할 해석기 지정하기 ¶  단지 HTML만 해석하고 싶을 경우, 조판을 BeautifulSoup 구성자에 넣기만 하면, 아마도 잘 처리될 것이다.
뷰티플수프는 해석기를 여러분 대신 선택해 데이터를 해석한다.
그러나 어느 해석기를 사용할지 바꾸기 위해 구성자에 건넬 수 있는 인자가 몇 가지 더 있다.
BeautifulSoup 구성자에 건네는 첫 번째 인자는 문자열이나 열린 파일핸들-즉 해석하기를 원하는 조판이 첫 번째 인자이다.
두 번째 인자는 그 조판이 어떻게 해석되기를 바라는지 지정한다.
아무것도 지정하지 않으면, 설치된 해석기중 최적의 HTML 해석기가 배당된다.
뷰티플수프는 lxml 해석기를 최선으로 취급하고, 다음에 html5lib 해석기, 그 다음이 파이썬의 내장 해석기를 선택한다.
이것은 다음 중 하나로 덮어쓸 수 있다:   해석하고 싶은 조판의 종류.
현재 “html”, “xml”, 그리고 “html5”가 지원된다.
사용하고 싶은 해석기의 이름.
현재 선택은 “lxml”, “html5lib”, 그리고 “html.parser” (파이썬의 내장  HTML 해석기)이다.
해석기 설치하기 섹션에 지원 해석기들을 비교해 놓았다.
적절한 해석기가 설치되어 있지 않다면, 뷰티플수프는 여러분의 요구를 무시하고 다른 해석기를 선택한다.
지금 유일하게 지원되는 XML 해석기는 lxml이다.
lxml 해석기가 설치되어 있지 않으면,  XML 해석기를 요구할 경우 아무것도 얻을 수 없고, “lxml”을 요구하더라도 얻을 수 없다.
해석기 사이의 차이점들 ¶  뷰티플수프는 다양한 해석기에 대하여 인터페이스가 같다.
그러나 각 해석기는 다르다.
해석기마다 같은 문서에서 다른 해석 트리를 만들어낸다.
가장 큰 차이점은 HTML 해석기와 XML 해석기 사이에 있다.
다음은 HTML로 해석된 짧은 문서이다:  BeautifulSoup ( "<a><b /></a>" )  # <html><head></head><body><a><b></b></a></body></html>    빈 <b /> 태그는 유효한 HTML이 아니므로, 해석기는 그것을 <b></b> 태그 쌍으로 변환한다.
다음 똑같은 문서를 XML로 해석한 것이다 (이를 실행하려면 lxml이 설치되어 있어야 한다).
빈 <b /> 태그가 홀로 남았음에 유의하자.
그리고  <html> 태그를 출력하는 대신에 XML 선언이 주어졌음을 주목하자:  BeautifulSoup ( "<a><b /></a>" ,  "xml" )  # <?xml version="1.0" encoding="utf-8"?>  # <a><b /></a>    HTML 해석기 사이에서도 차이가 있다.
뷰티플수프에 완벽하게 모양을 갖춘 HTML 문서를 주면, 이 차이는 문제가 되지 않는다.
비록 해석기마다 속도에 차이가 있기는 하지만, 모두 원래의 HTML 문서와 정확하게 똑같이 보이는 데이터 구조를 돌려준다.
그러나 문서가 불완전하게 모양을 갖추었다면, 해석기마다 결과가 다르다.
다음은 짧은 무효한 문서를 lxml의 HTML 해석기로 해석한 것이다.
나홀로 </p> 태그는 그냥 무시된다:  BeautifulSoup ( "<a></p>" ,  "lxml" )  # <html><body><a></a></body></html>    다음은 같은 문서를 html5lib로 해석하였다:  BeautifulSoup ( "<a></p>" ,  "html5lib" )  # <html><head></head><body><a><p></p></a></body></html>    나홀로 </p> 태그를 무시하는 대신에, html5lib는 여는 <p> 태그로 짝을 맞추어 준다.
이 해석기는 또한 빈 <head> 태그를 문서에 추가한다.
다음은 같은 문서를 파이썬 내장 HTML 해석기로 해석한 것이다:  BeautifulSoup ( "<a></p>" ,  "html.parser" )  # <a></a>    html5lib처럼, 이 해석기는 닫는 </p> 태그를 무시한다.
html5lib와 다르게, 이 해석기는 <body> 태그를 추가해서 모양을 갖춘 HTML 문서를 생성하려고 아무 시도도 하지 않는다.
lxml과 다르게, 심지어 <html> 태그를 추가하는 것에도 신경쓰지 않는다.
문서 “<a></p>”는 무효하므로, 이 테크닉중 어느 것도 “올바른” 처리 방법이 아니다.
html5lib 해석기는 HTML5 표준에 있는 테크닉을 사용하므로, 아무래도 “가장 올바른” 방법이라고 주장할 수 있지만, 세 가지 테크닉 모두 같은 주장을 할 수 있다.
해석기 사이의 차이점 때문에 스크립트가 영향을 받을 수 있다.
스크립트를 다른 사람들에게 나누어 줄 계획이 있다면, 또는 여러 머신에서 실행할 생각이라면, BeautifulSoup 구성자에 해석기를 지정해 주는 편이 좋다.
그렇게 해야 여러분이 해석한 방식과 다르게 사용자가 문서를 해석할 위험성이 감소한다.
인코딩 ¶  HTML이든 XML이든 문서는 ASCII나 UTF-8 같은 특정한 인코딩으로 작성된다.
그러나 문서를 뷰티플수프에 적재하면, 문서가 유니코드로 변환되었음을 알게 될 것이다:  markup  =  "<h1>Sacr \xc3\xa9 bleu!</h1>"  soup  =  BeautifulSoup ( markup )  soup .
마법이 아니다(확실히 좋은 것이다.).
뷰티플수프는 Unicode, Dammit 라는 하위 라이브러리를 사용하여 문서의 인코딩을 탐지하고 유니코드로 변환한다.
자동 인코딩 탐지는 BeautifulSoup 객체의 .original_encoding 속성으로 얻을 수 있다:  soup .
original_encoding  'utf-8'    Unicode, Dammit은 대부분 올바르게 추측하지만, 가끔은 실수가 있다.
가끔 올바르게 추측하지만, 문서를 바이트 하나 하나 오랫동안 탐색한 후에야 그렇다.
혹시 문서의 인코딩을 미리 안다면, 그 인코딩을 BeautifulSoup 구성자에 from_encoding 로 건네면 실수를 피하고 시간을 절약할 수 있다.
다음은 ISO-8859-8로 작성된 문서이다.
이 문서는 Unicode, Dammit이 충분히 살펴보기에는 너무 짧아서, ISO-8859-7로 잘못 인식한다:  markup = b"<h1>\xed\xe5\xec\xf9</h1>"
soup = BeautifulSoup(markup)
soup.h1
<h1>νεμω</h1>
soup.original_encoding
'ISO-8859-7'   이를 해결하려면 올바른 from_encoding 을 건네면 된다:  soup = BeautifulSoup(markup, from_encoding="iso-8859-8")
soup.h1
<h1>םולש</h1>
soup.original_encoding
'iso8859-8'   아주 드물게 (보통 UTF-8 문서 안에 텍스트가 완전히 다른 인코딩으로 작성되어 있을 경우), 유일하게 유니코드를 얻는 방법은 몇 가지 문자를 특별한 유니코드 문자 “REPLACEMENT CHARACTER” (U+FFFD, �)로 교체하는 것이다 .
Unicode, Dammit이 이를 필요로 하면, UnicodeDammit 이나 BeautifulSoup 객체에 대하여 .contains_replacement_characters 속성에 True 를 설정할 것이다.
이렇게 하면 유니코드 표현이 원래의 정확한 표현이 아니라는 사실을 알 수 있다.
약간 데이터가 손실된다.
문서에  �가 있지만, .contains_replacement_characters 가 False 라면, 원래부터 거기에 있었고 데이터 손실을 감내하지 않는다는 사실을 알게 될 것이다.
출력 인코딩 ¶  뷰티플수프로 문서를 작성할 때, UTF-8 문서를 얻는다.
그 문서가 처음에는 UTF-8이 아니었다고 할지라도 말이다.
다음은 Latin-1 인코딩으로 작성된 문서이다:  markup  =  b '''  <html>  <head>  <meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" />  </head>  <body>  <p>Sacr \xe9 bleu!</p>  </body>  </html>  '''  soup  =  BeautifulSoup ( markup )  print ( soup .
#   </p>  #  </body>  # </html>    <meta> 태그가 재작성되어 문서가 이제 UTF-8이라는 사실을 반영하고 있음을 주목하자.
UTF-8이 싫으면, 인코딩을 prettify() 에 건넬 수 있다:  print ( soup .
prettify ( "latin-1" ))  # <html>  #  <head>  #   <meta content="text/html; charset=latin-1" http-equiv="Content-type" />  # ...
   또 encode()를 BeautifulSoup 객체, 또는 수프의 다른 어떤 요소에라도 호출할 수 있다.
마치 파이썬 문자열처럼 말이다:  soup .
encode ( "utf-8" )  # '<p>Sacr\xc3\xa9 bleu!</p>'    선택한 인코딩에서 표현이 불가능한 문자는 숫자의 XML 개체 참조로 변환된다.
다음은 유니코드 문자 SNOWMAN이 포함된 문자이다:  markup  =  u"<b> \N{SNOWMAN} </b>"  snowman_soup  =  BeautifulSoup ( markup )  tag  =  snowman_soup .
b    눈사람 문자는 UTF-8 문서에 포함될 수 있지만 (☃처럼 생김), ISO-Latin-1이나 ASCII에 그 문자에 대한 표현이 없다.
그래서 “&#9731”으로 변환된다:  print ( tag .
encode ( "ascii" )  # <b>&#9731;</b>      이런, 유니코드군 ¶  뷰티플수프를 사용하지 않더라도 유니코드를 사용할 수 있다.
인코딩을 알 수 없는 데이터가 있을 때마다 그냥 유니코드가 되어 주었으면 하고 바라기만 하면 된다:  from  bs4  import  UnicodeDammit  dammit  =  UnicodeDammit ( "Sacr \xc3\xa9 bleu!"
original_encoding  # 'utf-8'    유니코드에 더 많은 데이터를 줄 수록, Dammit은 더 정확하게 추측할 것이다.
나름대로 어떤 인코딩일지 짐작이 간다면, 그것들을 리스트로 건넬 수 있다:  dammit  =  UnicodeDammit ( "Sacr \xe9 bleu!"
original_encoding  # 'latin-1'    Unicode, Dammit는 뷰티플수프가 사용하지 않는 특별한 특징이 두 가지 있다.
지능형 따옴표 ¶  Unicode, Dammit을 사용하여 마이크로소프트 지능형 따옴표를 HTML이나 XML 개체로 변환할 수 있다:  markup  =  b "<p>I just \x93 love \x94 Microsoft Word \x92 s smart quotes</p>"  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "html" ) .
unicode_markup  # u'<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'    또 마이크로소프트 지능형 따옴표를 ASCII 따옴표로 변환할 수 있다:  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "ascii" ) .
unicode_markup  # u'<p>I just "love" Microsoft Word\'s smart quotes</p>'    모쪼록 이 특징이 쓸모가 있기를 바라지만, 뷰티플수프는 사용하지 않는다.
뷰티플수프는 기본 행위를 선호하는데, 기본적으로 마이크로소프트 지능형 따옴표를 다른 모든 것과 함께 유니코드 문자로 변환한다:  UnicodeDammit ( markup ,  [ "windows-1252" ]) .
unicode_markup  # u'<p>I just \u201clove\u201d Microsoft Word\u2019s smart quotes</p>'      비 일관적인 인코딩 ¶  어떤 경우 문서 대부분이 UTF-8이지만, 안에 (역시) 마이크로소프트 지능형 따옴표와 같이 Windows-1252 문자가 들어 있는 경우가 있다.
한 웹 사이트에 여러 소스로 부터 데이터가 포함될 경우에 이런 일이 일어날 수 있다.
UnicodeDammit.detwingle() 을 사용하여 그런 문서를 순수한 UTF-8 문서로 변환할 수 있다.
다음은 간단한 예이다:  snowmen  =  ( u" \N{SNOWMAN} "  *  3 )  quote  =  ( u" \N{LEFT DOUBLE QUOTATION MARK} I like snowmen!
encode ( "windows_1252" )    이 문서는 뒤죽박죽이다.
눈사람은 UTF-8인데 따옴표는 Windows-1252이다.
눈사람 아니면 따옴표를 화면에 나타낼 수 있지만, 둘 다 나타낼 수는 없다:  print ( doc )  # ☃☃☃�I like snowmen!�  print ( doc .
decode ( "windows-1252" ))  # â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”    문서를 UTF-8로 디코딩하면 UnicodeDecodeError 가 일어나고, Windows-1252로 디코딩하면 알 수 없는 글자들이 출력된다.
다행스럽게도, UnicodeDammit.detwingle() 는 그 문자열을 순수 UTF-8로 변환해 주므로, 유니코드로 디코드하면 눈사람과 따옴표를 동시에 화면에 보여줄 수 있다:  new_doc  =  UnicodeDammit .
decode ( "utf8" ))  # ☃☃☃“I like snowmen!”    UnicodeDammit.detwingle() 는 오직 UTF-8에 임베드된 (또는 그 반대일 수도 있지만) Windows-1252을 다루는 법만 아는데, 이것이 가장 일반적인 사례이다.
BeautifulSoup 이나 UnicodeDammit 구성자에 건네기 전에 먼저 데이터에 UnicodeDammit.detwingle() 을 호출하는 법을 반드시 알아야 한다.
뷰티플수프는 문서에 하나의 인코딩만 있다고 간주한다.
그것이 무엇이든 상관없이 말이다.
UTF-8과 Windows-1252를 모두 포함한 문서를 건네면, 전체 문서가 Windows-1252라고 생각할 가능성이 높고, 그 문서는 다음 ` â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”`처럼 보일 것이다.
UnicodeDammit.detwingle() 은 뷰티플수프 4.1.0에서 새로 추가되었다.
문서의 일부만을 해석하기 ¶  뷰티플수프를 사용하여 문서에서 <a> 태그를 살펴보고 싶다고 해보자.
전체 문서를 해석해서 훓어가며 <a> 태그를 찾는 일은 시간 낭비이자 메모리 낭비이다.
처음부터 <a> 태그가 아닌 것들을 무시하는 편이 더 빠를 것이 분명하다.
SoupStrainer 클래스는 문서에 어느 부분을 해석할지 고르도록 해준다.
그냥 SoupStrainer 를 만들고 그것을 BeautifulSoup 구성자에 parse_only 인자로 건네면 된다.
( 이 특징은 html5lib 해석기를 사용중이라면 작동하지 않음을 주목하자 .
html5lib을 사용한다면, 어쨋거나 문서 전체가 해석된다.
이것은 html5lib가 작업하면서 항상 해석 트리를 재정렬하기 때문이다.
문서의 일부가 실제로 해석 트리에 맞지 않을 경우, 충돌을 일으킨다.
혼란을 피하기 위해, 아래의 예제에서 뷰티플수프에게 파이썬의 내장 해석기를 사용하라고 강제하겠다.)
SoupStrainer ¶  SoupStrainer 클래스는 트리 탐색하기 의 전형적인 메쏘드와 같은 인자들을 취한다: name , attrs , text , 그리고 **kwargs 이 그 인자들이다.
다음은 세 가지 SoupStrainer 객체이다:  from  bs4  import  SoupStrainer  only_a_tags  =  SoupStrainer ( "a" )  only_tags_with_id_link2  =  SoupStrainer ( id = "link2" )  def  is_short_string ( string ):  return  len ( string )  <  10  only_short_strings  =  SoupStrainer ( text = is_short_string )    다시 한 번 더“three sisters” 문서로 돌아가 보겠다.
문서를 세 가지 SoupStrainer 객체로 해석하면 어떻게 보이는지 살펴보자:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_a_tags ) .
prettify ())  # Elsie  # ,  # Lacie  # and  # Tillie  # ...
 #    또한 SoupStrainer 를 트리 탐색하기 에서 다룬 메쏘드에 건넬 수 있다.
이는 별로 유용하지는 않지만, 그럼에도 언급해 둔다:  soup  =  BeautifulSoup ( html_doc )  soup .
find_all ( only_short_strings )  # [u'\n\n', u'\n\n', u'Elsie', u',\n', u'Lacie', u' and\n', u'Tillie',  #  u'\n\n', u'...', u'\n']       문제 해결 ¶   버전 불일치 문제 ¶   SyntaxError:  Invalid  syntax (다음 ROOT_TAG_NAME  =  u'[document]' 줄에서): 코드를 변경하지 않고서, 파이썬 2 버전의 뷰티플수프를 파이썬 3 아래에서 사용하기 때문에 야기된다.
ImportError:  No  module  named  HTMLParser - 파이썬 2 버전의 뷰티플수프를 파이썬 3 아래에서 사용하기 때문에 야기된다.
ImportError:  No  module  named  html.parser - 파이썬 3 버전의 뷰티플수프를 파이썬 2에서 실행하기 때문에 야기된다.
ImportError:  No  module  named  BeautifulSoup - 뷰티플수프 3 코드를 BS3가 설치되어 있지 않은 시스템에서 실행할 때 야기된다.
또는 꾸러미 이름이 bs4 로 바뀌었음을 알지 못하고 뷰티플수프 4 코드를 실행하면 야기된다.
ImportError:  No  module  named  bs4 - 뷰티플수프 4 코드를 BS4가 설치되어 있지 않은 시스템에서 실행하면 야기된다.
XML 해석하기 ¶  기본값으로, 뷰티플수프는 문서를 HTML로 해석한다.
문서를 XML로 해석하려면, “xml”를 두 번째 인자로 BeautifulSoup 구성자에 건네야 한다:  soup  =  BeautifulSoup ( markup ,  "xml" )    lxml이 설치되어 있어야 한다 .
기타 해석기 문제 ¶   스크립트가 한 컴퓨터에서는 잘 되는데 다른 컴퓨터에서는 작동하지 않는다면, 아마도 두 컴퓨터가 다른 해석기를 가지고 있기 때문일 것이다.
예를 들어, lxml이 설치된 컴퓨터에서 스크립트를 개발해 놓고, 그것을 html5lib만 설치된 컴퓨터에서 실행하려고 했을 수 있다.
왜 이것이 문제가 되는지는 해석기들 사이의 차이점 을 참고하고, BeautifulSoup 구성자에 특정 라이브러리를 지정해서 문제를 해결하자.
HTMLParser.HTMLParseError:  malformed  start  tag or HTMLParser.HTMLParseError:  bad  end  tag - 파이썬의 내장 HTML 해석기에 처리가 불가능한 문서를 건네면 야기된다.
다른 HTMLParseError 도 아마 같은 문제일 것이다.
해결책: lxml이나 html5lib를 설치하자.
알고 있는데 문서에서 그 태그를 발견할 수 없다면 (다시 말해, find_all() 이 [] 를 돌려주거나 find() 가 None 을 돌려줄 경우), 아마도 파이썬의 내장 HTML 해석기를 사용하고 있을 가능성이 높다.
이 해석기는 가끔 이해하지 못하면 그 태그를 무시하고 지나간다.
HTML 태그와 속성 은 대소문자를 구별하므로, 세가지 HTML 해석기 모두 태그와 속성 이름을 소문자로 변환한다.
다시 말해, 다음 조판 <TAG></TAG>는 <tag></tag>로 변환된다.
태그와 속성에 대소문자 혼합 또는 대문자를 그대로 유지하고 싶다면, 문서를 XML로 해석할 필요가 있다.
기타 ¶   KeyError:  [attr] - tag['attr'] 에 접근했는데 해당 태그에 attr 속성이 정의되어 있지 않을 때 야기된다.
가장 흔한 에러는 KeyError:  'href' 그리고 KeyError:  'class' 이다.
attr 이 정의되어 있는지 잘 모르겠다면, 파이썬 사전에 그렇게 하듯이, tag.get('attr') 을 사용하자.
UnicodeEncodeError:  'charmap'  codec  can't  encode  character  u'\xfoo'  in  position  bar (또는 그냥 기타 다른 UnicodeEncodeError 에 관한 모든 것) - 이 에러는 뷰티플수프에 관련된 문제가 아니다 .이 문제는 두 가지 상황에서 출현한다.
첫 째, 유니코드 문자열을 인쇄했는데 콘솔이 표시할 줄 모를 경우가 있다.
( 파이썬 위키에서 도움을 받자.)
둘째, 파일에 쓰는데 기본 인코딩으로 지원되지 않는 유니코드 문자열을 건넬 경우가 있다.
이런 경우, 가장 쉬운 해결책은 u.encode("utf8") 을 지정해서 그 유니코드 문자열을 UTF-8로 명시적으로 인코드하는 것이다.
수행성능 개선 ¶  뷰티플수프는 그 밑에 깔린 해석기보다 더 빠를 수는 없다.
응답 시간이 중요하다면, 다시 말해, 시간제로 컴퓨터를 쓰고 있거나 아니면 컴퓨터 시간이 프로그래머 시간보다 더 가치가 있는 다른 이유가 조금이라도 있다면, 그렇다면 뷰티플수프는 잊어 버리고 직접 lxml 위에 작업하는 편이 좋을 것이다.
그렇지만, 뷰티플수프의 속도를 높일 수 있는 방법이 있다.
아래에 해석기로 lxml을 사용하고 있지 않다면, 당장 시작해 보기를 조언한다.
뷰티플수프는 html.parser나 html5lib를 사용하는 것보다 lxml을 사용하는 것이 문서를 상당히 더 빠르게 해석한다.
cchardet 라이브러리를 설치하면 인코딩 탐지 속도를 상당히 높일 수 있다.
가끔 Unicode, Dammit 는 바이트별로 파일을 조사해서 인코딩을 탐지할 수 있을 뿐이다.
이 때문에 뷰티플수프가 기어가는 원인이 된다.
본인의 테스트에 의하면 이런 일은 파이썬 2.x 버전대에서만 일어나고, 러시아나 중국어 인코딩을 사용한 문서에 아주 많이 발생했다.
이런 일이 일어나면, cchardet을 설치하거나, 스크립트에 Python 3를 사용하여 문제를 해결할 수 있다.
혹시 문서의 인코딩을 안다면, 그 인코딩을 BeautifulSoup 구성자에 from_encoding 로 건네면, 인코딩 탐지를 완전히 건너뛴다.
문서의 일부만 해석하기 는 문서를 해석하는 시간을 많이 절약해 주지는 못하겠지만, 메모리가 절약되고, 문서를 훨씬 더 빨리 탐색할 수 있을 것이다 .
뷰티플수프 3 ¶  뷰티플수프 3는 이전의 구형으로서, 더 이상 활발하게 개발되지 않는다.
현재는 주요 리눅스 배포본에 모두 함께 꾸려넣어진다:  $  apt-get  install  python-beautifulsoup  또 PyPi를 통하여 BeautifulSoup 로 출간되어 있다:  $  easy_install  BeautifulSoup  $  pip  install  BeautifulSoup  또한 뷰티플수프 3.2.0 압축파일을 내려받을 수 있다.
easy_install  beautifulsoup 이나 easy_install  BeautifulSoup 을 실행했는데, 코드가 작동하지 않으면, 실수로 뷰티플수프 3을 설치한 것이다.
easy_install  beautifulsoup4 을 실행할 필요가 있다.
뷰티플수프 3 문서는 온라인에 보관되어 있다 .
모국어가 중국어라면, 뷰티플수프 3 문서 중국어 번역본 을 보는 것이 더 쉬울 것이다.
그 다음에 이 문서를 읽고 뷰티플수프 4에서 변한 것들을 알아보자.
BS4로 코드 이식하기 ¶  뷰티플수프 3용 코드는 하나만 살짝 바꾸면 뷰티플수프 4에도 작동한다.
꾸러미 이름을 BeautifulSoup 에서 bs4 로 바꾸기만 하면 된다.
그래서 다음은:  from  BeautifulSoup  import  BeautifulSoup    다음과 같이 된다:  from  bs4  import  BeautifulSoup     “No module named BeautifulSoup”와 같이 ImportError 를 만난다면, 문제는 뷰티플수프 3 코드를 시도하는데 뷰티플수프 4만 설치되어 있기 때문이다.
“No module named bs4”와 같은 ImportError 를 만난다면, 문제는 뷰티플수프 4 코드를 시도하는데 뷰티플수프 3만 설치되어 있기 때문이다.
BS4는 BS3와 대부분 하위 호환성이 있으므로, 대부분의 메쏘드는 폐기되고 PEP 8을 준수하기 위해 새로운 이름이 주어졌다.
이름바꾸기와 변화가 많이 있지만, 그 중에 몇 가지는 하위 호환성이 깨진다.
다음은 BS3 코드를 변환해 BS4에 이식하고자 할 때 알아야 할 것들이다:   해석기가 필요해 ¶  뷰티플수프 3는 파이썬의 SGMLParser 해석기를 사용했다.
이 모듈은 파이썬 3.0에서 제거되었다.
뷰티플수프 4는 기본으로 html.parser 을 사용하지만, 대신에 lxml이나 html5lib을 설치해 사용할 수있다.
비교는 해석기 설치하기 를 참조하자.
html.parser 는 SGMLParser 와 같은 해석기가 아니기 때문에, 무효한 조판을 다르게 취급한다.
보통 “차이점은” 무효한 조판을 다룰 경우  html.parser 가 해석기가 충돌을 일으키는 것이다.
이런 경우, 또다른 해석기를 설치할 필요가 있다.
그러나 html.parser 는 SGMLParser 와는 다른 해석 트리를 생성한다.
이런 일이 일어나면, BS3 코드를 업데이트하여 새로운 트리를 다루도록 해야 할 필요가 있다.
메쏘드 이름 ¶   renderContents -> encode_contents  replaceWith -> replace_with  replaceWithChildren -> unwrap  findAll -> find_all  findAllNext -> find_all_next  findAllPrevious -> find_all_previous  findNext -> find_next  findNextSibling -> find_next_sibling  findNextSiblings -> find_next_siblings  findParent -> find_parent  findParents -> find_parents  findPrevious -> find_previous  findPreviousSibling -> find_previous_sibling  findPreviousSiblings -> find_previous_siblings  nextSibling -> next_sibling  previousSibling -> previous_sibling   뷰티플수프 구성자에 건네는 인자들 중에서 같은 이유로 이름이 바뀌었다:   BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...)  BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...)   파이썬 3와의 호환을 위해 한 가지 메쏘드 이름을 바꾸었다:   Tag.has_key() -> Tag.has_attr()   더 정확한 용어를 위해 한 속성의 이름을 바꾸었다:   Tag.isSelfClosing -> Tag.is_empty_element   파이썬에서 특별한 의미가 있는 단어들을 피해서 세 가지 속성의 이름을 바꾸었다.
다른 것들과 다르게 이 변경사항은 하위 호환이 되지 않는다.
이런 속성을 BS3에 사용하면, BS4로 이식할 때 코드가 깨질 것이다.
UnicodeDammit.unicode -> UnicodeDammit.unicode_markup  Tag.next -> Tag.next_element  Tag.previous -> Tag.previous_element     발생자 ¶  발생자에 PEP 8을-준수하는 이름을 부여하고, 특성으로 변환하였다:   childGenerator() -> children  nextGenerator() -> next_elements  nextSiblingGenerator() -> next_siblings  previousGenerator() -> previous_elements  previousSiblingGenerator() -> previous_siblings  recursiveChildGenerator() -> descendants  parentGenerator() -> parents   그래서 다음과 같이 하는 대신에:  for  parent  in  tag .
parentGenerator ():  ...
   다음과 같이 작성할 수 있다:  for  parent  in  tag .
parents :  ...
   (그러나 구형 코드도 여전히 작동한다.)
어떤 발생자들은 일이 끝난후 None 을 돌려주곤 했다.
그것은 버그였다.
이제 발생자는 그냥 멈춘다.
두 가지 발생자가 새로 추가되었는데, .strings와 .stripped_strings 가 그것이다.
.strings 는 NavigableString 객체를 산출하고, .stripped_strings 는 공백이 제거된 파이썬 문자열을 산출한다.
XML ¶  이제 XML 해석을 위한 BeautifulStoneSoup 클래스는 더 이상 없다.
XML을 해석하려면“xml”을 두번째 인자로 BeautifulSoup 구성자에 건네야 한다.
같은 이유로, BeautifulSoup 구성자는 더 이상 isHTML 인자를 인지하지 못한다.
뷰티플수프의 빈-원소 XML 태그 처리 방식이 개선되었다.
전에는 XML을 해석할 때 명시적으로 어느 태그가 빈-원소 태그로 간주되는지 지정해야 했었다.
구성자에 selfClosingTags 인자를 보내 봐야 더 이상 인지하지 못한다.
대신에,
뷰티플수프는 빈 태그를 빈-원소 태그로 간주한다.
빈-원소 태그에 자손을 하나 추가하면, 더 이상 빈-원소 태그가 아니다.
개체 ¶  HTML이나 XML 개체가 들어 오면 언제나 그에 상응하는 유니코드 문자로 변환된다.
뷰티플수프 3는 개체들을 다루기 위한 방법이 중첩적으로 많았다.
이제 중복이 제거되었다.
BeautifulSoup 구성자는 더 이상 smartQuotesTo 이나 convertEntities 인자를 인지하지 않는다.
( Unicode, Dammit 은 여전히 smart_quotes_to 가 있지만, 그의 기본값은 이제 지능형 따옴표를 유니코드로 변환하는 것이다.)
HTML_ENTITIES , XML_ENTITIES , 그리고 XHTML_ENTITIES 상수는 제거되었다.
왜냐하면 이제 더 이상 존재하지 않는 특징을 구성하기 때문이다 (유니코드 문자열을 제대로 모두 변환하지 못했다).
유니코드 문자들을 다시 출력시에 HTML 개체로 변환하고 싶다면, 그것들을 UTF-8 문자로 변환하기 보다, 출력 포맷터 를 사용할 필요가 있다.
기타 ¶  Tag.string 은 이제 재귀적으로 작동한다.
태그 A에 태그 B만 달랑 있고 다른 것이 없다면, A.string은 B.string과 똑같다.
(이전에서는 None이었다.)
다중-값 속성 은 class 와 같이 문자열이 아니라 문자열 리스트를 그 값으로 가진다.
이 사실은 CSS 클래스로 검색하는 방식에 영향을 미친다.
find* 메쏘드에 text  그리고  name 같은 태그-종속적 인자를 모두 건네면, 뷰티플수프는 태그-종속적 기준에 부합하고 그 태그의 Tag.string 이 text 값에 부합하는 태그들을 탐색한다.
문자열 자체는 찾지 않는다 .
이전에, 뷰티플수프는 태그-종속적 인자는 무시하고 문자열을 찾았다.
BeautifulSoup 구성자는 더 이상 markupMassage 인자를 인지하지 않는다.
이제 조판을 제대로 처리하는 일은 해석기의 책임이다..
 ICantBelieveItsBeautifulSoup 그리고 BeautifulSOAP 와 같이 거의-사용되지 않는 해석기 클래스는 제거되었다.
이제 애매모호한 조판을 처리하는 방법은 해석기가 결정한다.
prettify() 메쏘드는 이제, bytestring이 아니라 유니코드 문자열을 돌려준다.
목차   뷰티플수프 문서  도움 얻기    빨리 시작하기  뷰티플수프 설치  설치 이후의 문제  해석기 설치하기    수프 만들기  객체의 종류  태그(Tag)  이름(Name)  속성(Attributes)  다중-값 속성(Multi-valued attributes)      NavigableString  BeautifulSoup  주석 그리고 기타 특수 문자들    트리 항해하기  트리 내려가기  태그 이름을 사용하여 항해하기  .contents 와 .children  .descendants  .string  .strings 와 stripped_strings    올라가기  .parent  .parents    옆으로 가기  .next_sibling 와 .previous_sibling  .next_siblings 와 .previous_siblings    앞뒤로 가기  .next_element 와 .previous_element  .next_elements 와 .previous_elements      트리 탐색하기  여과기의 종류  문자열  정규 표현식  리스트  True  함수    find_all()   name 인자  키워드 인자  CSS 클래스로 탐색하기  text 인자  limit 인자  recursive 인자    태그를 호출하는 것은 find_all() 을 호출하는 것과 같다.
find()  find_parents() 와 find_parent()  find_next_siblings() 와 find_next_sibling()  find_previous_siblings() 와 find_previous_sibling()  find_all_next() 와 find_next()  find_all_previous() 와 find_previous()  CSS 선택자    트리 변경하기  태그 이름과 속성을 바꾸기  .string 변경하기  append()  BeautifulSoup.new_string() 와 .new_tag()  insert()  insert_before() 와 insert_after()  clear()  extract()  decompose()  replace_with()  wrap()  unwrap()    출력  예쁘게 인쇄하기  있는 그대로 인쇄하기  출력 포맷  get_text()    해석기 지정하기  해석기 사이의 차이점    인코딩  출력 인코딩  이런, 유니코드군  지능형 따옴표  비일관적인 인코딩      문서의 일부만 해석하기  SoupStrainer    문제 해결  버전 불일치 문제  XML 해석하기  기타 해석기 문제  잡동사니  수행성능 향상하기    뷰티플수프 3  BS4로 코드 이식하기  해석기가 필요해  메쏘드 이름  발생자  XML  개체  기타       이 페이지   소스 보여주기    빠른 검색        용어나 모듈, 클래스 또는 함수 이름을 입력하시오.
항해    인덱스  뷰티플수프 4.0.0 문서 »    © Copyright 2012, Leonard Richardson.
Created using Sphinx 1.1.3.
Navigation    index  Beautiful Soup 4.4.0 documentation »  Documentação Beautiful Soup         Documentação Beautiful Soup ¶   Beautiful Soup é uma biblioteca
Python de extração de dados de arquivos HTML e XML.
Ela funciona com o seu interpretador (parser) favorito
a fim de prover maneiras mais intuitivas de navegar, buscar e modificar uma árvore de análise (parse tree).
Ela geralmente economiza horas ou dias de trabalho de programadores ao redor do mundo.
Estas instruções ilustram as principais funcionalidades do Beautiful Soup 4
com exemplos.
Mostro para o que a biblioteca é indicada, como funciona,
como se usa e como fazer aquilo que você quer e o que fazer quando ela frustra suas
expectativas.
Os exemplos nesta documentação devem funcionar da mesma maneira em Python 2.7 e Python 3.2.
Você pode estar procurando pela documentação do Beautiful Soup 3 .
Se está, informo que o Beautiful Soup 3 não está mais sendo desenvolvido,
e que o Beautiful Soup 4 é o recomendado para todos os novos projetos.
Se você quiser saber as diferenças entre as versões 3 e 4, veja Portabilidade de código para BS4 .
Esta documentação foi traduzida para outros idiomas pelos usuários do Beautiful Soup:   这篇文档当然还有中文版.
Como conseguir ajuda: ¶  Se você tem perguntas sobre o Beautiful Soup ou está com dificuldades, envie uma mensagem para nosso grupo de discussão .
Se o seu
problema envolve a interpretação de um documento HTML, não esqueça de mencionar o que a função diagnose() diz sobre seu documento.
Início Rápido ¶  Este é o HTML que usarei como exemplo ao longo deste documento
É um trecho de “Alice no País das Maravilhas”:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """    Executando o arquivo “three sisters” através do Beautiful Soup, ele nos
retorna um objeto BeautifulSoup , que apresenta o documento como uma estrutura
de dados aninhada:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )  print ( soup .
#   </p>  #   <p class="story">  #    ...
 #   </p>  #  </body>  # </html>    Abaixo verificamos algumas maneiras simples de navegar na estrutura:  soup .
find ( id = "link3" )  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>    Uma tarefa comum é extratir todas as URLs encontradas nas tags <a> de uma página:  for  link  in  soup .
get ( 'href' ))  # http://example.com/elsie  # http://example.com/lacie  # http://example.com/tillie    Outra tarefa comum é extrair todo o texto de uma página:  print ( soup .
#  # ...
   Isso se parece com o que você precisa?
Então vá em frente!
Instalando o Beautiful Soup ¶  Se você está usando uma versão recente das distribuições Linux Debian ou Ubuntu,
você pode instalar o Beautiful Soup facilmente utilizando o gerenciador de pacotes  $  apt - get  install  python - bs4 (for Python 2)  $  apt - get  install  python3 - bs4 (for Python 3)  O Beautiful Soup 4 também está publicado no PyPi.
Portanto, se
você não conseguir instalá-lo através de seu gerenciador de pacotes, você
pode fazer isso com easy_install ou pip .
O nome do pacote é beautifulsoup4 ,
e o mesmo pacote é válido tanto para Python 2 quanto Python 3.
Tenha certeza de utilizar
a versão correta de pip ou easy_install para sua versão do Python (estarão
nomeados como pip3 ou easy_install3 ,respectivamente, se você estiver usando Python 3).
$  easy_install  beautifulsoup4  $  pip  install  beautifulsoup4  (O pacote BeautifulSoup provavelmente não é o que você quer.
Esta
é a versão anterior, Beautiful Soup 3 .
Muitos softwares utilizam
BS3, por isso ele ainda está disponível, mas se você está criando algo novo,
você deve instalar o beautifulsoup4 .)
Se você não possui o easy_install ou pip instalados, você pode fazer
o download através do tarball do arquivo fonte do Beautiful Soup 4
< http://www.crummy.com/software/BeautifulSoup/download/4.x/ >`_ e
instalar através do setup.py .
$  python  setup.py  install  Se tudo isso falhar, a licença do Beautiful Soup lhe permite empacotar
toda a biblioteca em sua aplicação.
Você pode fazer o download do arquivo
tarball, copiar o diretório bs4 do código-fonte para sua aplicação e
utilizar o Beautiful Soup sem nenhum processo de instalação.
Eu utilizo Python 2.7 e Python 3.2 para desenvolver o Beautiful Soup,
mas ele também funcionará com outras versões recentes.
Problemas após a instalação ¶  O Beautiful Soup é empacotado em Python 2.
Quando você o instala utilizando
Python 3 ele é automaticamente convertido para esta versão.
Se você não instalar o pacote, o
código não será convertido.
Também foi relatado versões erradas sendo instaladas em
máquinas Windows.
Se você receber um ImportError “No module named HTMLParser”, seu problema
é que você está utilizando o formato de código Python 2 sob Python 3.
Se você receber um ImportError “No module named html.parser”, seu problema
é que você está utilizando o formato de código Python 3 sob Python 2.
Em ambos os casos, sua melhor opção é remover completamente a
instalação do Beautiful Soup do seu sistema (incluindo qualquer diretório
criado quando o tarball foi descompactado) e realizar a instalação novamente.
Se você receber um SyntaxError “Invalid syntax” na linha ROOT_TAG_NAME  =  u'[document]' , você terá que converter o Python 2
em Python 3.
Você pode fazer isso instalando o pacote:  $  python3  setup.py  install  ou manualmente executando o script de conversão 2to3 no
diretório bs4 :  $  2to3 - 3.2  - w  bs4    Instalando um interpretador (parser) ¶  O Beautiful Soup não só suporta o parser HTML incluído na biblioteca
padrão do Python como também inúmeros parsers de terceiros.
Um deles é o parser lxml .
Dependendo de sua configuração,
você podera instalar o lxml com algum dos seguintes comandos:  $  apt - get  install  python - lxml  $  easy_install  lxml  $  pip  install  lxml  Outra alternativa é o parser html5lib do Python puro, o qual analisa o HTML
da mesma maneira que o navegador o faz.
Dependendo de sua configuração,
você podera instalar o html5lib com algum dos seguintes comandos:  $  apt - get  install  python - html5lib  $  easy_install  html5lib  $  pip  install  html5lib  Esta tabela resume as vantagens e desvantagens de cada parser:-   Parser  Uso Padrão  Vantagens  Desvantagens   html.parser (puro)  BeautifulSoup(markup,  "html.parser")   Baterias inclusas  Velocidade Decente  Leniente (Python 2.7.3
e 3.2.)
Não tão rápido quanto
lxml, menos leniente
que html5lib.
HTML (lxml)  BeautifulSoup(markup,  "lxml")   Muito rápido  Leniente     Dependencia externa de
C     XML (lxml)  BeautifulSoup(markup,  "lxml-xml")  BeautifulSoup(markup,  "xml")   Muito rápido  O único parser XML atualmente
suportado     Dependência externa de
C     html5lib  BeautifulSoup(markup,  "html5lib")   Extremamente leniente  Analisa as páginas da mesma
maneira que o navegador o faz  Cria HTML5 válidos     Muito lento  Dependência externa de
Python      Se for possível recomendo que você instale e utilize o lxml pelo desempenho.
Se você está utilizando o Python 2 anterior a 2.7.3 ou uma versão do Python 3
anterior a 3.2.2, é essencial que você instale o lxml ou o html5lib.
O parser
HTML nativo do Python não é muito bom para versões mais antigas.
Note que se um documento é inválido, diferentes parsers irão gerar
diferentes árvores BeautifulSoup para isso.
Veja Diferenças entre os interpretadores (parsers) para detalhes.
Criando a “Sopa” ¶  Para analisar um documento, passe-o como argumento dentro de um construtor BeautifulSoup .
Você pode passar este argumento como uma string ou manipulador da função open():  from  bs4  import  BeautifulSoup  with  open ( "index.html" )  as  fp :  soup  =  BeautifulSoup ( fp )  soup  =  BeautifulSoup ( "<html>data</html>" )    Primeiro, o documento é convertido para Unicode e as entidades HTML
são convertidas para caracteres Unicode:  BeautifulSoup("Sacr&eacute; bleu!")
<html><head></head><body>Sacré bleu!</body></html>   O Beautiful Soup então interpreta o documento usando o melhor parser disponível.
Ele irá utilizar um parser HTML ao menos que você indique a ele que utilize um
parser XML.
(Veja Analisando um XML .)
Tipos de objetos ¶  O Beautiful Soup transforma um documento HTML complexo em uma complexa árvore de objetos Python.
Mas você terá apenas que lidar com quatro tipos de objetos: Tag , NavigableString , BeautifulSoup ,
e Comment .
Tag ¶  Um objeto Tag corresponde a uma tag XML ou HTML do documento original:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' )  tag  =  soup .
b  type ( tag )  # <class 'bs4.element.Tag'>    As tags possuem muitos atributos e métodos que eu falarei mais sobre em Navegando pela árvore e Buscando na árvore .
Por agora, as características
mais importantes da tag são seu nome e atributos.
Nome ¶  Toda tag possui um nome, acessível através de .name :  tag .
name  # u'b'    Se você mudar o nome de uma tag, a alteração será refletida em qualquer HTML gerado pelo
Beautiful Soup:  tag .
name  =  "blockquote"  tag  # <blockquote class="boldest">Extremely bold</blockquote>      Atributos ¶  Uma tag pode ter inúmeros atributos.
A tag <b  id="boldest"> possui um atributo “id” que possui o valor “boldest”.
Você pode
acessar um atributo de uma tag tratando-a como um dicionário:  tag [ 'id' ]  # u'boldest'    Você pode acessar este dicionário diretamente através de .attrs :  tag .
attrs  # {u'id': 'boldest'}    Você pode adicionar, remover ou modificar os atributos de uma tag.
Novamente, isso pode
ser feito tratando a tag como um dicionário:  tag [ 'id' ]  =  'verybold'  tag [ 'another-attribute' ]  =  1  tag  # <b another-attribute="1" id="verybold"></b>  del  tag [ 'id' ]  del  tag [ 'another-attribute' ]  tag  # <b></b>  tag [ 'id' ]  # KeyError: 'id'  print ( tag .
get ( 'id' ))  # None     Atributos com múltiplos valores ¶  O HTML 4 define alguns atributos que podem ter múltiplos valores.
O HTML 5
removeu alguns deles, mas definiu alguns novos.
O atributo mais comum
que pode receber múltiplos valores é o class (ou seja, a tag pode ter mais de uma classe CSS).
Outros são rel , rev , accept-charset , headers , e accesskey .
O Beautiful Soup apresenta o(s) valor(es) de um atributo deste tipo como uma lista:  css_soup  =  BeautifulSoup ( '<p class="body"></p>' )  css_soup .
p [ 'class' ]  # ["body"]  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' )  css_soup .
p [ 'class' ]  # ["body", "strikeout"]    Se um atributo possui mais de um valor, mas não é um atributo
que aceita múltiplos valores conforme definido por qualquer versão do
padrão HTML, o Beautiful Soup retornará como um valor único:  id_soup  =  BeautifulSoup ( '<p id="my id"></p>' )  id_soup .
p [ 'class' ]  # u'body strikeout'    Você pode utilizar `get_attribute_list para retornar um valor no formato de lista, seja um atributo de
múltiplos valores ou não:  id_soup .
get_attribute_list ( 'id' )  # ["my id"]    Se você analisar um documento como XML, nenhum atributo será tratado como de múltiplos valores:  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' )  xml_soup .
p [ 'class' ]  # u'body strikeout'    Novamente, você pode configurar isto usando o argumento multi_valued_attributes :  class_is_multi =  {  '*'  :  'class' }  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' ,  multi_valued_attributes = class_is_multi )  xml_soup .
p [ 'class' ]  # [u'body', u'strikeout']    Você provavelmente não precisará fazer isso, mas se fizer, use os padrões como guia.
Eles implementam as regras descritas na especificação do HTML:  from  bs4.builder  import  builder_registry  builder_registry .
DEFAULT_CDATA_LIST_ATTRIBUTES        NavigableString ¶  Uma string corresponde a um texto dentro de uma tag.
O Beautiful Soup usa a classe NavigableString para armazenar este texto:  tag .
string )  # <class 'bs4.element.NavigableString'>    Uma NavigableString é como uma string Unicode do Python, exceto
que ela também suporta algumas características descritas em Navegando pela árvore e Buscando na árvore .
Você pode converter um NavigableString em uma string Unicode utilizando unicode() :  unicode_string  =  unicode ( tag .
string )  unicode_string  # u'Extremely bold'  type ( unicode_string )  # <type 'unicode'>    Você não pode editar uma string “in place”, mas você pode substituir
uma string por outra usando replace_with() :  tag .
replace_with ( "No longer bold" )  tag  # <blockquote>No longer bold</blockquote>    NavigableString suporta a maior parte das características descritas em Navegando pela árvore e Buscando na árvore , mas não todas elas.
Em particular, desde que uma string não pode conter de tudo (da maneira que
uma tag pode conter uma string ou outra tag), as strings não suportam os
atributos .contents ou .string ou o método find() .
Se você quer utilizar uma NavigableString fora do Beautiful Soup,
você deve chamar o unicode() para transformá-la em uma string Unicode Python
padrão.
Se você não fizer isso, sua string irá carregar uma referência de toda sua
árvore Beautiful Soup, mesmo que você já não esteja mais usando ela, o que é um grande
desperdício de memória.
BeautifulSoup ¶  O objeto BeautifulSoup em si representa o documento como um todo.
Para maioria dos propósitos, você pode tratá-lo como um objeto Tag .
Isso significa que irá suportar a maioria dos métodos descritos em Navegando pela árvore e Buscando na árvore .
Sabendo que o objeto BeautifulSoup não corresponde a uma tag
HTML ou XML propriamente dita, ele não tem nome e nem atributos.
Mas em alguns
casos é útil observar seu .name ; então, foi dado o especial .name “[document]”:  soup .
name  # u'[document]'      Comentários e outras strings especiais ¶  Tag , NavigableString , e BeautifulSoup abrangem quase
tudo o que você encontrará em um arquivo HTML ou XML, mas há alguns
pontos faltando.
O único deles que você provavelmente precisará se preocupar
é o comentário:  markup  =  "<b><!--Hey, buddy.
string  type ( comment )  # <class 'bs4.element.Comment'>    O objeto Comment é apenas um tipo especial de NavigableString :  comment  # u'Hey, buddy.
Want to buy a used parser'    Mas quando aparece como parte de um documento HTML, um Comment é
exibido com uma formatação especial:  print ( soup .
Want to buy a used parser?-->  # </b>    O Beautiful Soup define classes para qualquer outra coisa que possa
aparecer em um documento XML: CData , ProcessingInstruction , Declaration e Doctype .
Assim como Comment , estas classes
são subclasses de NavigableString que adicionam algo a string.
Aqui está um exemplo que substitui o comentário por um bloco CDATA:  from  bs4  import  CData  cdata  =  CData ( "A CDATA block" )  comment .
[CDATA[A CDATA block]]>  # </b>       Navegando pela árvore ¶  Aqui está o documento HTML “Three sisters” novamente:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    Eu usarei este documento como exemplo para mostrar como navegar
de uma parte para outra do documento.
Descendo na Árvore ¶  As tags podem conter strings e outras tags.
Estes elementos são as tags filhas (children).
O Beautiful Soup oferece diferentes atributos para
navegar e iterar sobre as tags filhas.
Note que as strings Beautiful Soup não suportam qualquer destes atributos,
porque uma string não pode ter filhos.
Navegar usando os nomes das tags ¶  A maneira mais simples de navegar pela árvore é utilizar
o nome da tag que você quer.
Se você quer a tag <head>,
simplesmente use soup.head :  soup .
title  # <title>The Dormouse's story</title>    Você pode usar este truque de novo, e de novo, para focar em certa parte da
árvore de análise.
Este código retorna a primeira tag <b> abaixo da tag <body>:  soup .
b  # <b>The Dormouse's story</b>    Utilizando o nome da tag como atributo irá lhe retornar apenas a primeira tag com aquele nome:  soup .
a  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    Se você precisar retornar todas as tags <a>, ou algo mais complicado
que a primeira tag com um certo nome, você precisará utilizar um dos
métodos descritos em Buscando na árvore , como find_all() :  soup .
find_all ( 'a' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      .contents e .children ¶  As tags filhas de uma outra tag estão disponíveis em uma lista chamada por .contents :  head_tag  =  soup .
contents  # [u'The Dormouse's story']    O objeto BeautifulSoup em si possui filhos.
Neste caso, a tag
<html> é a filha do objeto BeautifulSoup .
name  # u'html'    Uma string não possui o atributo .contents , porque ela não pode conter
nada:  text  =  title_tag .
contents  # AttributeError: 'NavigableString' object has no attribute 'contents'    Ao invés de retorná-las como uma lista, você pode iterar sobre as
tag’s filhas usando o gerador .children :  for  child  in  title_tag .
children :  print ( child )  # The Dormouse's story      .descendants ¶  Os atributos .contents e .children somente consideram tags que
são filhas diretas .
Por instância, a tag <head> tem apenas uma tag filha direta,
a tag <title>:  head_tag .
contents  # [<title>The Dormouse's story</title>]    Mas a tag <title> em si possui uma filha: a string “The Dormouse’s story”.
Existe uma percepção de que esta string também é filha da tag <head>.
O atributo .descendants permite que você itere sobre todas as tags filhas, recursivamente: suas filhas diretas, as filhas de suas filhas, e assim por diante:  for  child  in  head_tag .
descendants :  print ( child )  # <title>The Dormouse's story</title>  # The Dormouse's story    A tag <head> possui apenas uma filha, mas também possui dois descentendes :
a tag <title> e a filha da tag <title>.
O objeto BeautifulSoup possui apenas
uma filha direta (a tag <html>), mas ele possui vários descendentes:  len ( list ( soup .
descendants ))  # 25      .string ¶  Se uma tag possui apenas uma filha, e esta filha é uma NavigableString ,
esta filha pode ser disponibilizada através de .string :  title_tag .
string  # u'The Dormouse's story'    Se a filha única de uma tag é outra tag e esta tag possui uma .string , então considera-se que a tag mãe tenha a mesma .string como sua filha:  head_tag .
string  # u'The Dormouse's story'    Se uma tag contém mais de uma coisa, então não fica claro a que .string deve se referir, portanto .string será definida como None :  print ( soup .
string )  # None      .strings e stripped_strings ¶  Se existe mais de alguma coisa dentro da tag, você pode continuar
olhando apenas as strings.
Use o gerador .strings :  for  string  in  soup .
# u'\n\n'  # u'...'  # u'\n'    Estas strings tendem a ter muitos espaços em branco, os quais você
pode remover utilizando o gerador .stripped_strings como alternativa:  for  string  in  soup .
# u'...'    Aqui, strings formadas inteiramente por espaços em branco serão ignoradas,
e espaços em branco no início e no fim das strings serão removidos.
Subindo na Árvore ¶  Continuando a analogia da árvore como “família”, toda tag e toda string possuem tags mães (parents) : a tag que as contém.
.parent ¶  Você pode acessar o elemento mãe com o atributo .parent .
No
exemplo “three sisters”, a tag <head> é mãe da tag <title>:  title_tag  =  soup .
parent  # <head><title>The Dormouse's story</title></head>    A string de title tem uma mãe: a tag <title> que a contém:  title_tag .
parent  # <title>The Dormouse's story</title>    A tag mãe de todo documento (<html>) é um objeto BeautifulSoup em si:  html_tag  =  soup .
parent )  # <class 'bs4.BeautifulSoup'>    E o .parent de um objeto BeautifulSoup é definido como None:  print ( soup .
parent )  # None      .parents ¶  Você pode iterar sobre todos os elementos pais com .parents .
Este exemplo usa .parents para viajar de uma tag <a>
profunda no documento, para o elemento mais ao topo da árvore do documento:  link  =  soup .
name )  # p  # body  # html  # [document]  # None       Navegando para os lados: ¶  Considere um simples documento como este:  sibling_soup  =  BeautifulSoup ( "<a><b>text1</b><c>text2</c></b></a>" )  print ( sibling_soup .
prettify ())  # <html>  #  <body>  #   <a>  #    <b>  #     text1  #    </b>  #    <c>  #     text2  #    </c>  #   </a>  #  </body>  # </html>    A tag <b> e a tag <c> estão no mesmo nível: ambas são filhas diretas
da mesma tag.
Nós podemos chamá-las irmãs ( siblings ).
Quando um documento é pretty-printed, irmãs aparecem no mesmo nível de identação.
Você pode utilizar esta relação nos códigos que você escrever.
.next_sibling e .previous_sibling ¶  Você pode usar .next_sibling e .previous_sibling para navegar
entre os elementos da página que estão no mesmo nível da árvore:  sibling_soup .
previous_sibling  # <b>text1</b>    A tag <b> possui .next_sibling , mas não .previous_sibling ,
porque não há nada antes da tag <b> no mesmo nível na árvore .
Pela mesma razão, a tag <c> possui .previous_sibling mas não .next_sibling :  print ( sibling_soup .
next_sibling )  # None    As strings “text1” e “text2” não são irmãs, porque elas não tem a mesma tag mãe:  sibling_soup .
next_sibling )  # None    No mundo real, .next_sibling ou .previous_sibling de uma tag
geralmente são strings contendo espaços em branco.
Voltando ao documento
“three sisters”:  < a  href = "http://example.com/elsie"  class = "sister"  id = "link1" > Elsie </ a >  < a  href = "http://example.com/lacie"  class = "sister"  id = "link2" > Lacie </ a >  < a  href = "http://example.com/tillie"  class = "sister"  id = "link3" > Tillie </ a >    Você pode pensar que o .next_sibling da primeira tag <a> será a segunda tag <a>.
Mas na verdade é uma string: a vírgula e um caracter de nova linha (n) que separam
a primeira da segunda tag <a>:  link  =  soup .
next_sibling  # u',\n'    A segunda tag <a> é, na verdade, a .next_sibling da vírgula:  link .
next_sibling  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>      .next_siblings e .previous_siblings ¶  Você pode iterar sobre as tag’s filhas com .next_siblings ou .previous_siblings :  for  sibling  in  soup .
previous_siblings :  print ( repr ( sibling ))  # ' and\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # u',\n'  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  # u'Once upon a time there were three little sisters; and their names were\n'  # None       Indo e voltando ¶  Dê uma olhada no início do documento “three sisters”:  < html >< head >< title > The  Dormouse 's story</title></head>  < p  class = "title" >< b > The  Dormouse 's story</b></p>    Um parser HTML transforma estas strings em uma série de eventos: “abrir
uma tag <html>”, “abrir uma tag <head>”, “abrir uma tag <title>”,
“adicionar uma string”, “fechar uma tag <title>,
“abrir uma tag <p>”, e daí por diante.
O Beautiful Soup oferece ferramentas
para reconstruir a análise inicial do documento.
.next_element e .previous_element ¶  O atributo .next_element de uma string ou tag aponta para
qualquer coisa que tenha sido interpretado posteriormente.
Isso deveria ser o mesmo que .next_sibling , mas é
drasticamente diferente.
Aqui está a tag <a> final no “three sisters”.
Sua .next_sibling é uma string: a conclusão da sentença
que foi interrompida pelo início da tag <a>.
:  last_a_tag  =  soup .
Mas no .next_element da tag <a>, o que é analisado imediatamente
depois da tag <a> não é o resto da sentença: é a palavra “Tillie”.
last_a_tag.next_element
# u’Tillie’   Isso porque na marcação original, a palavra “Tillie” apareceu
antes do ponto e virgula.
O parser encontrou uma tag <a>, então
a palavra “Tillie”, então fechando a tag </a>, então o ponto e vírgula e o
resto da sentença.
O ponto e vírgula estão no mesmo nível que a tag <a>,
mas a palavra “Tillie” foi encontrada primeiro.
O atributo .previous_element é exatamente o oposto de .next_element .
Ele aponta para qualquer elemento que
seja analisado antes do respectivo:  last_a_tag .
next_element  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>      .next_elements e .previous_elements ¶  Você deve ter entendido a idéia agora.
Você pode usar estes iteradores
para andar para frente e para atrás no documento quando ele for analisado:  for  element  in  last_a_tag .
# u'\n\n'  # <p class="story">...</p>  # u'...'  # u'\n'  # None        Buscando na árvore ¶  O Beautiful Soup define vários métodos para buscar na árvore que está sendo analisada,
mas eles são todos muito similares.
Vou usar a maior parte do tempo para explicar os dois mais
populares métodos: find() e find_all() .
Os outros métodos recebem exatamente
os mesmos argumentos, portanto, vou cobrí-los apenas brevemente.
Mais uma vez, utilizarei o documento “three sisters” como exemplo:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    Utilizando em um filtro um argumento como find_all() , você pode
“dar um zoom” nas partes do documento que você está interessado.
Tipos de filtros ¶  Antes de entrar em detalhes sobre o find_all() e métodos similares,
quero mostrar exemplos de diferentes filtros que você pode passar dentro
destes métodos.
Estes filtros aparecerão de novo e de novo por toda API
de pesquisa.
Você pode usá-los para realizar filtros baseados nos nomes das tags,
nos seus atributos, no texto de uma strings ou em alguma combinação entre eles.
Uma string ¶  O filtro mais simples é uma string.
Passando uma string para um método de pesquisa,
o Beautiful Soup irá buscar uma correspondência a esta exata string.
O seguinte código
encontrará todas as tags <b> no documento:  soup .
find_all ( 'b' )  # [<b>The Dormouse's story</b>]    Se você passar uma byte string, o Beautiful Soup assumirá que a string
esta codificada como UTF-8.
Você pode evitar isso passando ao invés disso
uma string Unicode.
Uma expressão regular (regex) ¶  Se você passar um objeto regex , o Beautiful Soup irá
realizar um filtro com ela utilizando seu método search() .
O código seguinte buscará todas as tags as quais os nomes comecem com
a letra “b”; neste caso, a tag <body> e a tag <b>:  import  re  for  tag  in  soup .
name )  # body  # b    Este código buscará todas as tags cujo nome contenha a letra “t”:  for  tag  in  soup .
name )  # html  # title      Uma lista ¶  Se você passar uma lista, o Beautiful Soup irá buscar
uma correspondência com qualquer item dessuma lista.
O código seguinte buscará todas as tags <a> e todas
as tags <b>:  soup .
find_all ([ "a" ,  "b" ])  # [<b>The Dormouse's story</b>,  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      True ¶  O valor True irá corresponder com tudo.
O código abaixo encontrará todas as tags do documento,
mas nenhuma das strings:  for  tag  in  soup .
name )  # html  # head  # title  # body  # p  # b  # p  # a  # a  # a  # p      Uma função ¶  Se nenhuma das opções anteriores funcionar para você, defina uma
função que pegará um elemento como seu único argumento.
A função
deverá retornar True se o argumento corresponder e False caso contrário.
Aqui você tem uma função que irá retornar True se uma tag definir
o atributo class , mas não definir o atributo id :  def  has_class_but_no_id ( tag ):  return  tag .
has_attr ( 'id' )    Passe esta função dentro de find_all() e você irá retornar todas
as tags <p>:  soup .
find_all ( has_class_but_no_id )  # [<p class="title"><b>The Dormouse's story</b></p>,  #  <p class="story">Once upon a time there were...</p>,  #  <p class="story">...</p>]    Esta função irá encontrar apenas as tags <p>.
Não irá encontrar as tags <a>,
porque elas definem “class e “id” ao mesmo tempo.
Ela não encontrará
as tags <html> e <title>, porque estas tags não definem um atributo
“class”.
Se você passar uma função para filtrar um atributo específico como href , o argumento passado na função será o nome do atributo e
não toda a tag.
Aqui vemos uma função que encontra todas as tags <a>
em que o atributo href não corresponde a expressão regular passada:  def  not_lacie ( href ):  return  href  and  not  re .
find_all ( href = not_lacie )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    A função pode ser tão complexa quanto você precise que seja.
Aqui temos uma função que retorna True se uma tag esta
cercada por objetos string:  from  bs4  import  NavigableString  def  surrounded_by_strings ( tag ):  return  ( isinstance ( tag .
name  # p  # a  # a  # a  # p    Agora nós estamos prontos para olhar os métodos de busca em detalhes.
find_all() ¶  Definição: find_all( name , attrs , recursive , string , limit , **kwargs )  O método find_all() busca entre os decendentes de uma tag e retorna todos os decendentes
que correspondem a seus filtros.
Dei diversos exemplos em Tipos de filtros ,
mas aqui estão mais alguns:  soup .
compile ( "sisters" ))  # u'Once upon a time there were three little sisters; and their names were\n'    Alguns podem parecer familiares, mas outros são novos.
O que significa passar um valor string ou id ?
Por que find_all("p",  "title") encontra uma tag <p> com a classe CSS “title”?
Vamos dar uma olhada nos argumentos de find_all() .
O argumento name ¶  Passe um valor para name e você dirá para o Beautiful Soup
considerar apenas as tags com certos nomes.
Strings de texto seão ignoradas,
assim como os nomes que não corresponderem ao argumento name  Este é o uso mais simples:  soup .
find_all ( "title" )  # [<title>The Dormouse's story</title>]    Lembre-se de Tipos de filtros que o valor para name pode ser uma
string , uma expressão regular , uma lista , uma função , ou o valor True .
Os argumentos “palavras-chave” ¶  Qualquer argumento que não for reconhecido se tornará um filtro
de atributos da tag.
Se você passar um valor para um argumento
chamado id , o Beautiful Soup irá buscar correspondentes entre
todas tags id :  soup .
find_all ( id = 'link2' )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Se você passar um valor para href , o Beautiful Soup buscar correspondentes
em cada tag que possua o atributo href :  soup .
compile ( "elsie" ))  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    Você pode filtrar um atributo baseado em uma string , uma expressão regular , uma lista , uma função , ou o valor True .
Este código encontra todas as tags em que o atributo id possuem um valor, independente de qual valor seja:  soup .
find_all ( id = True )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Você pode filtrar múltiplos atributos de uma vez passando mais de um argumento
palavra-chave:  soup .
compile ( "elsie" ),  id = 'link1' )  # [<a class="sister" href="http://example.com/elsie" id="link1">three</a>]    Alguns atributos, como o atributo data-* do HTML5, possuem nomes que não
podem ser usados como argumentos palavra-chave::  data_soup  =  BeautifulSoup ( '<div data-foo="value">foo!</div>' )  data_soup .
find_all ( data - foo = "value" )  # SyntaxError: keyword can't be an expression    Você pode usar estes atributos para realizar buscas, colocando-os
em um dicionário e passando o dicionário em find_all() , como o argumento attrs :  data_soup .
find_all ( attrs = { "data-foo" :  "value" })  # [<div data-foo="value">foo!</div>]    Você não pode utilizar um argumento palavra-chave para buscar pelo elemento
HTML “name”, porque o Beautiful Soup utiliza o argumento name para
conter o nome da própria tag.
Ao invés disso, você pode passar o valor para
“name” no argumento attrs :  name_soup  =  BeautifulSoup ( '<input name="email"/>' )  name_soup .
find_all ( attrs = { "name" :  "email" })  # [<input name="email"/>]      Buscando por uma classe CSS ¶  É muito útil buscar por uma tag que tem uma certa classe CSS, mas
o nome do atributo CSS, “class”, é uma palavra reservada no Python.
Utilizar class como um argumento palavra-chave lhe trará um erro
de sintaxe.
A partir do Beautiful Soup 4.1.2, você pode buscar por uma
classe CSS utilizando o argumento palavra-chave class_ :  soup .
find_all ( "a" ,  class_ = "sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Assim como qualquer argumento palavra-chave, você pode passar para class_ uma string, uma expressão regular (regex), uma função ou True :  soup .
find_all ( class_ = has_six_characters )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Lembre-se que uma tag pode ter valores múltiplos
para seu atributo classe.
Quando você buscar por uma tag que tenha
uma certa classe CSS, você esta buscando correspodência em qualquer de suas classes CSS:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' )  css_soup .
find_all ( "p" ,  class_ = "body" )  # [<p class="body strikeout"></p>]    Você pode também buscar por uma string exata como valor de class :  css_soup .
find_all ( "p" ,  class_ = "body strikeout" )  # [<p class="body strikeout"></p>]    Mas ao procurar por variações de uma string, isso não irá funcionar:  css_soup .
find_all ( "p" ,  class_ = "strikeout body" )  # []    Se voce quiser buscar por tags que correspondem a duas ou mais classes CSS,
você deverá utilizar um seletor CSS:  css_soup .
select ( "p.strikeout.body" )  # [<p class="body strikeout"></p>]    Em versões mais antigas do Beautiful Soup, as quais não possuem o atalho class_ você pode utilizar o truque attrs conforme mencionado acima.
Será criado um dicionário
do qual o valor para “class” seja uma string ( ou uma expressão regular, ou qualquer
outra coisa) que você queira procurar:  soup .
find_all ( "a" ,  attrs = { "class" :  "sister" })  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      O argumento string ¶  Com string você pode buscar por strings ao invés de tags.
Assim como name e os argumentos palavras-chave, você pode passar uma string , uma
expressão regular , uma lista , uma função , ou o valor True .
Aqui estão alguns exemplos:  soup .
compile ( "Dormouse" ))  [ u "The Dormouse's story" ,  u "The Dormouse's story" ]  def  is_the_only_string_within_a_tag ( s ):   """Return True if this string is the only child of its parent tag."""
find_all ( string = is_the_only_string_within_a_tag )  # [u"The Dormouse's story", u"The Dormouse's story", u'Elsie', u'Lacie', u'Tillie', u'...']    Mesmo que string seja para encontrar strings, você pode combiná-lo com argumentos
para encontrar tags: o Beautiful Soup encontrará todas as tags as quais .string corresponder seu valor em string .
O código seguinte encontra
a tag <a>, a qual a .string é “Elsie”:  soup .
find_all ( "a" ,  string = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]    O argumento string é novo no Beautiful Soup 4.4.0.
Em versões anteriores
ele era chamado de text :  soup .
find_all ( "a" ,  text = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]      O argumento limit ¶  find_all() retorna todas as tags e strings que correspondem aos seus
filtros.
Isso pode levar algum tmepo se o documento for extenso.
Se você
não precisar de todos os resultados, você pode passar um número limite
( limit ).
Ele funciona assim como o parâmetro LIMIT utilizado em SQL.
Ele diz ao Beautiful Soup para parar de adquirir resultados assim que atingir
um certo número.
Existem três links no documento “three sisters”, mas este código encontra somente
os dois primeiros:  soup .
find_all ( "a" ,  limit = 2 )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]      O argumento recursive ¶  Se você chamar mytag.find_all() , o Beautiful Soup irá examinar todos os descendentes
de mytag : suas filhas, as filhas de suas filhas e daí em diante.
Se você quer apenas que
o Beautiful Soup considere filhas diretas, você pode passar o parâmetro recursive=False .
Veja a diferença aqui:  soup .
find_all ( "title" ,  recursive = False )  # []    Aqui está o trecho do documento:  < html >  < head >  < title >  The  Dormouse 's story  </ title >  </ head >  ...
   O tag <title> esta abaixo da tag <html>, mas não está diretamente abaixo de <html>: a tag <head> está no caminho entre elas.
O Beautiful Soup encontra a tag
<title> quando é autorizado a olhar todos os descendentes de <html>, mas
quando recursive=False é restringido o acesso as filhas imediatas de <html>.
O Beautiful Soup oferece diversos métodos de busca na árvore (como vimos acima), e a maioria
deles recebe os mesmos argumentos que find_all() : name , attrs , string , limit , e os argumentos palavras-chave.
Mas o
argumento recursive é diferente: find_all() e find() são
os únicos métodos que o suportam.
Passar recursive=False em um método
como find_parents() não seria muito útil.
Chamar uma tag é como chamar find_all() ¶  Por find_all() ser o método mais popular na API de busca do
Beautiful Soup, você pode usar um atalho para ele.
Se você tratar
o objeto BeautifulSoup ou um objeto Tag como se fosse uma
função, então é o mesmo que chamar find_all() para aquele objeto.
Estas duas linhas de código são equivalentes:  soup .
find_all ( "a" )  soup ( "a" )    Estas duas linhas também são equivalentes:  soup .
title ( string = True )      find() ¶  Signature: find( name , attrs , recursive , string , **kwargs )  O método find_all() varre todo o documento em busca de resultados,
mas algumas vezes você irá querer apenas um resultado.
Se você sabe que
o documento possui apenas uma tag <body>, é perda de tempo varrer todo o
o documento procurando por outras.
Ao invés de passar limit=1 toda vez em que chamar find_all , você pode usar o método find() .
Estas duas linhas de código são quase equivalentes:  soup .
find ( 'title' )  # <title>The Dormouse's story</title>    A única diferença é que find_all() retorna uma lista contendo apenas
um resuldado, enquanto find() retorna o resultado.
Se find_all() não encontrar nada, ele retornará uma lista vazia.
Se find() não encontrar nada, ele retornará None :  print ( soup .
find ( "nosuchtag" ))  # None    Lembre-se do truque soup.head.title de Navegar usando os nomes das tags ?
Aquele truque funciona chamando repetidamente find() :  soup .
find ( "title" )  # <title>The Dormouse's story</title>      find_parents() e find_parent() ¶  Signature: find_parents( name , attrs , string , limit , **kwargs )  Signature: find_parent( name , attrs , string , **kwargs )  Levei muito tempo cobrindo find_all() e find() acima.
O API do Beautiful Soup define dez outros métodos
para buscas na árvore, mas não tenha medo!
Cinco destes métodos são
basicamente o mesmo que find_all() , e os outros cinco são basicamente
o mesmo que find() .
A única diferença está em qual parte da árvore
eles procuram.
Primeiro vamos considerar find_parents() e find_parent() .
Lembre-se que find_all() e find() trabalham
de sua própria maneira descendo através da árvore, procurando pelos
descendentes de uma tag.
Estes métodos fazem o contrário: eles trabalham subindo a árvore, procurando pelas mães de uma tag (ou string).
Vamos experimentá-los: começando por uma string “enterrada” no documento
“three daughters”:  a_string  =  soup .
find_parents ( "p" ,  class_ = "title" )  # []    Uma das três tags <a> é diretamente um nível superior da string em
questão, então nossa busca a encontra.
Uma das três tags <p> é uma mãe
indireta da string e nossa busca também a encontra.
Há uma tag <p> com
a classe CSS “title” em algum lugar no documento, mas não é nenhuma das tags mães
da string, portanto, não podemos encontrá-la com find_parents() .
Você já deve ter feito a conexão entre find_parent() e find_parents() , e os atributos .parent e .parents mencionados
anteriormente.
A conexão é muito forte.
Estes métodos de busca utilizam .parents para iterar sobre todos as mãesS e compara cada um com o filtro passado
para verificar se preenche o requisito.
find_next_siblings() e find_next_sibling() ¶  Signature: find_next_siblings( name , attrs , string , limit , **kwargs )  Signature: find_next_sibling( name , attrs , string , **kwargs )  Estes métodos utilizam .next_siblings para
iterar sobre o resto dos filhos de um elemento da árvore.
O método find_next_siblings() retornará todos os filhos que atendem o
requisito find_next_sibling() retorna apenas o primeiro:  first_link  =  soup .
find_next_sibling ( "p" )  # <p class="story">...</p>      find_previous_siblings() e find_previous_sibling() ¶  Signature: find_previous_siblings( name , attrs , string , limit , **kwargs )  Signature: find_previous_sibling( name , attrs , string , **kwargs )  Estes métodos utilizam .previous_siblings para iterar sobre os filhos de um elemento que
o precede na árvore.
O método find_previous_siblings() retorna todos os filhos que atendem o requisito e find_previous_sibling() retorna apenas o primeiro:  last_link  =  soup .
find_previous_sibling ( "p" )  # <p class="title"><b>The Dormouse's story</b></p>      find_all_next() e find_next() ¶  Signature: find_all_next( name , attrs , string , limit , **kwargs )  Signature: find_next( name , attrs , string , **kwargs )  Estes métodos utilizam .next_elements para
iterar sobre qualquer tag e string que aparecer depois da atual no documento.
O método find_all_next() retorna todos os casos que atendem, e find_next() retorna somente o primeiro caso:  first_link  =  soup .
find_next ( "p" )  # <p class="story">...</p>    No primeiro exemplo, a string “Elsie” foi encontrada, mesmo estando
dentro da tag <a>.
No segundo exemplo, a última tag <p> do documento foi
encontrada, mesmo que não esteja na mesma parte da árvore que <a> onde começamos.
Para estes métodos, o que importa é que um elemento corresponda ao filtro e esteja
depois do elemento de início no documento.
find_all_previous() e find_previous() ¶  Signature: find_all_previous( name , attrs , string , limit , **kwargs )  Signature: find_previous( name , attrs , string , **kwargs )  Estes métodos utilizam .previous_elements para
iterar sobre  as tags e strings que aparecem antes do elemento indicado no argumento.
O método find_all_previous() retorna todos que correspondem a busca e o método find_previous() apenas a primeira correspondência:  first_link  =  soup .
find_previous ( "title" )  # <title>The Dormouse's story</title>    Quando se chama find_all_previous("p") é encontrado não só o
primeiro parágrafo do documento (o que possui class=”title”), mas também o
segundo parágrafo, a tag <p> que contém a tag <a> por onde começamos.
Isso não deveria ser tão surpreendente: nós estamos olhando para todas as tags
que apareceram anteriormente no documento incluindo aquela onde começamos.
Uma
tag <p> que contenha uma tag <a> deve aparecer antes da tag <a> que ela contém.
Seletores CSS ¶  A partir da versão 4.7.0, o Beautiful Soup suporta a maior parte dos seletores CSS4
através do projeto SoupSieve .
Se você
instalou o Beautiful Soup através do pip ,o SoupSieve foi instalado ao mesmo tempo,
portanto você não precisará realizar nenhuma etapa adicional.
BeautifulSoup possui um método .select() o qual utiliza o SoupSieve para
executar um seletor CSS selector sobre um documento a ser analisado e retorna todos os
elementos correspondentes.
Tag possui um método similar que executa um seletor CSS
sobre o conteúdo de uma única tag.
(Versões anteriores do Beautiful Soup também possuem o método .select() , mas somente os seletores CSS mais populares são suportados.
A documentação SoupSieve
lista todos os seletores suportados atualmente, mas aqui estão alguns dos
básicos:  Você pode encontrar tags:  soup .
select ( "p:nth-of-type(3)" )  # [<p class="story">...</p>]     Encontrar tags aninhadas com outras:: soup.select(“body a”)
# [<a class=”sister” href=” http://example.com/elsie ” id=”link1”>Elsie</a>,
#  <a class=”sister” href=” http://example.com/lacie ”  id=”link2”>Lacie</a>,
#  <a class=”sister” href=” http://example.com/tillie ” id=”link3”>Tillie</a>]  soup.select(“html head title”)
# [<title>The Dormouse’s story</title>]    Encontrar tags diretamente abaixo de outras tags no aninhamento:  soup .
select ( "body > a" )  # []    Encontrar as irmãs de alguma tag:  soup .
select ( "#link1 + .sister" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Encontrar tags pela classe CSS:  soup .
select ( "[class~=sister]" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Encontrar tags pelo ID:  soup .
select ( "a#link2" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Encontrar tags que se relacionam com qualquer seletor em uma lista de seletores:  soup .
select ( "#link1,#link2" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Testar a existência de um atributo:  soup .
select ( 'a[href]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Encontrar tags pelo valor do atributo:  soup .
select ( 'a[href*=".com/el"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    Há outro método chamado select_one() , o qual encontra somente
a primeira tag que combina com um seletor:  soup .
select_one ( ".sister" )  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    Se você analisou um XML que define namespaces, você pode
utilizar nos seletores CSS:  from  bs4  import  BeautifulSoup  xml  =  """<tag xmlns:ns1="http://namespace1/" xmlns:ns2="http://namespace2/">  <ns1:child>I'm in namespace 1</ns1:child>  <ns2:child>I'm in namespace 2</ns2:child>  </tag> """  soup  =  BeautifulSoup ( xml ,  "xml" )  soup .
select ( "child" )  # [<ns1:child>I'm in namespace 1</ns1:child>, <ns2:child>I'm in namespace 2</ns2:child>]  soup .
select ( "ns1|child" ,  namespaces = namespaces )  # [<ns1:child>I'm in namespace 1</ns1:child>]    Quando manipulando um seletor CSS que utiliza
namespaces,o Beautiful Soup utiliza a abreviação do namespace
que encontrou quando estava analisando o documento.
Você pode evitar isso
passando um dicionário com suas próprias abreviações:  namespaces  =  dict ( first = "http://namespace1/" ,  second = "http://namespace2/" )  soup .
select ( "second|child" ,  namespaces = namespaces )  # [<ns1:child>I'm in namespace 2</ns1:child>]    Todo este negócio de seletor CSS é conveniente
para pessoas que já sabem a sintaxe do seletor CSS.
Você pode fazer tudo isso com a API do BeautifulSoup.
E se os seletores CSS são tudo o que você precisa,
você deveria analisar o documento com lxml: é mais rápido.
Mas isso deixa você combinar seletores CSS com a API do Beautiful Soup.
Modificando a árvore ¶  O principal poder do Beautiful Soup está na busca pela árvore, mas você
pode também modificar a árvore e escrever suas modificações como um novo
documento HTML ou XML.
Alterando nomes de tags e atributos ¶  Cobri este assunto anteriormente em Atributos , mas vale a pena repetir.
Você
pode renomear uma tag, alterar o valor de algum de seus atributos, adicionar novos
atributos e deletar qualquer um deles:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' )  tag  =  soup .
name  =  "blockquote"  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>      Modificando .string ¶  Se você definir o um atributo .string de uma tag, o conteúdo da
tag será substituido pela string que foi passada:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
tag  # <a href="http://example.com/">New link text.</a>    Cuidado: se a tag conter outra(s) tag(s), ela(s) e todo seu conteúdo
serão destruídos.
append() ¶  Você pode adicionar algo no conteúdo de uma tag com Tag.append() .
Funciona
da mesma maneira que .append() de uma lista:  soup  =  BeautifulSoup ( "<a>Foo</a>" )  soup .
contents  # [u'Foo', u'Bar']      extend() ¶  Com início no Beautiful Soup 4.7.0, Tag também suporta um método chamado .extend() , o qual funciona da mesma maneira que chamando .extend() em
uma lista:  soup  =  BeautifulSoup ( "<a>Soup</a>" )  soup .
extend ([ "'s" ,  " " ,  "on" ])  soup  # <html><head></head><body><a>Soup's on</a></body></html>  soup .
contents  # [u'Soup', u''s', u' ', u'on']      NavigableString() e .new_tag() ¶  Se você precisar adicionar uma string a um documento, sem problema – você
pode passar uma string Python através de append() , ou você pode chamar
o construtor NavigableString :  soup  =  BeautifulSoup ( "<b></b>" )  tag  =  soup .
contents  # [u'Hello', u' there']    Se você quiser criar um comentário ou alguma outra subclasse de NavigableString , apenas chame o construtor:  from  bs4  import  Comment  new_comment  =  Comment ( "Nice to see you."
(Esta é uma funcionalidade nova no Beautiful Soup 4.4.0.)
E se você precisar criar uma nova tag?
A melhor solução
é chamar o método BeautifulSoup.new_tag() :  soup  =  BeautifulSoup ( "<b></b>" )  original_tag  =  soup .
original_tag  # <b><a href="http://www.example.com">Link text.</a></b>    Somente o primeiro argumento (o nome da tag) é obrigatório.
insert() ¶  Tag.insert() funciona assim como Tag.append() , exceto que o novo elemento
não será inserido ao final do .contents de sua tag mãe.
Ele será inserido em qualquer posição
numérica que você informar.
Funciona assim como .insert() em uma lista:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
contents  # [u'I linked to ', u'but did not endorse', <i>example.com</i>]      insert_before() e insert_after() ¶  O método insert_before() insere tags ou strings imediatamente antes de algo
na árvore:  soup  =  BeautifulSoup ( "<b>stop</b>" )  tag  =  soup .
b  # <b><i>Don't</i>stop</b>    O método insert_after() insere tags ou strings imediatamente após algo
na árvore:  div  =  soup .
b  # <b><i>Don't</i> you <div>ever</div> stop</b>  soup .
contents  # [<i>Don't</i>, u' you', <div>ever</div>, u'stop']      clear() ¶  O Tag.clear() remove o conteúdo de uma tag:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  tag  =  soup .
clear ()  tag  # <a href="http://example.com/"></a>      extract() ¶  O PageElement.extract() remove uma tag ou string da árvore.
Ele retorna
a tag ou string que foi extraída:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
parent )  None    Neste ponto você efetivamente tem duas árvores de análise: uma baseada no objeto BeautifulSoup que você usou para analisar o documento, e outra baseada na tag que foi
extraída.
Você pode também chamar extract em um filho do elemento que você extraiu:  my_string  =  i_tag .
parent )  # None  i_tag  # <i></i>      decompose() ¶  O Tag.decompose() remove uma tag da árvore, então destrói completamente ela
e seu conteúdo:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
decompose ()  a_tag  # <a href="http://example.com/">I linked to</a>      replace_with() ¶  Um PageElement.replace_with() remove uma tag ou string da árvore e
substitui pela tag ou string que você escolher:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
replace_with ( new_tag )  a_tag  # <a href="http://example.com/">I linked to <b>example.net</b></a>    replace_with() retorna a tag ou string que foi substituída, então você pode
examiná-la ou adicioná-la novamente em outra parte da árvore.
wrap() ¶  O PageElement.wrap() envelopa um elemento na tag que você especificar.
Ele
retornará o novo empacotador:  soup  =  BeautifulSoup ( "<p>I wish I was bold.</p>" )  soup .
new_tag ( "div" )  # <div><p><b>I wish I was bold.</b></p></div>    Este método é novo no Beautiful Soup 4.0.5.    unwrap() ¶  O Tag.unwrap() é o oposto de wrap() .
Ele substitui uma tag pelo
que estiver dentro dela.
É uma boa maneira de remover marcações:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  a_tag  =  soup .
unwrap ()  a_tag  # <a href="http://example.com/">I linked to example.com</a>    Assim como replace_with() , unwrap() retorna a tag que foi
substituída.
smooth() ¶  Após chamar vários métodos que modificam a árvore, você pode acabar com um ou dois objetos NavigableString próximos um ao outro.
O Beautiful Soup não tem nenhum problema com isso, mas como isso não pode acontecer em um documento que acabou de ser analisado, você não deve esperar um comportamento como o seguinte:  soup  =  BeautifulSoup ( "<p>A one</p>" )  soup .
contents  # [u'A one', u', a two']  print ( soup .
encode ())  # <p>A one, a two</p>  print ( soup .
prettify ())  # <p>  #  A one  #  , a two  # </p>    Você pode chamar Tag.smooth() para limpar a árvore analisada, consolidando strings adjacentes:  soup .
contents  # [u'A one, a two']  print ( soup .
prettify ())  # <p>  #  A one, a two  # </p>    O método smooth() é novo no Beautiful Soup 4.8.0.
Saída ¶   Pretty-printing ¶  O método prettify() irá transformar uma árvore do Beautiful Soup em
uma string Unicode devidamente formatada, com uma linha para cada tag e cada string:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup )  soup .
prettify ())  # <html>  #  <head>  #  </head>  #  <body>  #   <a href="http://example.com/">  #    I linked to  #    <i>  #     example.com  #    </i>  #   </a>  #  </body>  # </html>    Você pode chamar prettify() no top-level do objeto BeautifulSoup ,
ou em qualquer de seus objetos Tag :  print ( soup .
prettify ())  # <a href="http://example.com/">  #  I linked to  #  <i>  #   example.com  #  </i>  # </a>      Non-pretty printing ¶  Se você quer apenas uma string, sem nenhuma formatação, você pode chamar unicode() ou str() para o objeto BeautifulSoup ou uma Tag dentro dele:  str ( soup )  # '<html><head></head><body><a href="http://example.com/">I linked to <i>example.com</i></a></body></html>'  unicode ( soup .
a )  # u'<a href="http://example.com/">I linked to <i>example.com</i></a>'    A função str() retorna uma string codificada em UTF-8.
Veja Codificação (Encoding) para outras opções.
Você também pode chamar encode() para ter uma bytestring, e decode() para ter Unicode.
Output formatters ¶  Se você der para o Beautiful Soup um documento que contém entidades HTML como
“&lquot;”, elas serão convertidades em caracteres Unicode:  soup  =  BeautifulSoup ( "&ldquo;Dammit!&rdquo; he said."
)  unicode ( soup )  # u'<html><head></head><body>\u201cDammit!\u201d he said.</body></html>'    Se você converter o documento em uma string, os caracteres Unicode
serão codificados como UTF-8.
Você não irá ter suas entidades HTML de volta:  str ( soup )  # '<html><head></head><body>\xe2\x80\x9cDammit!\xe2\x80\x9d he said.</body></html>'    Por padrão, os únicos caracteres que escapam desta saída são o & e os sinais de <>.
Eles são convertidos em “&amp;”, “&lt;”,
e “&gt;”, com isso o Beautiful Soup não gera HTML e XML inválidos de maneira inadvertida.
soup = BeautifulSoup(“<p>The law firm of Dewey, Cheatem, & Howe</p>”)
soup.p
# <p>The law firm of Dewey, Cheatem, &amp; Howe</p>  soup = BeautifulSoup(‘<a href=” http://example.com/?foo=val1&bar=val2 ”>A link</a>’)
soup.a
# <a href=” http://example.com/?foo=val1&amp;bar=val2 ”>A link</a>   Você pode alterar este comportamento informando um valor para o argumento de formatter para prettify() , encode() , ou decode() .
Beautiful Soup reconhece cinco possiveis valores para formatter .
O padrão é formatter="minimal" .
Strings sempre serão processadas de maneira a garantir que o Beautiful Soup gere HTML/XML válidos:  french  =  "<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>"  soup  =  BeautifulSoup ( french )  print ( soup .
prettify ( formatter = "minimal" ))  # <html>  #  <body>  #   <p>  #    Il a dit &lt;&lt;Sacré bleu!&gt;&gt;  #   </p>  #  </body>  # </html>    Se você passar formatter="html" , Beautiful Soup irá converter caracteres
Unicode para entidades HTML sempre que possível:  print ( soup .
prettify ( formatter = "html" ))  # <html>  #  <body>  #   <p>  #    Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;  #   </p>  #  </body>  # </html>    Se você passar um formatter="html5" , é o mesmo que formatter="html" ,
mas o Beautiful Soup irá omitir a barra de fechamento HTML:  soup  =  BeautifulSoup ( "<br>" )  print ( soup .
encode ( formatter = "html" ))  # <html><body><br/></body></html>  print ( soup .
encode ( formatter = "html5" ))  # <html><body><br></body></html>    Se você passar formatter=None , Beautiful Soup não irá modificar
as strings na saída.
Esta é a opção mais rápida, mas permitirá que o
Beautiful Soup gere HTML/XML inválidos, como nestes exemplos:  print ( soup .
encode ( formatter = None ))  # <a href="http://example.com/?foo=val1&bar=val2">A link</a>    Se você precisar de controles mais sofisticados sobre sua saída,
você pode usar a classe Formatter do Beautiful Soup.
Aqui você pode ver um
formatter que converte strings para uppercase, quando elas ocorrem em um nó de texto
ou em um valor de algum atributo:  from  bs4.formatter  import  HTMLFormatter  def  uppercase ( str ):  return  str .
prettify ( formatter = formatter ))  # <html>  #  <body>  #   <p>  #    IL A DIT <<SACRÉ BLEU!>>  #   </p>  #  </body>  # </html>  print ( link_soup .
prettify ( formatter = formatter ))  # <a href="HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2">  #  A LINK  # </a>    Dividindo em subclasses HTMLFormatter ou XMLFormatter darão a você ainda
mais controle sobre a saída.
Por exemplo, o Beautiful Soup ordena os atributos em toda
tag por padrão:  attr_soup  =  BeautifulSoup ( b '<p z="1" m="2" a="3"></p>' )  print ( attr_soup .
encode ())  # <p a="3" m="2" z="1"></p>    Para desabilitar esta opção, você pode criar uma subclasse do método Formatter.attributes() ,
o qual controla qual atributo será usado na saída e em que ordem.
Esta
implementação também filtra o atributido chamado “m” quando ele aparece:  class  UnsortedAttributes ( HTMLFormatter ):  def  attributes ( self ,  tag ):  for  k ,  v  in  tag .
encode ( formatter = UnsortedAttributes ()))  # <p z="1" a="3"></p>    Um último conselho: se você criar um objeto CDATA , o texto dentro deste objeto
sempre estará presente exatamente como aparenta, com nenhuma formatação .
O Beautiful Soup irá chamar sua função de substituição da entidade, apenas
no caso de você ter escrito uma função personalizada que conta todas as strings
que existem no documento ou algo do tipo, mas ele irá ignorar o valor de retorno:  from  bs4.element  import  CData  soup  =  BeautifulSoup ( "<a></a>" )  soup .
[CDATA[one < three]]>  # </a>      get_text() ¶  Se você quer apenas o texto contido no documento ou em um par de tags, você
pode utilizar o método get_text() .
Ele retornará todo texto em um documento
ou dentro das tags como uma string Unicode:  markup  =  '<a href="http://example.com/"> \n I linked to <i>example.com</i> \n </a>'  soup  =  BeautifulSoup ( markup )  soup .
get_text ()  u ' \n I linked to example.com \n '  soup .
get_text ()  u 'example.com'    Você pode especificar uma string a ser usada para unir as partes do texto:  # soup.get_text("|")  u ' \n I linked to |example.com| \n '    Você pode dizer ao Beautiful Soup para excluir espaços em branco do início
e fim de cada parte de texto:  # soup.get_text("|", strip=True)  u 'I linked to|example.com'    Contudo para isso, você pode querer utilizar o gerador .stripped_strings e processar o texto você mesmo:  [ text  for  text  in  soup .
stripped_strings ]  # [u'I linked to', u'example.com']       Especificando um interpretador (parser) para uso ¶  Se você precisa analisar um pequeno HTML, você pode passá-lo no construtor do BeautifulSoup e será o suficiente.
O Beautiful Soup irá escolher um parser
para você e irá interpretar o dado.
Mas existem alguns argumentos adicionais que você
pode passar no construtor para alterar qual parser será usado.
O primeiro argumento do construtor BeautifulSoup é uma string ou uma variável contendo o
conteúdo do que você quer analisar.
O segundo argumento é como você quer interpretar aquele
conteúdo.
Se você não especificar nada, você irá utilizar o melhor analisador HTML instalado.
O Beautiful Soup classifica o lxml’s como sendo o melhor, logo em seguida o html5lib,
e então o parser nativo do Python.
Você pode substituí-lo, especificando de acordo
com as seguintes características:   O tipo de marcação que você quer analisar.
Atualmente são suportados
“html”, “xml”, and “html5”.
O nome do parser que você quer utilizar.
Atualmente são suportadas
as opções “lxml”, “html5lib”, e “html.parser” (parser nativo do Python).
A seção Instalando um interpretador (parser) compara os parsers suportados.
Se você não tem um parser apropriado instalado, o Beautiful Soup irá
ignorar sua solicitação e escolher um diferente.
Atualmente, o único parser
XML suportado é o lxml.
Se você não possui o lxml instalado, pedir um parser
XML não trará um e pedir por “lxml” não funcionará também.
Diferenças entre os interpretadores (parsers) ¶  O Beautiful Soup apresenta a mesma interface para diferentes parsers,
mas cada um é diferente.
Diferentes parsers irão criar diferentes análises da árvore
do mesmo documento.
As maiores diferenças estão entre os parsers HTML e XML.
Aqui está um pequeno documento analisado como HTML:  BeautifulSoup ( "<a><b /></a>" )  # <html><head></head><body><a><b></b></a></body></html>    Como uma tag <b /> vazia não é um HTML válido, o analisador a transforma
em um par <b></b>.
Aqui está o mesmo documento analisado como XML (partindo do princípio
que você tenha o lxml instalado).
Note que o a tag vazia <b /> é deixada sozinha,
e que é dada ao documento uma declaração XML ao invés de ser colocada dentro de uma tag <html>.
:  BeautifulSoup ( "<a><b /></a>" ,  "xml" )  # <?xml version="1.0" encoding="utf-8"?>  # <a><b/></a>    Há também diferenças entre analisadores HTML.
Se você der ao Beautiful
Soup um documento HTML perfeitamente formatado, estas diferenças não irão
importar.
Um analisador será mais rápido que outro, mas todos irão lhe
retornar uma estrutura de dados que se parece exatamente como o HTML original.
Mas se o documento não estiver perfeitamente formatado, diferentes analisadores
irão retornar diferentes resultados.
Aqui está um pequeno e inválido documento
analisado utilizando o analisador lxml HTML.
Note que a tag pendente </p> é
simplesmente ignorada:  BeautifulSoup ( "<a></p>" ,  "lxml" )  # <html><body><a></a></body></html>    Aqui está o mesmo documento analisado utilizando html5lib:  BeautifulSoup ( "<a></p>" ,  "html5lib" )  # <html><head></head><body><a><p></p></a></body></html>    Ao invés de ignorar a tag </p> pendente, o html5lib a equipara a uma tag
<p> aberta.
Este parser também adiciona uma tag <head> vazia ao documento.
Aqui está o mesmo documento analisado com o parser HTML nativo do Python:  BeautifulSoup ( "<a></p>" ,  "html.parser" )  # <a></a>    Assim como html5lib, este parser ignora a tag de fechamento </p>.
Este parser também não realiza nenhuma tentatida de criar um HTML bem
formatado adicionando uma tag <body>.
Como lxml, ele nem se importa em
adicionar uma tag <html>.
Sendo o documento “<a></p>” inválido, nenhuma dessas técnicas é a maneira
“correta” de lidar com isso.
O html5lib utiliza técnicas que são parte
do padrão HTML5, portanto vendo sendo definido como a maneira “mais correta”,
mas todas as três técnicas são legítimas.
Diferenças entre analisadores podem afetar o seu script.
Se você está
planejando distribuir seu script para outras pessoas, ou rodá-lo em
múltiplas máquinas, você deve especificar o analisador no construtor BeautifulSoup .
Isso irá reduzir as chances de que seus usuários
analisem um documento de forma diferente da maneira como você analisou.
Codificação (Encoding) ¶  Todo documento HTML ou XML é escrito em uma codificação (encoding) específica como ASCII
ou UTF-8.
Mas quando você carrega um documento no BeautifulSoup, você irá descobrir
que ele foi convertido para Unicode:  markup  =  "<h1>Sacr \xc3\xa9 bleu!</h1>"  soup  =  BeautifulSoup ( markup )  soup .
Não é mágica (Seria bem legal que fosse).
O BeautifulSoup utiliza uma
sub-biblioteca chamada Unicode, Dammit para detectar a codificação de
um documento e convertê-lo para Unicode.
A codificação detectada automaticamente está
disponível como objeto .original_encoding atributo do objeto BeautifulSoup  soup .
original_encoding  'utf-8'    Unicode, Dammit acerta na maioria das vezes, mas pode errar em algumas.
Outras vezes acerta, porém somente após uma busca byte a byte no documento,
o leva muito tempo.
Se você souber com antecedência a codificação, você poderá
evitar erros ou demora passando-o para o contrutor do BeautifulSoup através de from_encoding .
Abaixo você tem um documento escrito em ISO-8859-8.
O documento é tão
pequeno que o Unicode, Dammit não consegue verificar sua codificação
e acaba fazendo a identificação como ISO-8859-7:  markup  =  b "<h1> \xed\xe5\xec\xf9 </h1>"  soup  =  BeautifulSoup ( markup )  soup .
h1  < h1 > νεμω </ h1 >  soup .
original_encoding  'ISO-8859-7'    Podemos consertar isso passando a codificação correta com from_encoding :  soup  =  BeautifulSoup ( markup ,  from_encoding = "iso-8859-8" )  soup .
h1  < h1 > םולש </ h1 >  soup .
original_encoding  'iso8859-8'    Se você não sabe qual a codificação correta, mas você sabe que o Unicode, Dammit está errado, você pode passar as opções excluentes
como exclude_encodings :  soup  =  BeautifulSoup ( markup ,  exclude_encodings = [ "ISO-8859-7" ])  soup .
original_encoding  'WINDOWS-1255'    Windows-1255 não é 100% correto, mas é um superconjunto compatível com
ISO-8859-8, portanto é mais próximo do ideal.
( exclude_encodings é uma opção nova no Beautiful Soup 4.4.0.)
Em casos raros (geralmente quando um documento UTF-8 contém texto escrito
em uma codificação completamente diferente), a única maneira de ser convertido para
Unicode é convertendo alguns caracteres com o caractere especial Unicode
“REPLACEMENT CHARACTER” (U+FFFD, �).
Se o Unicode, Dammit precisar utilizá-lo,
ele será armazenado no atributo .contains_replacement_characters como True no UnicodeDammit ou objeto BeautifulSoup .
Isso deixa você ciente
que a representação Unicode não é uma representação exata do original - algum dado
foi perdido.
Se um documento possui �, mas .contains_replacement_characters é False ,
você poderá concluir então que o � já estava ali originalmente e não representa dados
perdidos.
Codificação de Saída ¶  Quando um documento é gerado pelo Beautiful Soup, ele é gerado como UTF-8,
mesmo que o documento não for um UTF-8 de início.
Aqui está um documento gerado
com codificação Latin-1:  markup  =  b '''  <html>  <head>  <meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" />  </head>  <body>  <p>Sacr \xe9 bleu!</p>  </body>  </html>  '''  soup  =  BeautifulSoup ( markup )  print ( soup .
#   </p>  #  </body>  # </html>    Note que a tag <meta> foi reescrita para refletir o fato que o documento
é agora um UTF-8.
Se você não quiser um UTF-8, você pode passar a codificação desejada como parâmetro de prettify() :  print ( soup .
prettify ( "latin-1" ))  # <html>  #  <head>  #   <meta content="text/html; charset=latin-1" http-equiv="Content-type" />  # ...
   Você também pode chamar encode() no objeto BeautifulSoup ou em qualquer elemento
do objeto, assim como se faz em uma string Python:  soup .
encode ( "utf-8" )  # '<p>Sacr\xc3\xa9 bleu!</p>'    Qualquer caractere que não pode ser representado na codificação escolhida
irá ser convertida para uma entidade de referência numérica XML.
Abaixo você
tem um documento que inclui o caractere Unicode SNOWMAN:  markup  =  u "<b> \N{SNOWMAN} </b>"  snowman_soup  =  BeautifulSoup ( markup )  tag  =  snowman_soup .
b    O caractere SNOWMAN faz parte da documentação UTF-8 (algo como
☃), mas não possui representação para este caractere em ISO-latin-1 ou
ASCII, portanto ele é convertido para “&#9731” para as essas codificações:  print ( tag .
encode ( "ascii" )  # <b>&#9731;</b>      Unicode, Dammit ¶  Você pode usar o Unicode, Dammit fora do Beautiful Soup.
É útil
quando você possui dados em uma codificação desconhecida e quer
simplesmente convertê-la para Unicode:  from  bs4  import  UnicodeDammit  dammit  =  UnicodeDammit ( "Sacr \xc3\xa9 bleu!"
original_encoding  # 'utf-8'    As respostas do Unicode, Dammit serão um pouco mais precisas se você
instalar as bibliotecas chardet ou cchardet .
Quanto maior a quantidade
de dados no arquivo que você passar para o Unicode, Dammit , mais precisas serão
as conversões.
Se você possui suas suspeitas sobre qual a codificação original,
você pode passar as opções em uma lista:  dammit  =  UnicodeDammit ( "Sacr \xe9 bleu!"
original_encoding  # 'latin-1'    Unicode, Dammit possui duas características que o Beautiful Soup não utiliza.
Smart quotes ¶  Você pode utilizar Unicode, Dammit para converter Microsoft smart quotes para
entidades HTML ou XML:  markup  =  b "<p>I just \x93 love \x94 Microsoft Word \x92 s smart quotes</p>"  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "html" ) .
unicode_markup  # u'<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'    Você também pode converter Microsoft smart quotes para ASCII:  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "ascii" ) .
unicode_markup  # u'<p>I just "love" Microsoft Word\'s smart quotes</p>'    Espero que você ache estas características úteis, mas o Beautiful Soup não
as usa.O Beautiful Soup dá preferência ao comportamento padrão, que é
converter para caracteres Unicode:  UnicodeDammit ( markup ,  [ "windows-1252" ]) .
unicode_markup  # u'<p>I just \u201clove\u201d Microsoft Word\u2019s smart quotes</p>'      Codificação Inconsistente ¶  Algumas vezes um documento é em sua maioria UTF-8, mas contém  caracteres
Windows-1252 assim como (de novo) Microsoft smart quotes.
Isso pode acontecer
quando um website compostos de dados de muitas fontes diferentes.
Você pode
utilizar UnicodeDammit.detwingle() para transformar este documento em um
UTF-8 puro.
Aqui está um exemplo:  snowmen  =  ( u " \N{SNOWMAN} "  *  3 )  quote  =  ( u " \N{LEFT DOUBLE QUOTATION MARK} I like snowmen!
encode ( "windows_1252" )    Este documento é uma bagunça.
O snowmen é um UTF-8 e as aspas são Windows-1252.
Você pode exibir o snowmen ou as aspas, mas não os dois ao mesmo tempo:  print ( doc )  # ☃☃☃�I like snowmen!�  print ( doc .
decode ( "windows-1252" ))  # â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”    Decodificar um documento como UTF-8 gera um UnicodeDecodeError , e
como um Windows-1252 lhe tras algo sem sentido.
Felizmente, UnicodeDammit.detwingle() irá converter a string para UTF-8 puro,
permitindo a você decodificá-la para Unicode e exibir o snowmen e as
aspas simultaneamente:  new_doc  =  UnicodeDammit .
decode ( "utf8" ))  # ☃☃☃“I like snowmen!”    UnicodeDammit.detwingle() sabe apenas como trabalhar com Windows-1252
contido em UTF-8 (ou vice versa, eu suponho), mas este é o caso mais comum.
Note que você deve chamar UnicodeDammit.detwingle() em seu dado
antes de passá-lo para BeautifulSoup ou para o construtor UnicodeDammit .
O Beautiful Soup assume que um documento possui apenas uma codificação,
independente de qual ela seja.
Se você passar um documento que
contém ambos UTF-8 e Windows-1252, é provável que ele pense que todo
o documento seja Windows-1252, e o documento parecerá â˜ƒâ˜ƒâ˜ƒ“I  like  snowmen!” .
UnicodeDammit.detwingle() é novo no Beautiful Soup 4.1.0.
Linhas numeradas ¶  Os interpretadores html.parser`  e  ``html5lib podem rastrear onde, no
documento original, cada tag foi encontrada.
Você pode acessar esta
informação através de Tag.sourceline (número da linha) e Tag.sourcepos (posição do início da tag na linha):  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  for  tag  in  soup .
find_all ( 'p' ):  print ( tag .
string )  # (1, 0, u'Paragraph 1')  # (2, 3, u'Paragraph 2')    Note que os dois interpretadores significam coisas levemente diferentes por sourceline e sourcepos .
Para html.parser, estes números representam
a posição do sinal menor que`inicial.
Para html5lib, representa a posição
do sinal `maior que final:  soup  =  BeautifulSoup ( markup ,  'html5lib' )  for  tag  in  soup .
string )  # (2, 1, u'Paragraph 1')  # (3, 7, u'Paragraph 2')    Você pode desabilitar esta característica passando store_line_numbers=False`  no  construtor  ``BeautifulSoup :  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  store_line_numbers = False )  soup .
sourceline  # None    Esta característica é nova no 4.8.1 e os analisadores baseados no lxml
não a suportam.
Comparando objetos por igualdade ¶  O Beautiful Soup diz que dois objetos NavigableString ou Tag são
iguais quando eles apresentam as mesma marcação HTML ou XML.
No exemplo
abaixo, as duas tags <b> são tratadas como iguais, mesmo estando em partes
diferentes da árvore do objeto, porque ambas estão como “<b>pizza</b>”:  markup  =  "<p>I want <b>pizza</b> and more <b>pizza</b>!</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  first_b ,  second_b  =  soup .
find_all ( 'b' )  print  first_b  ==  second_b  # True  print  first_b .
previous_element  # False    Se você quiser verificar se duas variáveis se referem exatamente ao
mesmo objeto, use is :  print  first_b  is  second_b  # False      Copiando objetos Beautiful Soup ¶  Você pode utilizar copy.copy() para criar uma cópia de qualquer Tag ou NavigableString :  import  copy  p_copy  =  copy .
p )  print  p_copy  # <p>I want <b>pizza</b> and more <b>pizza</b>!</p>    A cópia será considerada igual ao original, desde que ela apresente a mesma
marcação que o original, mas não será o mesmo objeto:  print  soup .
p  is  p_copy  # False    A única diferença real é que a cópia é completamente separada da árvore
original do Beautiful Soup, como se extract() fosse chamado para ela:  print  p_copy .
parent  # None    Isso acontece porque dois objetos Tag diferentes não podem ocupar o mesmo
espaço ao mesmo tempo.
Analisando apenas parte de um documento ¶  Suponhamos que você queira que o Beautiful Soup olhe apenas para as
tags <a> de um documento.
É um desperdício de tempo e memória analisar
todo o documento e, posteriormente, analisar novamente apenas para buscar
as tags <a>.
Seria muito mais rápido ignorar tudo o que não for <a> em
primeiro lugar.
A classe SoupStrainer permite que você escolha
qual partes do documento serão analisadas.
Você deverá penas criar uma
instância de SoupStrainer e passá-la ao construtor BeautifulSoup no argumento parse_only .
(Note que esta característica não funcionará se você estiver utilizando
o html5lib .
Se você utilizar o html5lib, todo o documento será analisado.
Isso acontece porque html5lib constantemente reorganiza a árvore de análise
e se alguma parte do documento realmente não fizer parte dela, ela irá quebrar.
Para evitar confusão, no exemplo abaixo, forçarei o Beautiful Soup a usar o
analisador nativo do Python).
SoupStrainer ¶  A classe SoupStrainer recebe os mesmos argumentos que qualquer método em Buscando na árvore : name , attrs , string , e **kwargs .
Aqui temos três objetos SoupStrainer  from  bs4  import  SoupStrainer  only_a_tags  =  SoupStrainer ( "a" )  only_tags_with_id_link2  =  SoupStrainer ( id = "link2" )  def  is_short_string ( string ):  return  len ( string )  <  10  only_short_strings  =  SoupStrainer ( string = is_short_string )    Irei trazer de volta o documento “three sisters” mais uma vez e veremos
como o documento se parece quando é analisado com estes três objetos SoupStrainer diferentes:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_a_tags ) .
prettify ())  # Elsie  # ,  # Lacie  # and  # Tillie  # ...
 #    Você pode também passar um SoupStrainer em qualquer método coberto em Buscando na árvore .
Este uso provavelmente não seja muito útil, mas pensei que deveria mencioná-lo:  soup  =  BeautifulSoup ( html_doc )  soup .
find_all ( only_short_strings )  # [u'\n\n', u'\n\n', u'Elsie', u',\n', u'Lacie', u' and\n', u'Tillie',  #  u'\n\n', u'...', u'\n']       Solucionando Problemas ¶   diagnose() ¶  Se você está tendo problemas em entender o que o Beautiful Soup está
fazendo com um documento, passe o documento pela função diagnose() .
(Nova no Beautiful Soup 4.2.0.)
O Beautiful Soup irá retornar um relatório mostrando como diferentes parsers
lidam com o documento e irá lhe dizer o Beautiful Soup poderia estar utilizando outro parser:  from  bs4.diagnose  import  diagnose  with  open ( "bad.html" )  as  fp :  data  =  fp .
# Found lxml version 2.3.2.0  #  # Trying to parse your data with html.parser  # Here's what html.parser did with the document:  # ...
   Olhando para o que diagnose() retorna, poderá lhe dizer como resolver
o seu problema.
Mesmo que não consiga, você poderá colar a saída de diagnose() quando solicitar ajuda.
Erros enquanto se analisa um documento ¶  Existem dois tipos diferentes de erros de análise.
Existem quebras
quando você passa para o Beautiful Soup um documento e ele retorna uma
exceção, geralmente um HTMLParser.HTMLParseError .
E existe o comportamento
inesperado, quando uma árvore de análise parece um pouco diferente do
documento usado para criá-la.
Quase nenhum destes problemas são parte do Beautiful Soup.
Não é
porque o Beautiful Soup é maravilhosamente um software bem escrito.
É
porque o Beautiful Soup não inclui nenhum código de análise.
Ao invés disso,
ele depende de analisadores externos.
Se um analisador não funciona com
certo documento, a melhor solução é tentar um analisador diferente.
Veja Instalando um interpretador para detalhes e uma comparação entre eles.
Os erros de interpretação mais comuns são HTMLParser.HTMLParseError:  malformed  start  tag e HTMLParser.HTMLParseError:  bad  end  tag .
Existem dois parsers gerados para o parser built in do Python
e a solução é install lxml ou html5lib.
Os tipos de erros de comportamento inesperado mais comuns acontecem
quando não é encontrada a tag buscada no documento.
Você vê a busca
sendo executada, mas find_all() retorna [] ou find() retorna None .
Este é um problema comum com o analisador HTML nativo do Python que algumas
vezes pula tags que ele não entende.
Novamente, a solução é instalar o lxml ou html5lib.
Problemas de incompatibilidade de versões ¶   SyntaxError:  Invalid  syntax (on the line ROOT_TAG_NAME  =  u'[document]' ): Causado por rodar a versão Python 2 do
Beautiful Soup no Python 3, sem converter o código.
ImportError:  No  module  named  HTMLParser - Causado por rodar a
versão Python 2  do Beautiful Soup no Python 3.
ImportError:  No  module  named  html.parser - Causado por rodar a
versão Python 3 do Beautiful Soup no Python 2.
ImportError:  No  module  named  BeautifulSoup - Causado por rodar
código do Beautiful Soup 3 em um sistema que não possui o BS3
instalado.
Ou por escrever código Beautiful Soup 4 sem saber que
o nome do pacote é diferente no bs4 .
ImportError:  No  module  named  bs4 - Causado por rodar código Beautiful
Soup 4 em um sistema que não possui o BS4 instalado.
Analisando um XML ¶  Por padrão, o Beautiful Soup analisa documento como HTML.
Para analisar um documento
como XML, passe “xml” como um segundo argumento ao construtor BeautifulSoup  soup  =  BeautifulSoup ( markup ,  "xml" )    Você precisará ter :ref:` lxml instalado <parser-installation>`.
Outros problemas com analisadores ¶   Se seu script funciona em um computador, mas não em outro,
ou em um ambiente virtual mas não em outro, ou fora do ambiente
virtual mas não dentro dele, provavelmente porque ambos os ambientes
possuem bibliotecas de analisadores difererentes.
Por exemplo, você pode
ter desenvolvido um script em um computador que possui lxml instalado,
e então estar tentando rodá-lo no seu computador que possui apenas html5lib
instalado.
Veja Diferenças entre os interpretadores (parsers) para entender porque isso importa,
e corrija o problema mencionando uma biblioteca específica no construtor BeautifulSoup .
Por tags HTML e atributos serem case-insensitive , todos os três
parsers HTML convertem tags e atributos para lowercase.
Isso é,
a marcação <TAG></TAG> é convertida para <tag></tag>.
Se você quiser
preservar a formatação anterior das tags e atributos, você precisará analisar o documento como XML.
Diversos ¶   UnicodeEncodeError:  'charmap'  codec  can't  encode  character  u'\xfoo'  in  position  bar (ou qualquer outro UnicodeEncodeError ) - Este não é um problema do Beautiful Soup.
Este problema poderá surgir em duas situações: a primeira quando você
tentar imprimir um caractere Unicode que seu console não sabe como
exibir.
(Veja Esta página na wiki do Python para saber mais.).
A segunda,
quando você está gravando um arquivo e passa um caractere Unicode que
não é suportado pelo seu codificador padrão.
Neste caso, a solução mais
simples é explicitamente converter a string Unicode em UTF-8 com u.encode("utf8") .
KeyError:  [attr] - Caused by accessing tag['attr'] quando a
tag em questão não define o atributo attr .
Os erros mais comuns são KeyError:  'href' e KeyError:  'class' .
Use tag.get('attr') se você não tem certeza se attr está
definido, assim como você faria em um dicionário Python.
AttributeError:  'ResultSet'  object  has  no  attribute  'foo' - Isso
geralmente ocorre quando você espera que find_all() retorne
uma única tag ou string.
Mas find_all() retorn uma _lista_ de tags
e strings–um objeto ResultSet .
Você precisa iterar sobre a lista e
buscar .foo para cada um.
Ou, se você realmente quiser apenas um resultado,
deverá usar find() ao invés de find_all() .
AttributeError:  'NoneType'  object  has  no  attribute  'foo' - Isso
geralmente acontece quando é chamado find() e então se tenta acessar
o atributo .foo` o resultado.
Mas no seu caso, find() não encontra nada,
então retorna None ao invés de retornar uma tag ou uma string.
Você precisa
descobrir porque find() não está retornando nada.
Melhorando a performance ¶  O Beautiful Soup nunca será tão rápido quanto os parsers em que
ele foi construido em cima.
Se o tempo de resposta se tornar crítico,
se você estiver pagando por hora de uso de um computador ou se há
qualquer outra razão para que o tempo de processamento seja mais
valioso que o tempo de programação, você deve esquecer o Beautiful Soup
e trabalhar diretamente em cima do lxml .
Dito isso, existem algumas coisas que você pode fazer para acelerar o
Beautiful Soup.
Se você não está utilizando o lxml como seu parser,
meu conselho é que o faça start .
O Beautiful Soup analisa documentos significativamente mais rápido
utilizando o lxml do que usando o html.parser ou html5lib.
Você pode acelerar a detecção da codificação significativamente instalando
a biblioteca cchardet .
Analisando apenas parte de um documento não irá lhe poupar muito tempo de
análise, mas irá poupar muita memória e fará a busca no documento muito
mais rápida.
Beautiful Soup 3 ¶  O Beautiful Soup 3 é a versão anterior e não é mais desenvolvida
ativamente.
Ela atualmente faz parte da maioria das distribuições
Linux:  $  apt - get  install  python - beautifulsoup  Também está publicada no PyPi como BeautifulSoup .
:  $  easy_install  BeautifulSoup  $  pip  install  BeautifulSoup  Você também pode fazer o download de um tarball do Beautiful Soup 3.2.0 .
Se você rodar easy_install  beautifulsoup ou easy_install  BeautifulSoup , mas seu código não funcionar, você instalou o Beautiful
Soup 3 por engano.
Você precisa executar easy_install  beautifulsoup4 .
A documentação do Beautiful Soup 3 está arquivada online .
Portabilidade de código para BS4 ¶  A maioria dos códigos escritos em Beautiful Soup 3 irá funcionar no
Beautiful Soup 4 com uma pequena alteração.
Tudo que você precisa
fazer é alterar o nome do pacote de BeautifulSoup para bs4 .
Então:  from  BeautifulSoup  import  BeautifulSoup    deverá ser assim:  from  bs4  import  BeautifulSoup     Se for gerado um ImportError “No module named BeautifulSoup”, o
problema é que você está tentando executar um código Beautiful Soup 3,
mas possui apenas o Beautiful Soup 4 instalado.
Se for gerado um ImportError “No module named bs4”, o problema
é que você está tentando executar um código Beautiful Soup 4, mas
possui apenas o Beautiful Soup 3 instalado.
Apesar do BS4 ser quase totalmente compativel com BS3, a maioria de seus
métodos foram depreciados e renomeados para atender o padrão PEP 8 .
Existem muitas outras
renomeações e alterações, e algumas delas quebram esta compatibilidade.
Aqui está o que você irá precisar saber para converter seu código BS3 para BS4:   Você precisa de um interpretador (parser) ¶  O Beautiful Soup 3 utilizava o SGMLParser do Python, um módulo que
foi depreciado e removido no Python 3.0.
O Beautiful Soup 4 utiliza o html.parser por padrão, mas você pode adicionar o lxml ou html5lib
e utilizá-los como alternativa.
Veja Instalando um interpretador para
comparação.
Como o html.parser não é o mesmo analisador que SGMLParser , é possível
que o Beautiful Soup 4 retorne uma árvore de análise diferente da
gerada pelo Beautiful Soup 3 para as mesmas marcações.
Se você trocar html.parser por lxml ou html5lib, você poderá descorbrir que a árvore também
mudará.
Se isso acontecer, você precisará atualizar seu código para lidar com a
nova árvore.
Nomes dos Métodos ¶   renderContents -> encode_contents  replaceWith -> replace_with  replaceWithChildren -> unwrap  findAll -> find_all  findAllNext -> find_all_next  findAllPrevious -> find_all_previous  findNext -> find_next  findNextSibling -> find_next_sibling  findNextSiblings -> find_next_siblings  findParent -> find_parent  findParents -> find_parents  findPrevious -> find_previous  findPreviousSibling -> find_previous_sibling  findPreviousSiblings -> find_previous_siblings  getText -> get_text  nextSibling -> next_sibling  previousSibling -> previous_sibling   Alguns argumentos do construtor do Beautiful Soup foram renomeados pelas
mesmas razões:   BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...)  BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...)   Eu renomeei um método para compatibilidade com Python 3:   Tag.has_key() -> Tag.has_attr()   Eu renomeei um atributo para utilizar uma terminologia mais precisa:   Tag.isSelfClosing -> Tag.is_empty_element   Eu renomeei três atributos para evitar utilizar palavras reservadas do
Python.
Ao contrário das outras, estas alterações não são compativeis com
versões anteriores.
Se você utilizar estes atributos no BS3, seu código
irá quebrar no BS4 até você corrigí-los.
UnicodeDammit.unicode -> UnicodeDammit.unicode_markup  Tag.next -> Tag.next_element  Tag.previous -> Tag.previous_element     Geradores ¶  Eu dei nomes aos geradores de acordo com o PEP-8 e transformei-os
em propriedades:   childGenerator() -> children  nextGenerator() -> next_elements  nextSiblingGenerator() -> next_siblings  previousGenerator() -> previous_elements  previousSiblingGenerator() -> previous_siblings  recursiveChildGenerator() -> descendants  parentGenerator() -> parents   Então, ao invés de:  for  parent  in  tag .
parentGenerator ():  ...
   Você pode escrever:  for  parent  in  tag .
parents :  ...
   (Mas a versão antiga ainda funcionará.)
Alguns dos geradores eram utilizados para gerar None após
finalizado e então parar.
Isso era um bug.
Agora os geradores
apenas param.
Existem dois novos geradores, .strings e
.stripped_strings .
.strings gera objetos
NavigableString, e .stripped_strings gera strings Python com
espaços em branco removidos.
XML ¶  Não existe mais uma classe BeautifulStoneSoup para analisar XML.
Para
analisar XML você deverá passar “xml” como segundo argumento ao construtor BeautifulSoup .
Pela mesma razão, o construtor BeautifulSoup não
reconhece mais o argumento isHTML .
A manipulação do Beautiful Soup’s de tags XML vazias foi melhorada.
Anteriormente, quando você analisava um XML, deveria explicitamente
dizer quais tags seriam consideradas elementos de tag vazios.
O
argumento selfClosingTags não é mais reconhecido.
Ao invés disso,
o Beautiful Soup considera qualquer tag vazia como um elemento de tag vazia.
Se você adicionar uma filha a um elemento de tag vazia, ela deixará de ser vazia.
Entidades ¶  Uma entidade HTML ou XML de entrada é sempre convertida em
seu caractere Unicode correspondente.
O Beautiful Soup 3 possuia
inúmeras maneiras redundantes de lidar com entidades, as quais foram
removidas.
O construtor BeautifulSoup não reconhece mais os argumentos smartQuotesTo ou convertEntities .
( Unicode,
Dammit ainda possui smart_quotes_to , mas seu padrão agora é converter
smart quotes em Unicode.)
As constantes HTML_ENTITIES , XML_ENTITIES , e XHTML_ENTITIES foram removidas, desde que elas
se referiam a uma feature (transformar algumas, mas não todas as entidades
em caracteres Unicode) que não existe mais.
Se você quiser transformar caracteres Unicode novamente em entidades HTML
na saída, ao invés de transformá-las em caracteres UTF-8, você precisará
utilizar um output formatter .
Variados ¶  Tag.string agora opera recursivamente.
Se a tag A
contém apenas uma tag B e nada mais, então A.string é o mesmo que
B.string.
(Anteriormente era None)  Atributos com múltiplos valores como class possuem listas de strings
como valores e não strings.
Isso deverá afetar a maneira que você buscará
por classes CSS.
Se você passar um dos métodos find* , ambos string  e um argumento específico de uma tag como name , o Beautiful Soup
irá buscar por tags que atentem o seu critério de argumento específico e que Tag.string atenda o valor para string .
Isso não irá encontrar as strings por si.
Anteriormente, Beautiful Soup ignorava
o argumento específico  de uma tag e olhava apenas para as strings.
O construtor BeautifulSoup não reconhece mais o argumento markupMassage .
É agora responsabilidade do parser de manipular a marcação corretamente.
As classes raramente usadas do analisador como ICantBelieveItsBeautifulSoup e BeautifulSOAP foram removidas.
é agora decisão do analisador como manipular marcações ambiguas.
O método prettify() agora retorna uma string Unicode, e não bytestring.
Table of Contents   Documentação Beautiful Soup  Como conseguir ajuda:    Início Rápido  Instalando o Beautiful Soup  Problemas após a instalação  Instalando um interpretador (parser)    Criando a “Sopa”  Tipos de objetos  Tag  Nome  Atributos  Atributos com múltiplos valores      NavigableString  BeautifulSoup  Comentários e outras strings especiais    Navegando pela árvore  Descendo na Árvore  Navegar usando os nomes das tags  .contents e .children  .descendants  .string  .strings e stripped_strings    Subindo na Árvore  .parent  .parents    Navegando para os lados:  .next_sibling e .previous_sibling  .next_siblings e .previous_siblings    Indo e voltando  .next_element e .previous_element  .next_elements e .previous_elements      Buscando na árvore  Tipos de filtros  Uma string  Uma expressão regular (regex)  Uma lista  True  Uma função    find_all()  O argumento name  Os argumentos “palavras-chave”  Buscando por uma classe CSS  O argumento string  O argumento limit  O argumento recursive    Chamar uma tag é como chamar find_all()  find()  find_parents() e find_parent()  find_next_siblings() e find_next_sibling()  find_previous_siblings() e find_previous_sibling()  find_all_next() e find_next()  find_all_previous() e find_previous()  Seletores CSS    Modificando a árvore  Alterando nomes de tags e atributos  Modificando .string  append()  extend()  NavigableString() e .new_tag()  insert()  insert_before() e insert_after()  clear()  extract()  decompose()  replace_with()  wrap()  unwrap()  smooth()    Saída  Pretty-printing  Non-pretty printing  Output formatters  get_text()    Especificando um interpretador (parser) para uso  Diferenças entre os interpretadores (parsers)    Codificação (Encoding)  Codificação de Saída  Unicode, Dammit  Smart quotes  Codificação Inconsistente      Linhas numeradas  Comparando objetos por igualdade  Copiando objetos Beautiful Soup  Analisando apenas parte de um documento  SoupStrainer    Solucionando Problemas  diagnose()  Erros enquanto se analisa um documento  Problemas de incompatibilidade de versões  Analisando um XML  Outros problemas com analisadores  Diversos  Melhorando a performance    Beautiful Soup 3  Portabilidade de código para BS4  Você precisa de um interpretador (parser)  Nomes dos Métodos  Geradores  XML  Entidades  Variados         This Page   Show Source     Quick search               Navigation    index  Beautiful Soup 4.4.0 documentation »  Documentação Beautiful Soup    © Copyright 2004-2015, Leonard Richardson.
Navegación    índice   módulos |  documentación de Beautiful Soup - 4.12.0 »  Documentación de Beautiful Soup         Documentación de Beautiful Soup ¶   Beautiful Soup es una
librería de Python para extraer datos de archivos en formato HTML y XML.
Trabaja con tu analizador favorito para ofrecer maneras bien definidas
de navegar, buscar y modificar el árbol analizado.
Puede llegar a ahorrar
horas o días de trabajo a los programadores.
Este manual ilustra con ejemplos la funcionalidades más importantes
de Beautiful Soup 4.
Te muestro las cosas para las que la librería es buena,
cómo funciona, cómo usarla, cómo hacer lo que quieres y qué hacer cuando
no se cumplen tus expectativas.
Este documento cubre Beautiful Soup versión 4.12.1.
Los ejemplos en este
documento fueron escritos para Python 3.8.
Podrías estar buscando la documentación de Beautiful Soup 3 .
Si es así, debes saber que Beautiful Soup 3 ya no se desarrolla y
su soporte fue abandonado el 31 de diciembre de 2020.
Si quieres
conocer la diferencias entre Beautiful Soup 3 y Beautiful Soup 4,
mira Actualizar el código a BS4 .
Esta documentación ha sido traducida a otras lenguas por los usuarios
de Beautiful Soup:   这篇文档当然还有中文版.
Cómo conseguir ayuda ¶  Si tienes alguna pregunta sobre BeautifulSoup, o si tienes problemas, envía un correo electrónico al grupo de discusión .
Si tienes algún problema relacionado con el análisis de un documento HTML,
asegúrate de mencionar lo que la función diagnose() dice sobre dicho documento.
Cuando informes de algún error en esta documentación, por favor,
indica la traducción que estás leyendo.
Inicio rápido ¶  Este es un documento HTML que usaré como ejemplo a lo largo de este
documento.
Es parte de una historia de Alicia en el país de las maravillas :  html_doc  =  """<html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """    Al procesar el documento de «Las tres hermanas» en Beautiful Soup, se nos
devuelve un objeto BeautifulSoup , que representa el
documento como una estructura de datos anidada:  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )  print ( soup .
#   </p>  #   <p class="story">  #    ...
 #   </p>  #  </body>  # </html>    Estas son algunas de las maneras sencillas para navegar
por la estructura de datos:  soup .
find ( id = "link3" )  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>    Una tarea frecuente es extraer todas las URL encontradas en las etiquetas
<a> de una página:  for  link  in  soup .
get ( 'href' ))  # http://example.com/elsie  # http://example.com/lacie  # http://example.com/tillie    Otra tarea habitual es extraer todo el texto de una página:  print ( soup .
#  # ...
   ¿Esto se parece a lo que necesitas?
Si es así, sigue leyendo.
Instalar Beautiful Soup ¶  Si usas una versión reciente de Debian o Ubuntu Linux, puedes instalar
Beautiful Soup con el gestor de paquetes del sistema:  $  apt - get  install  python3 - bs4  Beautiful Soup 4 está publicado en Pypi, así que si no puedes instalarlo
con el gestor de paquetes, puedes instalarlo con easy_install o pip .
El nombre del paquete es beautifulsoup4 .
Asegúrate de que
usas la versión correcta de pip o easy_install para tu versión
de Python (podrían llamarse pip3 y easy_install3 , respectivamente):  $  easy_install  beautifulsoup4  $  pip  install  beautifulsoup4  (El paquete BeautifulSoup  no es el que quieres.
Ese es
el lanzamiento anterior Beautiful Soup 3 .
Muchos software utilizan
BS3, así que aún está disponible, pero si estás escribiendo nuevo código,
deberías instalar beautifulsoup4 ).
Si no tienes easy_install o pip instalados, puedes descargar el código de Beautiful Soup 4 comprimido en un tarball e
instalarlo con setup.py :  $  python  setup.py  install  Si aún así todo falla, la licencia de Beautiful Soup te permite
empaquetar la librería completa con tu aplicación.
Puedes descargar
el tarball , copiar su directorio bs4 en tu base de código y
usar Beautiful Soup sin instalarlo en absoluto.
Yo empleo Python 3.10 para desarrollar Beautiful Soup, aunque debería
funcionar con otras versiones recientes.
Instalar un analizador ¶  Beautiful Soup soporta el analizador de HTML incluido en la librería
estándar de Python, aunque también soporta varios analizadores de
Python de terceros.
Uno de ellos es el analizador de lxml .
Dependiendo de tu instalación, puedes instalar lxml con uno de los
siguientes comandos:  $  apt - get  install  python - lxml  $  easy_install  lxml  $  pip  install  lxml  Otra alternativa es usar el analizador de Python de html5lib ,
el cual analiza HTML de la misma manera en la que lo haría
un navegador web.
Dependiendo de tu instalación, puedes instalar
html5lib con uno de los siguientes comandos:  $  apt - get  install  python - html5lib  $  easy_install  html5lib  $  pip  install  html5lib  Esta tabla resume las ventajas e inconvenientes de cada librería de los analizadores:   Analizador  Uso típico  Ventajas  Desventajas   html.parser de Python  BeautifulSoup(markup,  "html.parser")   Ya incluido  Rapidez decente  Tolerante (en Python 3.2)     No tan rápido como lxml,
menos tolerante que
html5lib.
Analizador HTML de
lxml  BeautifulSoup(markup,  "lxml")   Muy rápido  Tolerante     Dependencia externa de C     Analizador XML de
lxml  BeautifulSoup(markup,  "lxml-xml")  BeautifulSoup(markup,  "xml")   Muy rápido  El único analizador XML
actualmente soportado     Dependencia externa de C     html5lib  BeautifulSoup(markup,  "html5lib")   Extremadamente tolerante  Analiza las páginas de la misma
manera que un navegador web  Crea HTML5 válido     Muy lento  Dependencia externa de
Python      Si puedes, te recomiendo que instales y uses lxml para mayor velocidad.
Ten en cuenta que si un documento es inválido, analizadores diferentes
generarán árboles de Beautiful Soup diferentes para él.
Mira Diferencias entre analizadores para más detalle.
Haciendo la sopa ¶  Para analizar un documento pásalo al constructor de BeautifulSoup .
Puedes pasar una cadena de caracteres o abrir un manejador de archivos:  from  bs4  import  BeautifulSoup  with  open ( "index.html" )  as  fp :  soup  =  BeautifulSoup ( fp ,  'html.parser' )  soup  =  BeautifulSoup ( "<html>a web page</html>" ,  'html.parser' )    Primero, el documento se convierte a Unicode, y las entidades HTML se
convierten a caracteres Unicode:  print ( BeautifulSoup ( "<html><head></head><body>Sacr&eacute; bleu!</body></html>" ,  "html.parser" ))  # <html><head></head><body>Sacré bleu!</body></html>    Entonces Beautiful Soup analiza el documento usando el mejor analizador
disponible.
Usará un analizador HTML a no ser que se especifique que se
use un analizador XML (ver Analizar XML ).
Tipos de objetos ¶  Beautiful Soup transforma un complejo documento HTML en un complejo árbol de objetos
de Python.
Pero tan solo tendrás que lidiar con cuatro tipos de objetos: Tag , NavigableString , BeautifulSoup y Comment .
Tag ¶  Un objeto Tag corresponde a una etiqueta XML o HTML en el documento
original.
b  type ( tag )  # <class 'bs4.element.Tag'>    Las etiquetas tienen muchos atributos y métodos, y cubriré la mayoría de ellos en Navegar por el árbol y Buscar en el árbol .
Por ahora, las características
más importantes de una etiqueta son su nombre y sus atributos.
name ¶  Toda etiqueta tiene un nombre:  tag .
name  # 'b'    Si cambias el nombre de una etiqueta, el cambio se verá reflejado en
cualquier especificación generada por Beautiful Soup a partir de entonces:  tag .
name  =  "blockquote"  tag  # <blockquote class="boldest">Extremely bold</blockquote>       attrs ¶  Una etiqueta HTML o XML puede tener cualquier cantidad de atributos.
La etiqueta <b  id="boldest"> tiene un atributo «id» cuyo valor
es «boldest».
Puedes acceder a los atributos de una etiqueta
usándola como un diccionario:  tag  =  BeautifulSoup ( '<b id="boldest">bold</b>' ,  'html.parser' ) .
b  tag [ 'id' ]  # 'boldest'    Puedes acceder a los atributos del diccionario directamente con .attrs :  tag .
attrs  # {'id': 'boldest'}    Puedes añadir, quitar y modificar los atributos de una etiqueta.
De nuevo, esto
se realiza usando la etiqueta como un diccionario:  tag [ 'id' ]  =  'verybold'  tag [ 'another-attribute' ]  =  1  tag  # <b another-attribute="1" id="verybold"></b>  del  tag [ 'id' ]  del  tag [ 'another-attribute' ]  tag  # <b>bold</b>  tag [ 'id' ]  # KeyError: 'id'  tag .
get ( 'id' )  # None     Atributos multivaluados ¶  HTML 4 define algunos atributos que pueden tomar múltiples valores.
HTML 5
elimina un par de ellos, pero define unos cuantos más.
El atributo multivaluado
más común es class (esto es, una etiqueta puede tener más de una clase de CSS).
Otros incluyen rel , rev , accept-charset , headers y accesskey .
Por defecto, Beautiful Soup transforma los valores de un atributo multivaluado en
una lista:  css_soup  =  BeautifulSoup ( '<p class="body"></p>' ,  'html.parser' )  css_soup .
p [ 'class' ]  # ['body', 'strikeout']    Si un atributo parece que tiene más de un valor, pero no es un atributo
multivaluado definido como tal por ninguna versión del estándar de HTML,
Beautiful Soup no modificará el atributo:  id_soup  =  BeautifulSoup ( '<p id="my id"></p>' ,  'html.parser' )  id_soup .
p [ 'class' ]  # 'body strikeout'    Puedes usar get_attribute_list para obtener un valor que siempre sea una lista,
sin importar si es un atributo multivaluado:  id_soup .
get_attribute_list ( 'id' )  # ["my id"]    Si analizas un documento como XML, no hay atributos multivaluados:  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' )  xml_soup .
p [ 'class' ]  # 'body strikeout'    Una vez más, puedes configurar esto usando el argumento multi_valued_attributes  class_is_multi =  {  '*'  :  'class' }  xml_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'xml' ,  multi_valued_attributes = class_is_multi )  xml_soup .
p [ 'class' ]  # ['body', 'strikeout']    Probablemente no tengas que hacer esto, pero si lo necesitas, usa los
parámetros por defecto como guía.
Implementan las reglas descritas en la
especificación de HTML:  from  bs4.builder  import  builder_registry  builder_registry .
NavigableString ¶    Un string corresponde a un trozo de texto en una etiqueta.
Beautiful Soup usa la clase NavigableString para contener estos trozos de texto:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
string )  # <class 'bs4.element.NavigableString'>    Un NavigableString es como una cadena de caracteres de Python Unicode,
exceptuando que también soporta algunas de las características descritas en Navegar por el árbol y Buscar en el árbol .
Puedes convertir un objeto NavigableString a una cadena de caracteres Unicode usando str :  unicode_string  =  str ( tag .
string )  unicode_string  # 'Extremely bold'  type ( unicode_string )  # <type 'str'>    No puedes editar dicha cadena, pero puedes reemplazar una cadena por otra, usando replace_with() :  tag .
replace_with ( "No longer bold" )  tag  # <b class="boldest">No longer bold</b>    NavigableString soporta la mayoría de las características descritas en Navegar por el árbol y Buscar en el árbol , pero no todas.
En particular, como una cadena no puede contener nada (la manera en la que
una etiqueta contiene una cadena de caracteres u otra etiqueta), strings no
admiten los atributos .contents` o .string , o el método find() .
Si quieres usar un NavigableString fuera de Beautiful Soup,
deberías llamar unicode() sobre él para convertirlo en una cadena de caracteres
de Python Unicode.
Si no, tu cadena arrastrará una referencia a todo el árbol analizado
de Beautiful Soup, incluso cuando hayas acabado de utilizar Beautiful Soup.
Esto es un
gran malgasto de memoria.
BeautifulSoup ¶    El objeto BeautifulSoup representa el documento analizado
en su conjunto.
Para la mayoría de propósitos, puedes usarlo como un objeto Tag .
Esto significa que soporta la mayoría de métodos descritos
en Navegar por el árbol and Buscar en el árbol .
Puedes también pasar un objeto BeautifulSoup en cualquiera de
los métodos definidos en Modificar el árbol , como si fuese un Tag .
Esto te permite hacer cosas como combinar dos documentos analizados:  doc  =  BeautifulSoup ( "<document><content/>INSERT FOOTER HERE</document" ,  "xml" )  footer  =  BeautifulSoup ( "<footer>Here's the footer</footer>" ,  "xml" )  doc .
replace_with ( footer )  # 'INSERT FOOTER HERE'  print ( doc )  # <?xml version="1.0" encoding="utf-8"?>  # <document><content/><footer>Here's the footer</footer></document>    Como un objeto BeautifulSoup no corresponde realmente con una
etiqueta HTML o XML, no tiene nombre ni atributos.
Aún así, es útil
comprobar su .name , así que se le ha dado el .name especial
«[document]»:  soup .
name  # '[document]'     Cadenas especiales ¶  Tag , NavigableString y BeautifulSoup cubren la mayoría de todo lo que verás en
un archivo HTML o XML, aunque aún quedan algunos remanentes.
El principal
que probablemente encuentres es el Comment .
string  type ( comment )  # <class 'bs4.element.Comment'>    El objeto Comment es solo un tipo especial de NavigableString :  comment  # 'Hey, buddy.
Want to buy a used parser'    Pero cuando aparece como parte de un documento HTML, un Comment se muestra con un formato especial:  print ( soup .
Want to buy a used parser?-->  # </b>     Para documentos HTML ¶  Beautiful Soup define algunas subclases de NavigableString para contener cadenas de caracteres encontradas dentro de etiquetas
HTML específicas.
Esto hace más fácil tomar el cuerpo principal de la
página, ignorando cadenas que probablemente representen directivas de
programación encontradas dentro de la página.
(Estas clases son nuevas
en Beautiful Soup 4.9.0, y el analizador html5lib no las usa) .
Stylesheet ¶   Una subclase de NavigableString que representa hojas de estilo
CSS embebidas; esto es, cualquier cadena en una etiqueta <style> durante el análisis del documento.
Script ¶   Una subclase de NavigableString que representa
JavaScript embebido; esto es, cualquier cadena en una etiqueta <script> durante el análisis del documento.
Template ¶   Una subclase de :py:class:NavigableString` que representa plantillas
HTML embebidas; esto es, cualquier cadena en una etiqueta <template> durante el análisis del documento.
Para documentos XML ¶  Beautiful Soup define algunas clases NavigableString para contener tipos especiales de cadenas de caracteres que pueden
ser encontradas en documentos XML.
Como Comment , estas
clases son subclases de NavigableString que añaden
algo extra a la cadena de caracteres en la salida.
Declaration ¶   Una subclase de NavigableString que representa la declaración al
principio de un documento XML.
Doctype ¶   Una subclase de NavigableString que representa la declaración del tipo de documento que puede encontrarse cerca del comienzo de un documento XML.
CData ¶   Una subclase de NavigableString que representa una sección CData .
ProcessingInstruction ¶   Una subclase de NavigableString que representa el contenido de
una instrucción de procesamiento XML .
Navegar por el árbol ¶  Aquí está el documento HTML de las «Tres hermanas» de nuevo:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    Usaré este como ejemplo para enseñarte cómo mover una parte de un
documento a otra.
Bajar ¶  Las etiquetas pueden contener cadenas u otras etiquetas.
Estos elementos
son los hijos ( children ) de la etiqueta.
Beautiful Soup ofrece muchos
atributos para navegar e iterar por los hijos de una etiqueta.
Debe notarse que las cadenas de Beautiful Soup no soportan ninguno
de estos atributos, porque una cadena no puede tener hijos.
Navegar usando nombres de etiquetas ¶  La manera más simple de navegar por el árbol analizado es indicar
el nombre de la etiqueta que quieres.
Si quieres la etiqueta <head>,
tan solo indica soup.head :  soup .
title  # <title>The Dormouse's story</title>    Puedes usar este truco una y otra vez para acercarte a una parte concreta
del árbol analizado.
Este código obtiene la primera etiqueta <b> dentro
de la etiqueta <body>:  soup .
b  # <b>The Dormouse's story</b>    Usar el nombre de la etiqueta como atributo te dará solo la primera etiqueta con ese nombre:  soup .
a  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    Si necesitas obtener todas las etiquetas <a>, o cualquier
cosa más complicada que la primera etiqueta con cierto nombre, tendrás
que usar uno de los métodos descritos en Buscar en el árbol , como find_all() :  soup .
find_all ( 'a' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      .contents y .children ¶  Los hijos de una etiqueta están disponibles en una lista llamada .contents :  head_tag  =  soup .
contents  # ['The Dormouse's story']    El objeto BeautifulSoup por sí solo ya tiene hijos.
En este caso,
la etiqueta <html> is hija del objeto BeautifulSoup .
name  # 'html'    Una cadena no tiene .contents , porque no puede contener nada:  text  =  title_tag .
contents  # AttributeError: 'NavigableString' object has no attribute 'contents'    En lugar de obtenerlos como una lista, puedes iterar sobre los hijos
de una etiqueta usando el generador .children :  for  child  in  title_tag .
children :  print ( child )  # The Dormouse's story    Si quieres modificar los hijos de una etiqueta, emplea los métodos
descritos en Modificar el árbol .
No modifiques la lista .contents directamente: eso podría ocasionar problemas que pueden
ser sutiles y difíciles de detectar.
.descendants ¶  Los atributos .contents y .children tan solo consideran los
hijos directos de una etiqueta.
Por ejemplo, la etiqueta <head>
tiene un único hijo directo–la etiqueta <title>:  head_tag .
contents  # [<title>The Dormouse's story</title>]    Pero la etiqueta <title> tiene un hijo: la cadena «The Dormouse’s
story».
Puede dar la sensación de que esa cadena es también hija de
la etiqueta <head>.
El atributo .descendants te permite iterar
sobre todos los hijos de una etiqueta recursivamente: sus hijos,
hijos de sus hijos directos, y así sucesivamente:  for  child  in  head_tag .
descendants :  print ( child )  # <title>The Dormouse's story</title>  # The Dormouse's story    La etiqueta <head> tiene un solo hijo, pero tiene dos descendientes:
la etiqueta <title> y el hijo de la etiqueta <title>.
El objeto BeautifulSoup tiene un hijo directo (la etiqueta <html>), pero
tiene otros muchos descendientes:  len ( list ( soup .
descendants ))  # 26      .string ¶  Si una etiqueta tiene solo un hijo, y dicho hijo es un NavigableString ,
el hijo se obtiene mediante .string :  title_tag .
string  # 'The Dormouse's story'    Si el único hijo de una etiqueta es otra etiqueta, y esa etiqueta tiene un .string , entonces se considera que
la etiqueta madre tiene el mismo .string que su hijo:  head_tag .
string  # 'The Dormouse's story'    Si una etiqueta contiene más una cadena, entonces no está claro
a qué se debería referir .string , así que .string pasa a valer None :  print ( soup .
string )  # None      .strings y stripped_strings ¶  Si hay más de una cosa dentro de una etiqueta, puedes seguir
obteniendo las cadenas.
Usa el generador .string :  for  string  in  soup .
# '\n'  # '...'  # '\n'    Estas cadenas tienden a tener muchos espacios en blanco extra, los
cuales puedes quitar usando el generador .stripped_strings :  for  string  in  soup .
# '...'    Aquí, las cadenas que consisten completamente en espacios en blanco
se ignoran, y espacios en blanco al principio y final de las cadenas
se eliminan.
Subir ¶  Continuando con la analogía del árbol genealógico, toda etiqueta
tiene una madre : la etiqueta que la contiene.
.parent ¶  Puedes acceder a la madre de una etiqueta con el atributo .parent .
En
el ejemplo de «Las tres hermanas», la etiqueta <head> es la madre
de la etiqueta <title>:  title_tag  =  soup .
parent  # <head><title>The Dormouse's story</title></head>    El texto de título tiene una madre: la etiqueta <title> que lo
contiene:  title_tag .
parent  # <title>The Dormouse's story</title>    La madre de una etiqueta de alto nivel como <html> es el objeto BeautifulSoup mismo:  html_tag  =  soup .
parent )  # <class 'bs4.BeautifulSoup'>    Y el .parent de un objeto BeautifulSoup se define como None :  print ( soup .
parent )  # None      .parents ¶  Puedes iterar sobre todas las madres de los elementos con .parents .
Este ejemplo usa .parent para moverse” de una
etiqueta <a> en medio del documento a lo más alto del documento:  link  =  soup .
name )  # p  # body  # html  # [document]       Hacia los lados ¶  Considera un documento sencillo como este:  sibling_soup  =  BeautifulSoup ( "<a><b>text1</b><c>text2</c></a>" ,  'html.parser' )  print ( sibling_soup .
prettify ())  #   <a>  #    <b>  #     text1  #    </b>  #    <c>  #     text2  #    </c>  #   </a>    Las etiquetas <b> y <c> están al mismo nivel: son hijas directas de la misma
etiqueta.
Las llamamos hermanas .
Cuando un documento está bien formateado,
las hermanas están al mismo nivel de sangría.
Puedes usar también esta
relación en el código que escribas.
.next_sibling y .previous_sibling ¶  Puedes usar .next_sibling y .previous_sibling para navegar
entre elementos de la página que están al mismo nivel del árbol
analizado:  sibling_soup .
previous_sibling  # <b>text1</b>    La etiqueta <b> tiene un .next_sibling , pero no .previous_sibling ,
porque no hay nada antes de la etiqueta <b> al mismo nivel del árbol .
Por la misma razón, la etiqueta <c> tiene un .previous_sibling pero no
un .next_sibling :  print ( sibling_soup .
next_sibling )  # None    Las cadenas «text1» y «text2» no son hermanas, porque no tienen la misma
madre:  sibling_soup .
next_sibling )  # None    En documentos reales, los .next_sibling o .previous_sibling de
una etiqueta normalmente serán cadenas que contengan espacios en blanco.
Retomando el documento de «Las tres hermanas»:  # <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  # <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  # <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;    Podrías pensar que la .next_sibling de la primera etiqueta <a> podría
ser la segunda etiqueta <a>.
Pero realmente es una cadena de caracteres:
la coma y el salto de línea que separan la primera etiqueta <a> de la
segunda:  link  =  soup .
next_sibling  # ',\n '    La segunda etiqueta <a> es realmente la .next_sibling de la coma:  link .
next_sibling  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>      .next_siblings y .previous_siblings ¶  Puedes iterar sobre las hermanas de una etiqueta con .next_siblings o .previuos_siblings :  for  sibling  in  soup .
previous_siblings :  print ( repr ( sibling ))  # ' and\n'  # <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>  # ',\n'  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>  # 'Once upon a time there were three little sisters; and their names were\n'       Hacia delante y hacia atrás ¶  Échale un vistazo al comienzo del documento de «Las tres hermanas»:  # <html><head><title>The Dormouse's story</title></head>  # <p class="title"><b>The Dormouse's story</b></p>    Un analizador HTML toma esta cadena de caracteres y la convierte en
una serie de eventos: «se abre una etiqueta <html>», «se abre una
etiqueta <head>», «se abre una etiqueta <title>», «se añade una cadena»,
«se cierra la etiqueta <title>», «se abre una etiqueta <p>» y así
sucesivamente.
Beautiful Soup ofrece herramientas para reconstruir
el análisis inicial del documento.
.next_element y .previous_element ¶  El atributo .next_element de una cadena o etiqueta apunta a cualquiera
que fue analizado inmediatamente después.
Podría ser igual que .next_sibling ,
pero normalmente es drásticamente diferente.
Aquí está la etiqueta final <a> en el documento de «Las tres hermanas».
Su ..next_sibling es una cadena: la terminación de la oración fue
interrumpida por el comienzo de la etiqueta <a>.
Pero el .next_element de esa etiqueta <a>, lo que fue analizado
inmediatamente después de la etiqueta <a>, no es el resto de la
oración: es la palabra «Tillie»:  last_a_tag .
next_element  # 'Tillie'    Esto se debe a que en el marcado original, la palabra «Tillie»
aparece antes del punto y coma.
El analizador se encontró con
una etiqueta <a>, después la palabra «Tillie», entonces la etiqueta
de cierre </a>, después el punto y coma y el resto de la oración.
El punto y coma está al mismo nivel que la etiqueta <a>, pero
la palabra «Tillie» se encontró primera.
El atributo .previous_element es exactamente el opuesto
de .next_element .
Apunta a cualquier elemento que
fue analizado inmediatamente antes que este:  last_a_tag .
next_element  # <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>      .next_elements y .previous_elements ¶  Ya te estarás haciendo a la idea.
Puedes usar estos iteradores
para moverte hacia delante y hacia atrás en el documento tal y como
fue analizado:  for  element  in  last_a_tag .
# '\n'  # <p class="story">...</p>  # '...'  # '\n'        Buscar en el árbol ¶  Beautiful Soup define una gran cantidad de métodos para buscar en
el árbol analizado, pero todos son muy similares.
Dedicaré mucho
tiempo explicando los dos métodos más populares: find() y find_all() .
Los otros métodos toman casi los mismos argumentos,
así que los cubriré brevemente.
De nuevo, usaré el documento de «Las tres hermanas» como ejemplo:  html_doc  =  """  <html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  from  bs4  import  BeautifulSoup  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )    Empleando en un filtro un argumento como find_all() , puedes
«acercar» aquellas partes del documento en las que estés interesado.
Tipos de filtros ¶  Antes de entrar en detalle sobre find_all() y métodos similares,
me gustaría mostrar ejemplos de diferentes filtros que puedes
utilizar en estos métodos.
Estos filtros aparecen una y otra vez a lo
largo de la API.
Puedes usarlos para filtrar basándote en el nombre de
una etiqueta, en sus atributos, en el texto de una cadena, o en alguna
combinación de estos.
Una cadena ¶  El filtro más simple es una cadena.
Pasa una cadena a un método de
búsqueda y Beautiful Soup buscará un resultado para esa cadena
exactamente.
Este código encuentra todas las etiquetas <b> en el
documento:  soup .
find_all ( 'b' )  # [<b>The Dormouse's story</b>]    Si pasas un cadena de bytes , Beautiful Soup asumirá que la cadena
está codificada como UTF-8.
Puedes evitar esto pasando una cadena
Unicode.
Una expresión regular ¶  Si pasas un objeto que sea una expresión regular, Beautiful Soup filtrará
mediante dicho expresión regular usando si su método search() .
Este
código encuentra todas las etiquetas cuyo nombre empiece por la letra
«b»; en este caso, las etiquetas <body> y <b>:  import  re  for  tag  in  soup .
name )  # body  # b    Este código encuentra todas las etiquetas cuyo nombre contiene
la letra “t”:  for  tag  in  soup .
name )  # html  # title      Una lista ¶  Si pasas una lista, Beautiful Soup hará una búsqueda por cadenas
con cualquier elemento en dicha lista.
Este código encuentra
todas las etiquetas <a> y todas las etiquetas <b>:  soup .
find_all ([ "a" ,  "b" ])  # [<b>The Dormouse's story</b>,  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      True ¶  El valor True empareja todo lo que pueda.
Este código encuentra todas las etiquetas del documento, pero ninguna de las cadenas
de texto:  for  tag  in  soup .
name )  # html  # head  # title  # body  # p  # b  # p  # a  # a  # a  # p      Una función ¶  Si ninguna de las formas de búsqueda anteriores te sirven, define
una función que tome un elemento como su único argumento.
La función
debería devolver True si el argumento se corresponde con lo indicado
en la función, y Falso en cualquier otro caso.
Esta es una función que devuelve True si una etiqueta tiene
definida el atributo «class» pero no el atributo «id»:  def  has_class_but_no_id ( tag ):  return  tag .
has_attr ( 'id' )    Pasa esta función a find_all() y obtendrás todas las etiquetas
<p>:  soup .
find_all ( has_class_but_no_id )  # [<p class="title"><b>The Dormouse's story</b></p>,  #  <p class="story">Once upon a time there were…bottom of a well.</p>,  #  <p class="story">...</p>]    Esta función solo devuelve las etiquetas <p>.
No obtiene las etiquetas
<a>, porque esas etiquetas definen ambas «class» y «id».
No devuelve
etiquetas como <html> y <title> porque dichas etiquetas no definen
«class».
Si pasas una función para filtrar un atributo en específico como href , el argumento que se pasa a la función será el valor de
dicho atributo, no toda la etiqueta.
Esta es una función que
encuentra todas las etiquetas <a> cuyo atributo href  no empareja con una expresión regular:  import  re  def  not_lacie ( href ):  return  href  and  not  re .
find_all ( href = not_lacie )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    La función puede ser tan complicada como la necesites.
Esta es una
función que devuelve True si una etiqueta está rodeada por
objetos string :  from  bs4  import  NavigableString  def  surrounded_by_strings ( tag ):  return  ( isinstance ( tag .
name )  # body  # p  # a  # a  # a  # p    Ahora ya estamos listos para entrar en detalle en los métodos
de búsqueda.
find_all() ¶  Firma del método: find_all( name , attrs , recursive , string , limit , **kwargs )  El método find_all() busca por los descendientes de una etiqueta y
obtiene todos aquellos que casan con tus filtros.
He mostrado varios
ejemplos en Tipos de filtros , pero aquí hay unos cuantos más:  soup .
compile ( "sisters" ))  # 'Once upon a time there were three little sisters; and their names were\n'    Algunos de estos deberían ser familiares, pero otros son nuevos.
¿Qué significa pasar un valor para string , o id ?
¿Por qué find_all("p",  "title") encuentra una etiqueta <p> con la clase
CSS «title»?
Echemos un vistazo a los argumentos de find_all() .
El argumento name ¶  Pasa un valor para name y notarás que Beautiful Soup solo
considera etiquetas con ciertos nombres.
Las cadenas de texto se
ignorarán, como aquellas etiquetas cuyo nombre no emparejen.
Este es el uso más simple:  soup .
find_all ( "title" )  # [<title>The Dormouse's story</title>]    Recuerda de Tipos de filtros que el valor para name puede ser una cadena , una expresión regular , una lista , una función ,
o el valor True .
El argumento palabras-clave ¶  Cualquier argumento que no se reconozca se tomará como un filtro para alguno
de los atributos de una etiqueta.
Si pasas un valor para un argumento llamado id , Beautiful Soup filtrará el atributo “id” de cada una de las etiquetas:  soup .
find_all ( id = 'link2' )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Si pasas un valor para href , Beautiful Soup filtrará
el atributo href de cada uno de las etiquetas:  soup .
compile ( "elsie" ))  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    Puedes filtrar un atributo basándote en una cadena , una expresión regular , una lista , una función , o el valor True .
Este código busca todas las etiquetas cuyo atributo id tiene
un valor, sin importar qué valor es:  soup .
find_all ( id = True )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Puedes filtrar varios atributos al mismo tiempo pasando más de un argumento
palabra-clave:  soup .
compile ( "elsie" ),  id = 'link1' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    Algunos atributos, como los atributos data-* en HTML5, tienen nombres que
no pueden ser usados como nombres de argumentos palabra-clave:  data_soup  =  BeautifulSoup ( '<div data-foo="value">foo!</div>' ,  'html.parser' )  data_soup .
find_all ( data - foo = "value" )  # SyntaxError: keyword can't be an expression    Puedes usar estos atributos en búsquedas insertándolos en un diccionario
y pasándolo a find_all() como el argumento attrs :  data_soup .
find_all ( attrs = { "data-foo" :  "value" })  # [<div data-foo="value">foo!</div>]    No puedes usar un argumento palabra-clave para buscar por el nombre
HTML de un elemento, porque BeautifulSoup usa el argumento name para guardar el nombre de la etiqueta.
En lugar de esto, puedes
darle valor a “name” en el argumento attrs :  name_soup  =  BeautifulSoup ( '<input name="email"/>' ,  'html.parser' )  name_soup .
find_all ( attrs = { "name" :  "email" })  # [<input name="email"/>]      Buscando por clase CSS ¶  Es muy útil para buscar una etiqueta que tenga una clase CSS específica,
pero el nombre del atributo CSS, «class», es una palabra reservada de
Python.
Usar class como argumento ocasionaría un error sintáctico.
Desde Beautiful Soup 4.1.2, se puede buscar por una clase CSS usando
el argumento palabra-clave class_ :  soup .
find_all ( "a" ,  class_ = "sister" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Como con cualquier argumento palabra-clave, puede pasar una cadena
de caracteres a class_ , una expresión regular, una función, o True :  soup .
find_all ( class_ = has_six_characters )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Recuerda que una sola etiqueta puede tener varios
valores para su atributo «class».
Cuando se busca por una etiqueta
que case una cierta clase CSS, se está intentando emparejar por cualquiera de sus clases CSS:  css_soup  =  BeautifulSoup ( '<p class="body strikeout"></p>' ,  'html.parser' )  css_soup .
find_all ( "p" ,  class_ = "body" )  # [<p class="body strikeout"></p>]    Puedes también buscar por la cadena de caracteres exacta del atributo class :  css_soup .
find_all ( "p" ,  class_ = "body strikeout" )  # [<p class="body strikeout"></p>]    Pero buscar por variantes de la cadena de caracteres no funcionará:  css_soup .
find_all ( "p" ,  class_ = "strikeout body" )  # []    Si quieres buscar por las etiquetas que casen dos o más clases CSS,
deberías usar un selector CSS:  css_soup .
select ( "p.strikeout.body" )  # [<p class="body strikeout"></p>]    En versiones antiguas de Beautiful Soup, que no soportan el
atajo class_ , puedes usar el truco del attrs mencionado
arriba.
Crea un diccionario cuyo valor para «class» sea la
cadena de caracteres (o expresión regular, o lo que sea) que
quieras buscar:  soup .
find_all ( "a" ,  attrs = { "class" :  "sister" })  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]      El argumento string ¶  Con string puedes buscar por cadenas de caracteres en vez de
etiquetas.
Como con name y argumentos palabras-clave, puedes
pasar una cadena , una expresión regular , una lista , una
función , o el valor True .
Aquí hay algunos ejemplos:  soup .
find_all ( string = is_the_only_string_within_a_tag )  # ["The Dormouse's story", "The Dormouse's story", 'Elsie', 'Lacie', 'Tillie', '...']    Aunque string es para encontrar cadenas, puedes combinarlo
con argumentos que permitan buscar etiquetas: Beautiful Soup
encontrará todas las etiquetas cuyo .string case con tu valor
para string .
Este código encuentra las etiquetas <a> cuyo .string es «Elsie»:  soup .
find_all ( "a" ,  string = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]    El argumento string es nuevo en Beautiful Soup 4.4.0.
En versiones
anteriores se llamaba text :  soup .
find_all ( "a" ,  text = "Elsie" )  # [<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>]      El argumento``limit`` ¶  find_all() devuelve todas las etiquetas y cadenas que emparejan
con tus filtros.
Esto puede tardar un poco si el documento es grande.
Si no necesitas todos los resultados, puedes pasar un número para limit .
Esto funciona tal y como lo hace la palabra LIMIT en SQL.
Indica a Beautiful Soup dejar de obtener resultados después de
haber encontrado un cierto número.
Hay tres enlaces en el documento de «Las tres hermanas», pero este
código tan solo obtiene los dos primeros:  soup .
find_all ( "a" ,  limit = 2 )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]      El argumento recursive ¶  Si llamas a mytag.find_all() , Beautiful Soup examinará todos los
descendientes de mytag : sus hijos, los hijos de sus hijos, y
así sucesivamente.
Si solo quieres que Beautiful Soup considere
hijos directos, puedes pasar recursive=False .
Observa las
diferencias aquí:  soup .
find_all ( "title" ,  recursive = False )  # []    Aquí está esa parte del documento:  < html >  < head >  < title >  The  Dormouse 's story  </ title >  </ head >  ...
   La etiqueta <title> va después de la etiqueta <html>, pero no está directamente debajo de la etiqueta <html>: la etiqueta <head>
está en medio de ambas.
Beautiful Soup encuentra la etiqueta <title> cuando
se permite observar todos los descendientes de la etiqueta <html>,
pero cuando recursive=False restringe a los hijos directos
de la etiqueta <html>, no se encuentra nada.
Beautiful Soup ofrece mucho métodos de análisis del árbol (descritos
más adelante), y la mayoría toman los mismos argumentos que find_all() : name , attrs , string , limit , y los argumentos
palabras-clave.
Pero el argumento recursive es diferente: find_all() y find() son los únicos métodos que lo soportan.
Pasar recursive=False en un método como find_parents() no sería
muy útil.
Llamar a una etiqueta es como llamar a find_all() ¶  Como find_all() es el método más popular en la API de búsqueda
de Beautiful Soup, puedes usar un atajo para usarlo.
Si utilizas
el objeto BeautifulSoup o un objeto Tag como si fuesen una función, entonces es lo mismo que llamar a find_all() en esos objetos.
Estos dos líneas de código son
equivalentes:  soup .
find_all ( "a" )  soup ( "a" )    Estas dos líneas de código son también equivalentes:  soup .
title ( string = True )      find() ¶  Firma del método: find( name , attrs , recursive , string , **kwargs )  El método find_all() examina todo el documento buscando por
resultados, pero a veces solo quieres encontrar un resultado.
Si sabes que un documento solo tiene una etiqueta <body>, es una
pérdida de tiempo examinar todo el documento buscando más
emparejamientos.
En lugar de pasar limit=1 siempre que se llame
a find_all(),  puedes  usar  el  método  ``find() .
Estas dos líneas
de código son casi equivalentes:  soup .
find ( 'title' )  # <title>The Dormouse's story</title>    La única diferencia es que find_all() devuelve una lista
conteniendo un resultado, y find() devuelve solo el resultado.
Si find_all() no encuentra nada, devuelve una lista vacía.
Si find() no encuentra nada, devuelve None :  print ( soup .
find ( "nosuchtag" ))  # None    ¿Recuerdas el truco de soup.head.title de Navegar usando nombres
de etiquetas ?
Ese truco funciona porque se llama repetidamente a find() :  soup .
find ( "title" )  # <title>The Dormouse's story</title>      find_parents() y find_parent() ¶  Firma del método: find_parents( name , attrs , string , limit , **kwargs )  Firma del método: find_parent( name , attrs , string , **kwargs )  He pasado bastante tiempo cubriendo find_all() y find() .
La API de Beautiful Soup define otros diez métodos para buscar por
el árbol, pero no te asustes.
Cinco de estos métodos son básicamente
iguales a find_all() , y los otros cinco son básicamente
iguales a find() .
La única diferencia reside en qué partes del
árbol buscan.
Primero consideremos find_parents() y find_paren() .
Recuerda
que find_all() y find() trabajan bajando por el árbol,
examinando a los descendientes de una etiqueta.
Estos métodos realizan
lo contrario: trabajan subiendo por el árbol, buscando a las madres
de las etiquetas (o cadenas).
Probémoslos, empezando por una cadena
de caracteres que esté bien enterrada en el documento de «Las tres
hermanas»:  a_string  =  soup .
find_parents ( "p" ,  class_ = "title" )  # []    Una de la tres etiquetas <a> is la madre directa de la cadena
en cuestión, así que nuestra búsqueda la encuentra.
Una de las
tres etiquetas <p> es una madre indirecta de la cadena, y nuestra
búsqueda también la encuentra.
Hay una etiqueta <p> con la clase
CSS «title» en algún sitio del documento, pero no en ninguno
de las madres de la cadena, así que no podemos encontrarla con find_parents() .
Puedes haber deducido la conexión entre find_parent() y find_parents() , y los atributos .parent y .parents mencionados anteriormente.
La conexión es muy fuerte.
Estos
métodos de búsqueda realmente usan .parents para iterar
sobre todas las madres, y comprobar cada una con el filtro
provisto para ver si emparejan.
find_next_siblings() y find_next_sibling() ¶  Firma del método: find_next_siblings( name , attrs , string , limit , **kwargs )  Firma del método: find_next_sibling( name , attrs , string , **kwargs )  Estos métodos usan next_siblings para iterar sobre el resto de los hermanos de un elemento en el
árbol.
El método find_next_siblings() devuelve todos los
hermanos que casen, y find_next_sibling() solo devuelve
el primero de ellos:  first_link  =  soup .
find_next_sibling ( "p" )  # <p class="story">...</p>      find_previous_siblings() y find_previous_sibling() ¶  Firma del método: find_previous_siblings( name , attrs , string , limit , **kwargs )  Firma del método: find_previous_sibling( name , attrs , string , **kwargs )  Estos métodos emplean .previous_siblings para iterar sobre
los hermanos de un elemento que les precede en el árbol.
El método find_previous_siblings() devuelve todos los hermanos que emparejan, y find_previous_sibling() solo devuelve el primero de ellos:  last_link  =  soup .
find_previous_sibling ( "p" )  # <p class="title"><b>The Dormouse's story</b></p>      find_all_next() y find_next() ¶  Firma del método: find_all_next( name , attrs , string , limit , **kwargs )  Firma del método: find_next( name , attrs , string , **kwargs )  Estos métodos usan .next_elements para
iterar sobre cualesquiera etiquetas y cadenas que vayan después
de ella en el documento.
El método find_all_next() devuelve
todos los resultados, y find_next() solo devuelve el primero:  first_link  =  soup .
find_next ( "p" )  # <p class="story">...</p>    En el primer ejemplo, la cadena «Elsie» apareció, aunque estuviese
contenida en la etiqueta <a> desde la que comenzamos.
En el segundo
ejemplo, la última etiqueta <p> en el documento apareció, aunque no
esté en la misma parte del árbol que la etiqueta <a> desde la que
comenzamos.
Para estos métodos, todo lo que importa es que un
elemento cumple con el filtro, y que aparezca en el documento
después del elemento inicial.
find_all_previous() y find_previous() ¶  Firma del método: find_all_previous( name , attrs , string , limit , **kwargs )  Firma del método: find_previous( name , attrs , string , **kwargs )  Estos métodos usan .previous_elements para iterar sobre las etiquetas y cadenas que iban antes en el
documento.
El método find_all_previous() devuelve todos los
resultados, y find_previous() solo devuelve el primero:  first_link  =  soup .
find_previous ( "title" )  # <title>The Dormouse's story</title>    La llamada a find_all_previous("p") encontró el primer
párrafo en el documento (el que tiene la clase=»title»), pero
también encuentra el segundo párrafo, la etiqueta <p> que
contiene la etiqueta <a> con la que comenzamos.
Esto no debería
ser demasiado sorprendente: estamos buscando todas las etiquetas
que aparecen en el documento después de la etiqueta con la que se
comienza.
Una etiqueta <p> que contiene una <a> debe aparecer
antes de la etiqueta <a> que contiene.
Selectores CSS mediante la propiedad .css ¶  Los objetos BeautifulSoup y Tag soportan los selectores
CSS a través de su atributo .css .
El paquete Soup Sieve ,
disponible a través de PyPI como soupsieve , gestiona la implementación real
del selector.
Si instalaste Beautiful Soup mediante pip , Soup Sieve se
instaló al mismo tiempo, así que no tienes que hacer nada adicional.
La documentación de Soup Sieve lista todos los selectores CSS soportados
actualmente , pero
estos son algunos de los básicos.
Puedes encontrar etiquetas:  soup .
select ( "p:nth-of-type(3)" )  # [<p class="story">...</p>]    Encontrar etiquetas dentro de otras etiquetas:  soup .
select ( "html head title" )  # [<title>The Dormouse's story</title>]    Encontrar etiquetas directamente después de otras etiquetas:  soup .
select ( "body > a" )  # []    Encontrar los hijos de etiquetas:  soup .
select ( "#link1 + .sister" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Encontrar etiquetas por su clase CSS:  soup .
select ( "[class~=sister]" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Encontrar etiquetas por su ID:  soup .
select ( "a#link2" )  # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Encontrar etiquetas que casen con cualquier selector que estés en una
lista de selectores:  soup .
select ( "#link1,#link2" )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]    Comprobar la existencia de un atributo:  soup .
select ( 'a[href]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]    Encontrar etiquetas por el valor de un atributo:  soup .
select ( 'a[href*=".com/el"]' )  # [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]    Hay también un método llamado select_one() , que encuentra solo
la primera etiqueta que case con un selector:  soup .
select_one ( ".sister" )  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    Por conveniencia, puedes llamar a select() y select_one() sobre
el objeto BeautifulSoup o Tag , omitiendo la
propiedad .css :  soup .
select_one ( ".sister" )  # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>    El soporte de selectores CSS es conveniente para personas que ya conocen
la sintaxis de los selectores CSS.
Puedes hacer todo esto con la API
de Beautiful Soup.
Si todo lo que necesitas son los selectores CSS, deberías
saltarte Beautiful Soup y analizar el documento con lxml : es mucho más
rápido.
Pero Soup Sieve te permite combinar selectores CSS con la API
de Beautiful Soup.
Características avanzadas de Soup Sieve ¶  Soup Sieve ofrece una API más amplia más allá de los métodos select() y select_one() , y puedes acceder a casi toda esa API a través del
atributo .css de Tag o Beautiful  Soup .
Lo que
sigue es solo una lista de los métodos soportados; ve a la documentación de
Soup Sieve para la documentación
completa.
El método iselect() funciona igualmente que select() , solo que
devuelve un generador en vez de una lista:  [ tag [ 'id' ]  for  tag  in  soup .
iselect ( ".sister" )]  # ['link1', 'link2', 'link3']    El método closest() devuelve la madre más cercana de una Tag dada
que case con un selector CSS, similar al método find_parent() de
Beautiful Soup:  elsie  =  soup .
closest ( "p.story" )  # <p class="story">Once upon a time there were three little sisters; and their names were  #  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,  #  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and  #  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;  #  and they lived at the bottom of a well.</p>    El método match() devuelve un booleano dependiendo de si
una Tag específica casa con un selector o no:  # elsie.css.match("#link1")  True  # elsie.css.match("#link2")  False    El método filter() devuelve un subconjunto de los hijos directos
de una etiqueta que casen con un selector:  [ tag .
filter ( 'a' )]  # ['Elsie', 'Lacie', 'Tillie']    El método escape() formatea los identificadores CSS que de otra
forma serían inválidos:  soup .
escape ( "1-strange-identifier" )  # '\\31 -strange-identifier'      Espacios de nombres en selectores CSS ¶  Si has analizado XML que define espacios de nombres, puedes usarlos
en selectores CSS:  from  bs4  import  BeautifulSoup  xml  =  """<tag xmlns:ns1="http://namespace1/" xmlns:ns2="http://namespace2/">  <ns1:child>I'm in namespace 1</ns1:child>  <ns2:child>I'm in namespace 2</ns2:child>  </tag> """  namespace_soup  =  BeautifulSoup ( xml ,  "xml" )  namespace_soup .
select ( "ns1|child" )  # [<ns1:child>I'm in namespace 1</ns1:child>]    Beautiful Soup intenta usar prefijos de espacios de nombres que tengan
sentido basándose en lo que vio al analizar el documento, pero siempre
puedes indicar tu propio diccionario de abreviaciones:  namespaces  =  dict ( first = "http://namespace1/" ,  second = "http://namespace2/" )  namespace_soup .
select ( "second|child" ,  namespaces = namespaces )  # [<ns1:child>I'm in namespace 2</ns1:child>]      Historia del soporte de selectores CSS ¶  La propiedad .css fue añadida en Beautiful Soup 4.12.0.
Anterior a esta,
solo los métodos convenientes .select() y select_one() se
soportaban.
La integración de Soup Sieve fue añadida en Beautiful Soup 4.7.0.
Versiones
anteriores tenían el método .select() , pero solo los selectores CSS
más comunes eran admitidos.
Modificar el árbol ¶  La mayor fortaleza de Beautiful Soup reside en buscar en el árbol
analizado, pero puedes también modificar el árbol y escribir tus
cambios como un nuevo documento HTML o XML.
Cambiar nombres de etiquetas y atributos ¶  Cubrí esto anteriormente, en Tag.attrs , pero vale la pena
repetirlo.
Puedes renombrar una etiqueta, cambiar el valor de sus
atributos, añadir nuevos atributos, y eliminar atributos:  soup  =  BeautifulSoup ( '<b class="boldest">Extremely bold</b>' ,  'html.parser' )  tag  =  soup .
name  =  "blockquote"  tag [ 'class' ]  =  'verybold'  tag [ 'id' ]  =  1  tag  # <blockquote class="verybold" id="1">Extremely bold</blockquote>  del  tag [ 'class' ]  del  tag [ 'id' ]  tag  # <blockquote>Extremely bold</blockquote>      Modificar .string ¶  Si quieres establecer el .string de una etiqueta a una nueva cadena de
caracteres, los contenidos de la etiqueta se pueden reemplazar con esa cadena:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
tag  # <a href="http://example.com/">New link text.</a>    Ten cuidado: si una etiqueta contiene otras, ellas y todo su contenido
serán destruidos.
append() ¶  Puedes añadir al contenido de una etiqueta con Tag.append() .
Funciona como llamar a .append() en una lista de Python:  soup  =  BeautifulSoup ( "<a>Foo</a>" ,  'html.parser' )  soup .
contents  # ['Foo', 'Bar']      extend() ¶  Desde Beautiful Soup 4.7.0, Tag también soporta un método
llamado .extend() , el cual añade todos los elementos de una lista
a una Tag , en orden:  soup  =  BeautifulSoup ( "<a>Soup</a>" ,  'html.parser' )  soup .
contents  # ['Soup', ''s', ' ', 'on']      NavigableString() y .new_tag() ¶  Si necesitas añadir una cadena a un documento, sin problema–puedes
pasar una cadena de Python a append() , o puedes llamar al constructor
de NavigableString :  from  bs4  import  NavigableString  soup  =  BeautifulSoup ( "<b></b>" ,  'html.parser' )  tag  =  soup .
contents  # ['Hello', ' there']    Si quieres crear un comentario o cualquier otra subclase
de NavigableString , solo llama al constructor:  from  bs4  import  Comment  new_comment  =  Comment ( "Nice to see you."
(Esto es una nueva característica en Beautiful Soup 4.4.0.)
¿Qué ocurre si necesitas crear una etiqueta totalmente nueva?
La mejor
solución es llamar al método de construcción ( factory method ) BeautifulSoup.new_tag() :  soup  =  BeautifulSoup ( "<b></b>" ,  'html.parser' )  original_tag  =  soup .
original_tag  # <b><a href="http://www.example.com">Link text.</a></b>    Solo el primer argumento, el nombre de la etiqueta, es
obligatorio.
insert() ¶  Tag.insert() es justo como Tag.append() , excepto que el nuevo
elemento no necesariamente va al final del .contents de su madre.
Se insertará en la posición numérica que le hayas indicado.
Funciona
como .insert() es una lista de Python:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
contents  # ['I linked to ', 'but did not endorse', <i>example.com</i>]      insert_before() y insert_after() ¶  El método insert_before() inserta etiquetas o cadenas
inmediatamente antes de algo en el árbol analizado:  soup  =  BeautifulSoup ( "<b>leave</b>" ,  'html.parser' )  tag  =  soup .
b  # <b><i>Don't</i>leave</b>    El método insert_after() inserta etiquetas o cadenas
inmediatamente después de algo en el árbol analizado:  div  =  soup .
contents  # [<i>Don't</i>, ' you', <div>ever</div>, 'leave']      clear() ¶  Tag.clear() quita los contenidos de una etiqueta:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  soup .
clear ()  tag  # <a href="http://example.com/"></a>      extract() ¶  PageElement.extract() elimina una etiqueta o una cadena de caracteres
del árbol.
Devuelve la etiqueta o la cadena que fue extraída:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
parent )  # None    En este punto tienes realmente dos árboles analizados: uno anclado en el
objeto BeautifulSoup que usaste para analizar el documento, y
uno anclado en la etiqueta que fue extraída.
Puedes llamar a extract en el hijo del elemento que extrajiste:  my_string  =  i_tag .
parent )  # None  i_tag  # <i></i>      decompose() ¶  Tag.decompose() quita una etiqueta del árbol, y luego lo destruye
completamente y su contenido también :  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
decompose ()  a_tag  # <a href="http://example.com/">I linked to</a>    El comportamiento de una Tag o NavigableString descompuesta
no está definido y no deberías usarlo para nada.
Si no estás seguro si algo
ha sido descompuesto, puedes comprobar su propiedad .decomposed  (nuevo en Beautiful Soup 4.9.0) :  i_tag .
decomposed  # False      replace_with() ¶  PageElement.replace_with() elimina una etiqueta o cadena del árbol,
y lo reemplaza con una o más etiquetas de tu elección:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
,  i_tag )  a_tag  # <a href="http://example.com/">I linked to <b>example</b>.<i>net</i></a>    replace_with() devuelve la etiqueta o cadena que se reemplazó,
así que puedes examinarla o añadirla de nuevo a otra parte del árbol.
La capacidad de pasar múltiples argumentos a replace_with() es nueva
en Beautiful Soup 4.10.0.    wrap() ¶  PageElement.wrap() envuelve un elemento en la etiqueta que especificas.
Devuelve la nueva envoltura:  soup  =  BeautifulSoup ( "<p>I wish I was bold.</p>" ,  'html.parser' )  soup .
new_tag ( "div" ))  # <div><p><b>I wish I was bold.</b></p></div>    Este método es nuevo en Beautiful Soup 4.0.5.    unwrap() ¶  Tag.unwrap() es el opuesto de wrap() .
Reemplaza una
etiqueta con lo que haya dentro de lo que haya en esa etiqueta.
Es bueno para eliminar anotaciones:  markup  =  '<a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  a_tag  =  soup .
unwrap ()  a_tag  # <a href="http://example.com/">I linked to example.com</a>    Como replace_with() , unwrap() devuelve la etiqueta que fue
reemplazada.
smooth() ¶  Tras llamar a un puñado de métodos que modifican el árbol analizado, puedes
acabar con dos o más objetos NavigableString uno al lado del otro.
Beautiful Soup no tiene ningún problema con esto, pero como no puede ocurrir
en un documento recién analizado, puedes no esperar un comportamiento como
el siguiente:  soup  =  BeautifulSoup ( "<p>A one</p>" ,  'html.parser' )  soup .
prettify ())  # <p>  #  A one  #  , a two  # </p>    Puedes llamar a Tag.smooth() para limpiar el árbol analizado consolidando
cadenas adyacentes:  soup .
prettify ())  # <p>  #  A one, a two  # </p>    Este método es nuevo en Beautiful Soup 4.8.0.
Salida ¶   Pretty-printing ¶  El método prettify() convertirá un árbol analizado de Beautiful Soup
en una cadena de caracteres Unicode bien formateado, con una línea
para cada etiqueta y cada cadena:  markup  =  '<html><head><body><a href="http://example.com/">I linked to <i>example.com</i></a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
prettify ())  # <html>  #  <head>  #  </head>  #  <body>  #   <a href="http://example.com/">  #    I linked to  #    <i>  #     example.com  #    </i>  #   </a>  #  </body>  # </html>    Puedes llamar prettify() a alto nivel sobre el objeto BeautifulSoup ,
o sobre cualquiera de sus objetos Tag :  print ( soup .
prettify ())  # <a href="http://example.com/">  #  I linked to  #  <i>  #   example.com  #  </i>  # </a>    Como añade un espacio en blanco (en la forma de saltos de líneas), prettify() cambia el sentido del documento HTML y no debe ser
usado para reformatearlo.
El objetivo de prettify() es ayudarte
a entender visualmente la estructura del documento en el que trabajas.
Non-pretty printing ¶  Si tan solo quieres una cadena, sin ningún formateo adornado,
puedes llamar a str() en un objeto BeautifulSoup , o
sobre una Tag dentro de él:  str ( soup )  # '<html><head></head><body><a href="http://example.com/">I linked to <i>example.com</i></a></body></html>'  str ( soup .
a )  # '<a href="http://example.com/">I linked to <i>example.com</i></a>'    La función str() devuelve una cadena codificada en UTF-8.
Mira Codificaciones para otras opciones.
Puedes también llamar a encode() para obtener un bytestring, y decode() para obtener Unicode.
Formatos de salida ¶  Si le das a Beautiful Soup un documento que contenga entidades HTML
como «&lquot;», serán convertidas a caracteres Unicode:  soup  =  BeautifulSoup ( "&ldquo;Dammit!&rdquo; he said."
Si después conviertes el documento a bytestring, los caracteres Unicode
serán convertidos a UTF-8.
No obtendrás de nuevo las entidades HTML:  soup .
Por defecto, los únicos caracteres que se formatean en la salida son
ampersands y comillas anguladas simples.
Estas se transforman en
«&amp;», «&lt;» y «&gt;», así Beautiful Soup no genera inadvertidamente
HTML o XML inválido:  soup  =  BeautifulSoup ( "<p>The law firm of Dewey, Cheatem, & Howe</p>" ,  'html.parser' )  soup .
a  # <a href="http://example.com/?foo=val1&amp;bar=val2">A link</a>    Puedes cambiar este comportamiento dando un valor al argumento formatter de prettify() , encode() o decode() .
Beautiful Soup reconoce cinco posibles valores para formatter .
El valor por defecto es formatter="minimal" .
Las cadenas solo
serán procesadas lo suficiente como para asegurar que Beautiful Soup
genera HTML/XML válido:  french  =  "<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>"  soup  =  BeautifulSoup ( french ,  'html.parser' )  print ( soup .
prettify ( formatter = "minimal" ))  # <p>  #  Il a dit &lt;&lt;Sacré bleu!&gt;&gt;  # </p>    Si pasas formatter="html" , Beautiful Soup convertirá caracteres
Unicode a entidades HTML cuando sea posible:  print ( soup .
prettify ( formatter = "html" ))  # <p>  #  Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;  # </p>    Si pasas formatter="html5" , es similar a formatter="html" , pero Beautiful Soup omitirá la barra de
cierre en etiquetas HTML vacías como «br»:  br  =  BeautifulSoup ( "<br>" ,  'html.parser' ) .
encode ( formatter = "html5" ))  # b'<br>'    Además, cualquier atributo cuyos valores son la cadena de
caracteres vacía se convertirán en atributos booleanos al
estilo HTML:  option  =  BeautifulSoup ( '<option selected=""></option>' ) .
encode ( formatter = "html5" ))  # b'<option selected></option>'    (Este comportamiento es nuevo a partir de Beautiful Soup 4.10.0.)
Si pasas formatter=None , Beautiful Soup no modificará en absoluto
las cadenas a la salida.
Esta es la opción más rápida, pero puede
ocasionar que Beautiful Soup genere HTML/XML inválido, como en estos
ejemplos:  print ( soup .
encode ( formatter = None ))  # b'<a href="http://example.com/?foo=val1&bar=val2">A link</a>'     Objetos Formatter ¶  Si necesitas un control más sofisticado sobre tu salida, puedes
instanciar uno de las clases formatters de Beautiful Soup y pasar
dicho objeto a formatter .
HTMLFormatter ¶   Usado para personalizar las reglas de formato para documentos HTML.
Aquí está el formatter que convierte cadenas de caracteres a mayúsculas,
como si están en un nodo de texto o en el valor de un atributo:  from  bs4.formatter  import  HTMLFormatter  def  uppercase ( str ):  return  str .
prettify ( formatter = formatter ))  # <a href="HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2">  #  A LINK  # </a>    Este es el formatter que incrementa la sangría cuando se realiza pretty-printing :  formatter  =  HTMLFormatter ( indent = 8 )  print ( link_soup .
XMLFormatter ¶   Usado para personalizar las reglas de formateo para documentos XML.
Escribir tu propio formatter ¶  Crear una subclase a partir de HTMLFormatter p XMLFormatter te dará incluso más control sobre la salida.
Por ejemplo, Beautiful Soup
ordena por defecto los atributos en cada etiqueta:  attr_soup  =  BeautifulSoup ( b '<p z="1" m="2" a="3"></p>' ,  'html.parser' )  print ( attr_soup .
encode ())  # <p a="3" m="2" z="1"></p>    Para detener esto, puedes modificar en la subclase creada
el método Formatter.attributes() , que controla los atributos
que se ponen en la salida y en qué orden.
Esta implementación también
filtra el atributo llamado «m» cuando aparezca:  class  UnsortedAttributes ( HTMLFormatter ):  def  attributes ( self ,  tag ):  for  k ,  v  in  tag .
encode ( formatter = UnsortedAttributes ()))  # <p z="1" a="3"></p>    Una última advertencia: si creas un objeto CData , el texto
dentro de ese objeto siempre se muestra exactamente como aparece, sin
ningún formato .
Beautiful Soup llamará a la función de sustitución de
entidad, por si hubieses escrito una función a medida que cuenta
todas las cadenas en el documento o algo así, pero ignorará el
valor de retorno:  from  bs4.element  import  CData  soup  =  BeautifulSoup ( "<a></a>" ,  'html.parser' )  soup .
[CDATA[one < three]]>  # </a>       get_text() ¶  Si solo necesitas el texto legible dentro de un documento o etiqueta, puedes
usar el método get_text() .
Devuelve todo el texto dentro del documento o
dentro de la etiqueta, como una sola cadena caracteres Unicode:  markup  =  '<a href="http://example.com/"> \n I linked to <i>example.com</i> \n </a>'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
get_text ()  'example.com'    Puedes especificar una cadena que usará para unir los trozos
de texto:  # soup.get_text("|")  ' \n I linked to |example.com| \n '    Puedes indicar a Beautiful Soup que quite los espacios en blanco del
comienzo y el final de cada trozo de texto:  # soup.get_text("|", strip=True)  'I linked to|example.com'    Pero en ese punto puedas querer usar mejor el generador .stripped_strings , y procesar el texto
por tu cuenta:  [ text  for  text  in  soup .
stripped_strings ]  # ['I linked to', 'example.com']    A partir de Beautiful Soup version 4.9.0, cuando lxml o html.parser
se usan, el contenido de las etiquetas <script>, <style>, y <template>
no se consideran texto, ya que esas etiquetas no son parte de la parte
legible del contenido de la página.
A partir de de Beautiful Soup version 4.10.0, puedes llamar a get_text(),
.strings, o .stripped_strings en un objeto NavigableString.
Devolverá
el propio objeto, o nada, así que la única razón para hacerlo es cuando
estás iterando sobre una lista mixta.
Especificar el analizador a usar ¶  Si lo único que necesitas es analizar algún HTML, puedes ponerlo en
el constructor de BeautifulSoup , y probablemente irá bien.
Beautiful Soup elegirá un analizador por ti y analizará los datos.
Pero hay algunos argumentos adicionales que puedes pasar al constructor
para cambiar el analizador que se usa.
El primer argumento del constructor de BeautifulSoup es una cadena
o un gestor de archivos abierto–el marcado que quieres analizar.
El segundo
argumento es cómo quieres que el marcado analizado.
Si no especificas nada, obtendrás el mejor analizador HTML que tengas
instalado.
Beautiful Soup clasifica al analizador de lxml como el mejor,
después el de html5lib, y luego el analizador integrado en Python.
Puedes
sobrescribir esto especificando uno de los siguientes:   El tipo de marcado que quieres analizar.
Actualmente se soportan
«html», «xml», y «html5».
El nombre de la librería del analizador que quieras usar.
Actualmente se
soportan «lxml», «html5lib», y «html.parser» (el analizador HTML integrado
de Python).
La sección Instalar un analizador contraste los analizadores admitidos.
Si no tienes un analizador apropiado instalado, Beautiful Soup ignorará
tu petición y elegirá un analizador diferente.
Ahora mismo, el único
analizador XML es lxml.
Si no tienes lxml instalado, solicitar un
analizador XML no te dará uno, y pedir por «lxml» tampoco funcionará.
Diferencias entre analizadores ¶  Beautiful Soup presenta la misma interfaz que varios analizadores,
pero cada uno es diferente.
Analizadores diferentes crearán
árboles analizados diferentes a partir del mismo documento.
La mayores
diferencias están entre los analizadores HTML y los XML.
Este es un
documento corto, analizado como HTML usando el analizador que viene
con Python:  BeautifulSoup ( "<a><b/></a>" ,  "html.parser" )  # <a><b></b></a>    Como una sola etiqueta <b/> no es HTML válido, html.parser lo convierte a
un par <b><b/>.
Aquí está el mismo documento analizado como XML (correr esto requiere que
tengas instalado lxml).
Debe notarse que la etiqueta independiente
<b/> se deja sola, y que en el documento se incluye una declaración XML
en lugar de introducirlo en una etiqueta <html>:  print ( BeautifulSoup ( "<a><b/></a>" ,  "xml" ))  # <?xml version="1.0" encoding="utf-8"?>  # <a><b/></a>    Hay también diferencias entre analizadores HTML.
Si le das a Beautiful
Soup un documento HTML perfectamente formado, esas diferencias no
importan.
Un analizador será más rápido que otro, pero todos te darán
una estructura de datos que será exactamente como el documento HTML
original.
Pero si el documento no está perfectamente formado, analizadores
diferentes darán diferentes resultados.
A continuación se presenta
un documento corto e incorrecto analizado usando el analizador
HTML de lxml.
Debe considerarse que la etiqueta <a> es envuelta
en las etiquetas <body> y <html>, y que la etiqueta colgada </p>
simplemente se ignora:  BeautifulSoup ( "<a></p>" ,  "lxml" )  # <html><body><a></a></body></html>    Este es el mismo documento analizado usando html5lib:  BeautifulSoup ( "<a></p>" ,  "html5lib" )  # <html><head></head><body><a><p></p></a></body></html>    En lugar de ignorar la etiqueta colgada </p>, html5lib la empareja
con una etiqueta inicial <p>.
html5lib también añade una etiqueta <head>
vacía; lxml no se molesta.
Este es el mismo documento analizado usando el analizador HTML integrado
en Python:  BeautifulSoup ( "<a></p>" ,  "html.parser" )  # <a></a>    Como lxml, este analizador ignora la etiqueta clausura </p>.
A diferencia de html5lib o lxml, este analizador no intenta
crear un documento HTML bien formado añadiendo las etiquetas
<html> o <body>.
Como el documento «<a></p>» es inválido, ninguna de estas técnicas
es la forma “correcta” de gestionarlo.
El analizador de html5lib usa
técnicas que son parte del estándar de HTML5, así que es la que más
se puede aproximar a ser la manera correcta, pero las tres técnicas
son legítimas.
Las diferencias entre analizadores pueden afectar a tu script.
Si
estás planeando en distribuir tu script con otras personas, o
ejecutarlo en varias máquinas, deberías especificar un analizador
en el constructor de BeautifulSoup .
Eso reducirá
las probabilidad que tus usuarios analicen un documento diferentemente
de la manera en la que tú lo analizas.
Codificaciones ¶  Cualquier documento HTML o XML está escrito en una codificación
específica como ASCII o UTF-8.
Pero cuando cargas ese documento en
Beautiful Soup, descubrirás que se convierte en Unicode:  markup  =  "<h1>Sacr \xc3\xa9 bleu!</h1>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
No es magia (seguro que eso sería genial).
Beautiful Soup usa una
sub-librería llamada Unicode, Dammit para detectar la codificación
de un documento y convertirlo a Unicode.
La codificación auto detectada
está disponible con el atributo .original_encoding del objeto Beautiful  Soup :  soup .
original_encoding  'utf-8'    Unicode, Dammit estima correctamente la mayor parte del tiempo, pero
a veces se equivoca.
A veces estima correctamente, pero solo después
de una búsqueda byte a byte del documento que tarda mucho tiempo.
Si ocurre que sabes a priori la codificación del documento, puedes
evitar errores y retrasos pasándola al constructor de BeautifulSoup con from_encoding .
Este es un documento escrito es ISO-8859-8.
El documento es tan corto que
Unicode, Dammit no da en el clave, y lo identifica erróneamente como
ISO-8859-7:  markup  =  b "<h1> \xed\xe5\xec\xf9 </h1>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  print ( soup .
original_encoding )  # iso-8859-7    Podemos arreglarlo pasándole el correcto a from_encoding :  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  from_encoding = "iso-8859-8" )  print ( soup .
original_encoding )  # iso8859-8    Si no sabes cuál es la codificación correcta, pero sabes que Unicode, Dammit
está suponiendo mal, puedes pasarle las opciones mal estimadas con exclude_encodings :  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  exclude_encodings = [ "iso-8859-7" ])  print ( soup .
original_encoding )  # WINDOWS-1255    Windows-1255 no es correcto al 100%, pero esa codificación es
una superconjunto compatible con ISO-8859-8, así que se acerca
lo suficiente.
( exlcude_encodings es una nueva característica
en Beautiful Soup 4.4.0).
En casos raros (normalmente cuando un documento UTF-8 contiene texto
escrito en una codificación completamente diferente), la única manera
para obtener Unicode es reemplazar algunos caracteres con el carácter
Unicode especial «REPLACEMENT CHARACTER» (U+FFFD, �).
Si Unicode, Dammit
necesita hacer esto, establecerá el atributo .contains_replacement_characters a True en el objeto UnicodeDammit o BeautifulSoup .
Esto
te permite saber si la representación Unicode no es una representación
exacta de la original–algún dato se ha perdido.
Si un documento contiene �,
pero contains_replacement_characteres es False , sabrás que �
estaba allí originalmente (como lo está en este párrafo) y no implica
datos perdidos.
Codificación de salida ¶  Cuando escribas completamente un documento desde Beautiful Soup,
obtienes un documento UTF-8, incluso cuando el documento no está en UTF-8
por el que empezar.
Este es un documento escrito con la codificación Latin-1:  markup  =  b '''  <html>  <head>  <meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" />  </head>  <body>  <p>Sacr \xe9 bleu!</p>  </body>  </html>  '''  soup  =  BeautifulSoup ( markup ,  'html.parser' )  print ( soup .
#   </p>  #  </body>  # </html>    Fíjate bien que la etiqueta <meta> ha sido reescrita para reflejar el hecho
de que el documento está ahora en UTF-8.
Si no quieres UTF-8, puedes pasar una codificación a prettify() :  print ( soup .
prettify ( "latin-1" ))  # <html>  #  <head>  #   <meta content="text/html; charset=latin-1" http-equiv="Content-type" />  # ...
   También puedes llamar a encode() sobre el objeto BeautifulSoup , o
cualquier elemento en el objeto, como si fuese una cadena de Python:  soup .
encode ( "utf-8" )  # b'<p>Sacr\xc3\xa9 bleu!</p>'    Cualesquiera caracteres que no puedan ser representados en la codificación
que has elegido se convierten en referencias a entidades numéricas XML.
Este es un documento que incluye el carácter Unicode SNOWMAN:  markup  =  u "<b> \N{SNOWMAN} </b>"  snowman_soup  =  BeautifulSoup ( markup ,  'html.parser' )  tag  =  snowman_soup .
b    El carácter SNOWMAN puede ser parte de un documento UTF-8 (se parece a ☃),
pero no hay representación para ese carácter en ISO-Latin-1 o ASCII,
así que se convierte en «&#9731» para esas codificaciones:  print ( tag .
encode ( "ascii" ))  # b'<b>&#9731;</b>'      Unicode, Dammit ¶  Puedes usar Unicode, Dammit sin usar Beautiful Soup.
Es útil cuando
tienes datos en una codificación desconocida y solo quieres convertirlo
a Unicode:  from  bs4  import  UnicodeDammit  dammit  =  UnicodeDammit ( b " \xc2\xab Sacr \xc3\xa9 bleu!
original_encoding  # 'utf-8'    Los estimaciones de Unicode, Dammit será mucho más precisas si instalas
una de estas librerías de Python: charset-normalizer , chardet ,
o cchardet .
Cuanto más datos le des a Unicode, Dammit, con mayor exactitud
estimará.
Si tienes alguna sospecha sobre las codificaciones que podrían ser, puedes
pasárselas en una lista:  dammit  =  UnicodeDammit ( "Sacr \xe9 bleu!"
original_encoding  # 'latin-1'    Unicode, Dammit tiene dos características especiales que Beautiful Soup no usa.
Comillas inteligentes ¶  Puedes usar Unicode, Dammit para convertir las comillas inteligentes de Microsoft
a entidades HTML o XML:  markup  =  b "<p>I just \x93 love \x94 Microsoft Word \x92 s smart quotes</p>"  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "html" ) .
unicode_markup  # '<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'    Puedes también convertir las comillas inteligentes de Microsoft a comillas ASCII:  UnicodeDammit ( markup ,  [ "windows-1252" ],  smart_quotes_to = "ascii" ) .
unicode_markup  # '<p>I just "love" Microsoft Word\'s smart quotes</p>'    Con suerte encontrarás esta característica útil, pero Beautiful Soup no la usa.
Beautiful Soup prefiere el comportamiento por defecto, el cual es convertir
las comillas inteligentes de Microsoft a caracteres Unicode junto al resto
de cosas:  UnicodeDammit ( markup ,  [ "windows-1252" ]) .
unicode_markup  # '<p>I just “love” Microsoft Word’s smart quotes</p>'      Codificaciones inconsistentes ¶  A veces un documento está mayoritariamente en UTF-8, pero contiene
caracteres Windows-1252 como (de nuevo) comillas inteligentes de Microsoft.
Esto puede ocurrir cuando un sitio web incluye datos de múltiples fuentes.
Puedes usar UnicodeDammit.detwingle() para convertir dicho documento en
puro UTF-8.
Este un ejemplo sencillo:  snowmen  =  ( u " \N{SNOWMAN} "  *  3 )  quote  =  ( u " \N{LEFT DOUBLE QUOTATION MARK} I like snowmen!
encode ( "windows_1252" )    Este documento es un desastre.
Los muñecos de nieve están en UTF-8 y las
comillas están en Windows-1252.
Puedes mostrar los muñecos de nieve o
las comillas, pero no ambos:  print ( doc )  # ☃☃☃�I like snowmen!�  print ( doc .
decode ( "windows-1252" ))  # â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”    Decodificar el documento en UTF-8 provoca un UnicodeDecodeError , y
decodificarlo como Windows-1252 te da un galimatías.
Afortunadamente, UnicodeDammit.detwingle() convertirá la cadena en puro UTF-8,
permitiéndote decodificarlo en Unicode y mostrar el muñeco de nieve
y marcas de comillas simultáneamente:  new_doc  =  UnicodeDammit .
decode ( "utf8" ))  # ☃☃☃“I like snowmen!”    UnicodeDammit.detwingle() solo sabe cómo gestionar Windows-1252 embebido
en UTF-8 (o viceversa, supongo), pero este es el caso más común.
Fíjate que debes saber que debes llamar a UnicodeDammit.detwingle() en tus datos antes de pasarlo a BeautifulSoup o el constructor
de UnicodeDammit .
Beautiful Soup asume que un documento tiene una
sola codificación, la que sea.
Si quieres pasar un documento que contiene
ambas UTF-8 y Windows-1252, es probable que piense que todo el documento
es Windows-1252, y el documento se parecerá a `â˜ƒâ˜ƒâ˜ƒ“I  like  snowmen!” .
UnicodeDammit.detwingle() es nuevo en Beautiful Soup 4.1.0.
Números de línea ¶  Los analizadores de html.parser y html5lib pueden llevar la cuenta
de los lugares en el documento original donde se han encontrado cada etiqueta.
Puedes acceder a esta información con Tag.sourceline (número de línea) y Tag.sourcepos (posición del comienzo de una etiqueta en una línea):  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  for  tag  in  soup .
string )))  # (1, 0, 'Paragraph 1')  # (3, 4, 'Paragraph 2')    Debe destacarse que los dos analizadores entienden cosas ligeramente
diferentes por sourceline y sourcepos .
Para html.parser, estos
números representan la posición del signo «menor» inicial.
Para html5lib,
estos números representan la posición del signo «mayor» final:  soup  =  BeautifulSoup ( markup ,  'html5lib' )  for  tag  in  soup .
string )))  # (2, 0, 'Paragraph 1')  # (3, 6, 'Paragraph 2')    Puedes interrumpir esta característica pasado store_line_numbers=False en el constructor de BeautifulSoup :  markup  =  "<p \n >Paragraph 1</p> \n <p>Paragraph 2</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  store_line_numbers = False )  print ( soup .
sourceline )  # None    Esta característica es nueva en 4.8.1, y los analizadores basados en lxml no la
soportan.
Comparar objetos por igualdad ¶  Beautiful Soup indica que dos objetos NavigableString o Tag son iguales cuando representan al mismo marcado HTML o XML.
En este ejemplo,
las dos etiquetas <b> son tratadas como iguales, aunque están en diferentes
partes del objeto árbol, porque ambas son «<b>pizza</b>»:  markup  =  "<p>I want <b>pizza</b> and more <b>pizza</b>!</p>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  first_b ,  second_b  =  soup .
previous_element )  # False    Si quieres saber si dos variables se refieren a exactamente el mismo
objeto, usa is :  print ( first_b  is  second_b )  # False      Copiar objetos de Beautiful Soup ¶  Puedes usar copy.copy() para crear una copia de cualquier Tag o NavigableString :  import  copy  p_copy  =  copy .
p )  print ( p_copy )  # <p>I want <b>pizza</b> and more <b>pizza</b>!</p>    La copia se considera igual que la original, ya que representa el mismo
marcado que el original, pero no son el mismo objeto:  print ( soup .
p  is  p_copy )  # False    La única diferencia real es que la copia está completamente desconectada
del objeto árbol de Beautiful Soup, como si extract() hubiese sido
llamada sobre ella:  print ( p_copy .
parent )  # None    Esto es porque dos diferentes objetos Tag no pueden ocupar
el mismo espacio al mismo tiempo.
Personalización avanzada del analizador ¶  Beautiful Soup ofrece numerosas vías para personalizar la manera en la que
el analizador trata HTML o XML entrante.
Esta sección cubre las técnicas
de personalizadas usadas más comúnmente.
Analizar solo parte del documento ¶  Digamos que quieres usar Beautiful Soup para observar las etiquetas <a> de un
documento.
Es un malgasto de tiempo y memoria analizar todo el documento y
después recorrerlo una y otra vez buscando etiquetas <a>.
Sería mucho más
rápido ignorar todo lo que no sea una etiqueta <a> desde el principio.
La clase SoupStrainer te permite elegir qué partes de un
documento entrante se analizan.
Tan solo crea un SoupStrainer y
pásalo al constructor de BeautifulSoup en el argumento parse_only .
(Debe notarse que esta característica no funcionará si estás usando el
analizador de html5lib .
Si usas html5lib, todo el documento será analizado,
no importa el resto.
Esto es porque html5lib constantemente reorganiza el
árbol analizado conforme trabaja, y si alguna parte del documento no
consigue introducirse en el árbol analizado, se quedará colgado.
Para evitar
confusión en los ejemplos más abajo forzaré a Beautiful Soup a que use
el analizador integrado de Python).
SoupStrainer ¶   La clase SoupStrainer toma los mismos argumentos que un típico
método de Buscar en el árbol : name , attrs , string , y **kwargs .
Estos son tres objetos SoupStrainer :  from  bs4  import  SoupStrainer  only_a_tags  =  SoupStrainer ( "a" )  only_tags_with_id_link2  =  SoupStrainer ( id = "link2" )  def  is_short_string ( string ):  return  string  is  not  None  and  len ( string )  <  10  only_short_strings  =  SoupStrainer ( string = is_short_string )    Voy a traer de nuevo el documento de «Las tres hermanas» una vez más,
y veremos cómo parece el documento cuando es analizado con estos
tres objetos SoupStrainer :  html_doc  =  """<html><head><title>The Dormouse's story</title></head>  <body>  <p class="title"><b>The Dormouse's story</b></p>  <p class="story">Once upon a time there were three little sisters; and their names were  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;  and they lived at the bottom of a well.</p>  <p class="story">...</p>  """  print ( BeautifulSoup ( html_doc ,  "html.parser" ,  parse_only = only_a_tags ) .
prettify ())  # Elsie  # ,  # Lacie  # and  # Tillie  # ...
 #    Puedes también pasar un SoupStrainer en cualquiera de los métodos
cubiertos en Buscar en el árbol .
Esto probablemente no sea terriblemente útil,
pero pensé en mencionarlo:  soup  =  BeautifulSoup ( html_doc ,  'html.parser' )  soup .
find_all ( only_short_strings )  # ['\n\n', '\n\n', 'Elsie', ',\n', 'Lacie', ' and\n', 'Tillie',  #  '\n\n', '...', '\n']      Personalizar atributos multivaluados ¶  En un documento HTML, a un atributo como class se le da una lista
de valores, y a un atributo como id se le da un solo valor, porque
la especificación de HTML trata a esos atributos de manera diferente:  markup  =  '<a class="cls1 cls2" id="id1 id2">'  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
a [ 'id' ]  # 'id1 id2'    Puedes interrumpir esto pasando multi_values_attributes=None .
Entonces
a todos los atributos se les dará un solo valor:  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  multi_valued_attributes = None )  soup .
a [ 'id' ]  # 'id1 id2'    Puedes personalizar este comportamiento un poco pasando un diccionario
a multi_values_attributes .
Si lo necesitas, échale un vistazo a HTMLTreeBuilder.DEFAULT_CDATA_LIST_ATTRIBUTES para ver la configuración
que Beautiful Soup usa por defecto, que está basada en la especificación
HTML.
(Esto es una nueva característica en Beautiful Soup 4.8.0).
Gestionar atributos duplicados ¶  Cuando se use el analizador de html.parser , puedes usar
el argumento del constructor on_duplicate_attribute para personalizar
qué hace Beautiful Soup cuando encuentra una etiqueta que define el mismo
atributo más de una vez:  markup  =  '<a href="http://url1/" href="http://url2/">'    El comportamiento por defecto es usar el último valor encontrado en la
etiqueta:  soup  =  BeautifulSoup ( markup ,  'html.parser' )  soup .
a [ 'href' ]  # http://url2/    Con on_duplicate_attribute='ignore' puedes indicar a Beautiful Soup que
use el primer valor encontrado e ignorar el resto:  soup  =  BeautifulSoup ( markup ,  'html.parser' ,  on_duplicate_attribute = 'ignore' )  soup .
a [ 'href' ]  # http://url1/    (lxml y html5lib siempre lo hacen así; su comportamiento no puede ser
configurado desde Beautiful Soup.)
Si necesitas más, puedes pasar una función que sea llamada en cada valor duplicado:  def  accumulate ( attributes_so_far ,  key ,  value ):  if  not  isinstance ( attributes_so_far [ key ],  list ):  attributes_so_far [ key ]  =  [ attributes_so_far [ key ]]  attributes_so_far [ key ] .
a [ 'href' ]  # ["http://url1/", "http://url2/"]    (Esto es una nueva característica en Beautiful Soup 4.9.1.)
Instanciar subclases personalizadas ¶  Cuando un analizador indica a Beautiful Soup sobre una etiqueta o una cadena,
Beautiful Soup instanciará un objeto Tag o NavigableString para contener esa información.
En lugar de ese comportamiento por defecto,
puedes indicar a Beautiful Soup que instancia subclases de Tag o NavigableString , subclases que defines con comportamiento
personalizado:  from  bs4  import  Tag ,  NavigableString  class  MyTag ( Tag ):  pass  class  MyString ( NavigableString ):  pass  markup  =  "<div>some text</div>"  soup  =  BeautifulSoup ( markup ,  'html.parser' )  isinstance ( soup .
string ,  MyString )  # True    Esto puede ser útil cuando se incorpore Beautiful Soup en un framework de pruebas.
(Esto es una nueva característica de Beautiful Soup 4.8.1.)
Resolución de problemas ¶   diagnose() ¶  Si estás teniendo problemas para entender qué hace Beautiful Soup a un
documento, pasa el documento a la función diagnose() .
(Nuevo en
Beautiful Soup 4.2.0) Beautiful Soup imprimirá un informe mostrándote
cómo manejan el documento diferentes analizadores, y te dirán si
te falta un analizador que Beautiful Soup podría estar usando:  from  bs4.diagnose  import  diagnose  with  open ( "bad.html" )  as  fp :  data  =  fp .
# Found lxml version 2.3.2.0  #  # Trying to parse your data with html.parser  # Here's what html.parser did with the document:  # ...
   Tan solo mirando a la salida de diagnose() puede mostrate cómo resolver
el problema.
Incluso si no, puedes pegar la salida de diagnose() cuando pidas ayuda.
Errores analizando un documento ¶  Hay dos tipos diferentes de errores de análisis.
Hay veces en que
se queda colgado, donde le das a Beautiful Soup un documento y
lanza una excepción, normalmente un HTMLParser.HTMLParseError .
Y hay
comportamientos inesperados, donde un árbol analizado de Beautiful Soup
parece muy diferente al documento usado para crearlo.
Casi ninguno de estos problemas resultan ser problemas con Beautiful Soup.
Esto no es porque Beautiful Soup sea una increíble y bien escrita pieza
de software.
Es porque Beautiful Soup no incluye ningún código de
análisis.
En lugar de eso, depende de análisis externos.
Si un analizador
no está funcionando en un documento concreto, la mejor solución es probar
con otro analizador.
Échale un vistazo a Instalar un analizador para
detalles y una comparativa de analizadores.
Los errores de análisis más comunes son HTMLParser.HTMLParseError:  malformed  start  tag y HTMLParser.HTMLParseError:  bad  end  tag .
Ambos son generados por la librería del analizador HTML
incluido en Python, y la solución es instalar lxml o html5lib.
El comportamiento inesperado más común es que no puedas encontrar
una etiqueta que sabes que está en el documento.
La viste llegar, pero find_all() devuelve [] o find() devuelve None .
Esto
es otro problema común con el analizador HTML integrado en Python, el cual
a veces omite etiquetas que no entiende.
De nuevo, la mejor solución es instalar lxml o html5lib.
.
Problemas de incompatibilidad de versiones ¶   SyntaxError:  Invalid  syntax (on the line ROOT_TAG_NAME  =  '[document]' ): Causado por ejecutar una version antigua de Beautiful
Soup de Python 2 bajo Python 3, sin convertir el código.
ImportError:  No  module  named  HTMLParser - Causado por ejecutar
una version antigua de Beautiful Soup de Python 2 bajo Python 3.
ImportError:  No  module  named  html.parser - Causado por ejecutar
una version de Beautiful Soup de Python 3 bajo Python 2.
ImportError:  No  module  named  BeautifulSoup - Causado por ejecutar
código de Beautiful Soup 3 en un sistema que no tiene BS3 instalado.
O
al escribir código de Beautiful Soup 4 sin saber que el nombre del paquete
se cambió a bs4 .
ImportError:  No  module  named  bs4 - Causado por ejecutar código de
Beautiful Soup 4 en un sistema que no tiene BS4 instalado.
Analizar XML ¶  Por defecto, Beautiful Soup analiza documentos HTML.
Para analizar
un documento como XML, pasa «xml» como el segundo argumento al
constructor BeautifulSoup :  soup  =  BeautifulSoup ( markup ,  "xml" )    Necesitarás tener lxml instalado .
Otros problemas de análisis ¶   Si tu script funciona en un ordenador pero no en otro, o en un
entorno virtual pero no en otro, o fuera del entorno virtual
pero no dentro, es probable porque los dos entornos tienen
diferentes librerías de analizadores disponibles.
Por ejemplo,
puedes haber desarrollado el script en un ordenador que solo
tenga html5lib instalado.
Mira Diferencias entre analizadores por qué esto importa, y solucionar el problema especificando una
librería de análisis en el constructor de Beautiful  Soup .
Porque las etiquetas y atributos de HTML son sensibles a mayúsculas
y minúsculas ,
los tres analizadores HTML convierten los nombres de las etiquetas y
atributos a minúscula.
Esto es, el marcado <TAG></TAG> se convierte
a <tag></tag>.
Si quieres preservar la mezcla entre minúscula y
mayúscula o mantener las mayúsculas en etiquetas y atributos,
necesitarás analizar el documento como XML.
Diversos ¶   UnicodeEncodeError:  'charmap'  codec  can't  encode  character  '\xfoo'  in  position  bar (o cualquier otro UnicodeEncodeError ) - Este problema aparece principalmente
en dos situaciones.
Primero, cuando intentas mostrar un carácter
Unicode que tu consola no sabe cómo mostrar (mira esta página en la
wiki de Python ).
Segundo,
cuando estás escribiendo en un archivo y pasas un carácter Unicode
que no se soporta en tu codificación por defecto.
En este caso,
la solución más simple es codificar explícitamente la cadena Unicode
en UTF-8 con u.encode("utf8") .
KeyError:  [attr] - Causado por acceder a tag['attr'] cuando
la etiqueta en cuestión no define el atributo 'attr' .
Los
errores más comunes son KeyError:  'href' y KeyError:  'class .
Usa tag.get('attr') si no estás seguro si attr está definido,
tal y como harías con un diccionario de Python.
AttributeError:  'ResultSet'  object  has  no  attribute  'foo' - Esto
normalmente ocurre cuando esperas que find_all() devuelva
una sola etiqueta o cadena.
Pero find_all() devuelve una lista de etiquetas y cadenas–un objeto ResultSet .
Tienes que
iterar sobre la lista y comprobar el .foo de cada uno, O, si solo
quieres un resultado, tienes que usar find() en lugar de find_all() .
AttributeError:  'NoneType'  object  has  no  attribute  'foo' - Esto
normalmente ocurre porque llamaste a find() y después intentaste
acceder al atributo .foo del resultado.
Pero en tu caso, find() no encontró nada, así que devolvió None , en lugar de devolver
una etiqueta o una cadena de caracteres.
Necesitas averiguar por qué find() no está devolviendo nada.
AttributeError:  'NavigableString'  object  has  no  attribute  'foo' - Esto ocurre normalmente porque estás tratando una
cadena de caracteres como si fuese una etiqueta.
Puedes estar iterando
sobre una lista, esperando que tan solo contenga etiquetas, pero en
realidad contiene tanto etiquetas como cadenas.
Mejorar el rendimiento ¶  Beautiful Soup nunca será tan rápido como los analizadores en los que
se basa.
Si el tiempo de respuesta es crítico, si estás pagando por
tiempo de uso por hora, o si hay alguna otra razón por la que el tiempo
de computación es más valioso que el tiempo del programador, deberías
olvidarte de Beautiful Soup y trabajar directamente sobre lxml .
Dicho esto, hay cosas que puedes hacer para aumentar la velocidad de
Beautiful Soup.
Si no estás usando lxml como el analizador que hay
por debajo, mi consejo es que empieces a usarlo .
Beautiful Soup analiza documentos significativamente más rápido usando
lxml que usando html.parser o html5lib.
Puedes aumentar la velocidad de detección de codificación significativamente
instalando la librería cchardet .
Analizar solo parte del documento no te ahorrará mucho tiempo de análisis, pero puede
ahorrar mucha memoria, y hará que buscar en el documento sea mucho más rápido.
Traducir esta documentación ¶  Nuevas traducciones de la documentación de Beautiful Soup se agradecen
enormemente.
Las traducciones deberían estar bajo la licencia del MIT, tal
y como están Beautiful Soup y su documentación en inglés.
Hay dos maneras para que tu traducción se incorpore a la base de código
principal y al sitio de Beautiful Soup:   Crear una rama del repositorio de Beautiful Soup, añadir tus
traducciones, y proponer una fusión ( merge ) con la rama principal, lo
mismo que se haría con una propuesta de código del código fuente.
Enviar un mensaje al grupo de discusión de Beautiful Soup con un
enlace a tu traducción, o adjuntar tu traducción al mensaje.
Utiliza la traducción china o portugués-brasileño como tu modelo.
En
particular, por favor, traduce el archivo fuente doc/source/index.rst ,
en vez de la versión HTML de la documentación.
Esto hace posible que la
documentación se pueda publicar en una variedad de formatos, no solo HTML.
Beautiful Soup 3 ¶  Beautiful Soup 3 es la serie de lanzamientos anterior, y no está siendo
activamente desarrollada.
Actualmente está empaquetada con las
distribuciones de Linux más grandes:  $  apt - get  install  python - beautifulsoup  También está publicada a través de PyPI como BeautifulSoup .
:  $  easy_install  BeautifulSoup  $  pip  install  BeautifulSoup  También puedes descargar un tarball de Beautiful Soup 3.2.0 .
Si ejecutaste easy_install  beautifulsoup o easy_install  BeautifulSoup ,
pero tu código no funciona, instalaste por error Beautiful Soup 3.
Necesitas
ejecutar easy_install  beautifulsoup4 .
La documentación de Beautiful Soup 3 está archivada online .
Actualizar el código a BS4 ¶  La mayoría del código escrito con Beautiful Soup 3 funcionará
con Beautiful Soup 4 con un cambio simple.
Todo lo que debes hacer
es cambiar el nombre del paquete de BeautifulSoup a bs4 .
Así que esto:  from  BeautifulSoup  import  BeautifulSoup    se convierte en esto:  from  bs4  import  BeautifulSoup     Si obtienes el ImportError «No module named BeautifulSoup`», tu
problema es que estás intentando ejecutar código de Beautiful Soup 3,
pero solo tienes instalado Beautiful Soup 4.
Si obtienes el ImportError «No module named bs4», tu problema
es que estás intentando ejecutar código Beautiful Soup 4, pero solo
tienes Beautiful Soup 3 instalado.
Aunque BS4 es mayormente compatible con la versión anterior BS3, la
mayoría de sus métodos han quedado obsoletos y dados nuevos nombres
para que cumplan con PEP 8 .
Hay muchos otros renombres y cambios, y algunos de ellos rompen
con la compatibilidad hacia atrás.
Esto es todo lo que necesitarás saber para convertir tu código y hábitos BS3 a
BS4:   Necesitas un analizador ¶  Beautiful Soup 3 usaba el SGMLParser de Python, un módulo que
fue obsoleto y quitado en Python 3.0.
Beautiful Soup 4 usa html.parser por defecto, pero puedes conectar lxml o html5lib
y usar esos.
Mira Instalar un analizador para una comparación.
Como html.parser no es el mismo analizador que SGMLParser ,
podrías encontrarte que Beautiful Soup 4 te de un árbol analizado
diferente al que te da Beautiful Soup 3 para el mismo marcado.
Si
cambias html.parser por lxml o html5lib, puedes encontrarte
que el árbol analizado también cambia.
Si esto ocurre, necesitarás
actualizar tu código de scraping para gestionar el nuevo árbol.
Nombre de los métodos ¶   renderContents -> encode_contents  replaceWith -> replace_with  replaceWithChildren -> unwrap  findAll -> find_all  findAllNext -> find_all_next  findAllPrevious -> find_all_previous  findNext -> find_next  findNextSibling -> find_next_sibling  findNextSiblings -> find_next_siblings  findParent -> find_parent  findParents -> find_parents  findPrevious -> find_previous  findPreviousSibling -> find_previous_sibling  findPreviousSiblings -> find_previous_siblings  getText -> get_text  nextSibling -> next_sibling  previousSibling -> previous_sibling   Algunos argumentos del constructor de Beautiful Soup fueron renombrados
por la misma razón:   BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...)  BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...)   Renombré un método para compatibilidad con Python 3:   Tag.has_key() -> Tag.has_attr()   Renombré un atributo para usar terminología más precisa:   Tag.isSelfClosing -> Tag.is_empty_element   Renombré tres atributos para evitar usar palabras que tienen un significado
especial en Python.
A diferencia de otros, estos cambios no soportan compatibilidad hacia atrás .
Si usaste estos atributos en BS3, tu código
se romperá en BS4 hasta que lo cambies.
UnicodeDammit.unicode -> UnicodeDammit.unicode_markup  Tag.next -> Tag.next_element  Tag.previous -> Tag.previous_element   Estos métodos sobras desde la API de Beautiful Soup 2.
Han quedado
obsoletos desde 2006, y no deberían usarse en absoluto:   Tag.fetchNextSiblings  Tag.fetchPreviousSiblings  Tag.fetchPrevious  Tag.fetchPreviousSiblings  Tag.fetchParents  Tag.findChild  Tag.findChildren     Generadores ¶  Le di a los generadores nombres que cumplan con PEP 8, y se transformaron
en propiedades:   childGenerator() -> children  nextGenerator() -> next_elements  nextSiblingGenerator() -> next_siblings  previousGenerator() -> previous_elements  previousSiblingGenerator() -> previous_siblings  recursiveChildGenerator() -> descendants  parentGenerator() -> parents   Así que en lugar de esto:  for  parent  in  tag .
parentGenerator ():  ...
   Puedes escribir esto:  for  parent  in  tag .
parents :  ...
   (Pero el código antiguo seguirá funcionando).
Algunos de los generadores solían devolver None después de que hayan
terminado, y después paran.
Eso era un error.
Ahora el generador tan solo
para.
Hay dos nuevos generadores, .strings y .stripped_strings .
.strings devuelve objetos NavigableString,
y .stripped_strings devuelve cadenas de Python cuyos espacios
en blanco al comienzo y al final han sido quitados.
XML ¶  Ya no hay una clase BeautifulStoneSoup para analizar XML.
Para
analizar XML pasas «xml» como el segundo argumento del constructor
de BeautifulSoup .
Por la misma razón, el constructor
de BeautifulSoup ya no reconoce el argumento isHTML .
La gestión de Beautiful Soup sobre las etiquetas XML sin elementos ha sido
mejorada.
Previamente cuando analizabas XML tenías que indicar
explícitamente qué etiquetas eran consideradas etiquetas sin elementos.
El argumento selfClosingTags al constructor ya no se reconoce.
En lugar de ello, Beautiful Soup considera cualquier etiqueta vacía como
una etiqueta sin elementos.
Si añades un hijo a una etiqueta sin elementos,
deja de ser una etiqueta sin elementos.
Entidades ¶  Una entidad HTML o XML entrante siempre se convierte al correspondiente
carácter Unicode.
Beautiful Soup 3 tenía varias formas solapadas para
gestionar entidades, las cuales se han eliminado.
El constructor de BeautifulSoup ya no reconoce los argumentos smartQuotesTo o convertEntities ( Unicode, Dammit aún tiene smart_quotes_to ,
pero por defecto ahora transforma las comillas inteligentes a Unicode).
Las constantes HTML_ENTITIES , XML_ENTITIES , y XHTML_ENTITIES han sido eliminadas, ya que configuran una característica (transformando
algunas pero no todas las entidades en caracteres Unicode) que ya no
existe.
Si quieres volver a convertir caracteres Unicode en entidades HTML
a la salida, en lugar de transformarlos a caracteres UTF-8, necesitas
usar un *formatter* de salida .
Otro ¶  Tag.string ahora funciona recursivamente.
Si una
etiqueta A contiene una sola etiqueta B y nada más, entonces
A.string es el mismo que B.string (Antes, era None ).
Los atributos multivaluados como class tienen listas de cadenas
de caracteres como valores, no cadenas.
Esto podría afectar la manera
en la que buscas por clases CSS.
Objetos Tag ahora implementan el método __hash__ , de tal
manera que dos objetos Tag se consideran iguales si generan
el mismo marcado.
Esto puede cambiar el comportamiento de tus scripts
si insertas los objetos Tag en un diccionario o conjunto.
Si pasas a unos de los métodos find* una cadena y
un argumento específico de una etiqueta como name , Beautiful
Soup buscará etiquetas que casen con tu criterio específico de la etiqueta
y cuyo Tag.string case con tu valor para la cadena .
No encontrará las cadenas mismas.
Anteriormente, Beautiful Soup ignoraba el
argumento específico de la etiqueta y buscaba por cadenas de caracteres.
El constructor de Beautiful  Soup ya no reconoce el argumento markupMassage .
Es ahora responsabilidad del analizador gestionar el marcado
correctamente.
Los analizadores alternativos, que rara vez se utilizaban, como ICantBelieveItsBeautifulSoup y BeautifulSOAP se han eliminado.
Ahora es decisión del analizador saber cómo gestionar marcado ambiguo.
El método prettify() ahora devuelve una cadena Unicode, no un bytestring.
Tabla de contenido   Documentación de Beautiful Soup  Cómo conseguir ayuda    Inicio rápido  Instalar Beautiful Soup  Instalar un analizador    Haciendo la sopa  Tipos de objetos  Tag  Tag.name  Tag.attrs    NavigableString  BeautifulSoup  Cadenas especiales  Comment  Para documentos HTML  Stylesheet  Script  Template    Para documentos XML  Declaration  Doctype  CData  ProcessingInstruction        Navegar por el árbol  Bajar  Navegar usando nombres de etiquetas  .contents y .children  .descendants  .string  .strings y stripped_strings    Subir  .parent  .parents    Hacia los lados  .next_sibling y .previous_sibling  .next_siblings y .previous_siblings    Hacia delante y hacia atrás  .next_element y .previous_element  .next_elements y .previous_elements      Buscar en el árbol  Tipos de filtros  Una cadena  Una expresión regular  Una lista  True  Una función    find_all()  El argumento name  El argumento palabras-clave  Buscando por clase CSS  El argumento string  El argumento``limit``  El argumento recursive    Llamar a una etiqueta es como llamar a find_all()  find()  find_parents() y find_parent()  find_next_siblings() y find_next_sibling()  find_previous_siblings() y find_previous_sibling()  find_all_next() y find_next()  find_all_previous() y find_previous()  Selectores CSS mediante la propiedad .css  Características avanzadas de Soup Sieve  Espacios de nombres en selectores CSS  Historia del soporte de selectores CSS      Modificar el árbol  Cambiar nombres de etiquetas y atributos  Modificar .string  append()  extend()  NavigableString() y .new_tag()  insert()  insert_before() y insert_after()  clear()  extract()  decompose()  replace_with()  wrap()  unwrap()  smooth()    Salida  Pretty-printing  Non-pretty printing  Formatos de salida  Objetos Formatter  HTMLFormatter  XMLFormatter    Escribir tu propio formatter    get_text()    Especificar el analizador a usar  Diferencias entre analizadores    Codificaciones  Codificación de salida  Unicode, Dammit  Comillas inteligentes  Codificaciones inconsistentes      Números de línea  Comparar objetos por igualdad  Copiar objetos de Beautiful Soup  Personalización avanzada del analizador  Analizar solo parte del documento  SoupStrainer    Personalizar atributos multivaluados  Gestionar atributos duplicados  Instanciar subclases personalizadas    Resolución de problemas  diagnose()  Errores analizando un documento  Problemas de incompatibilidad de versiones  Analizar XML  Otros problemas de análisis  Diversos  Mejorar el rendimiento    Traducir esta documentación  Beautiful Soup 3  Actualizar el código a BS4  Necesitas un analizador  Nombre de los métodos  Generadores  XML  Entidades  Otro         Esta página   Mostrar el código     Búsqueda rápida               Navegación    índice   módulos |  documentación de Beautiful Soup - 4.12.0 »  Documentación de Beautiful Soup    © Copyright 2004-2024, Leonard Richardson.
Creado usando Sphinx 7.2.6.
Navigation    index   next |  Beautiful Soup 4.9.0 documentation »  Beautiful Soup на русском языке         Beautiful Soup на русском языке ¶  Переведено на русский authoress .
Оглавление:   Документация Beautiful Soup  Техническая поддержка    Быстрый старт  Установка Beautiful Soup  Проблемы после установки  Установка парсера    Приготовление супа  Виды объектов  Tag  NavigableString  BeautifulSoup  Комментарии и другие специфичные строки    Навигация по дереву  Проход сверху вниз  Проход снизу вверх  Перемещение вбок  Проход вперед и назад    Поиск по дереву  Виды фильтров  find_all()  Вызов тега похож на вызов find_all()  find()  find_parents() и find_parent()  find_next_siblings() и find_next_sibling()  find_previous_siblings() и find_previous_sibling()  find_all_next() и find_next()  find_all_previous() и find_previous()  Селекторы CSS    Изменение дерева  Изменение имен тегов и атрибутов  Изменение .string  append()  extend()  NavigableString() и .new_tag()  insert()  insert_before() и insert_after()  clear()  extract()  decompose()  replace_with()  wrap()  unwrap()  smooth()    Вывод  Красивое форматирование  Без красивого форматирования  Средства форматирования вывода  get_text()    Указание парсера  Различия между парсерами    Кодировки  Кодировка вывода  Unicode, Dammit    Нумерация строк  Проверка объектов на равенство  Копирование объектов Beautiful Soup  Разбор части документа  SoupStrainer    Устранение неисправностей  diagnose()  Ошибки при разборе документа  Проблемы несоответствия версий  Разбор XML  Другие проблемы с парсерами  Прочие ошибки  Повышение производительности    Beautiful Soup 3  Перенос кода на BS4    Перевод документации  Об этом переводе              Next topic  Документация Beautiful Soup    This Page   Show Source     Quick search               Navigation    index   next |  Beautiful Soup 4.9.0 documentation »  Beautiful Soup на русском языке    © Copyright 2004-2020, Leonard Richardson.
Index of /software/BeautifulSoup/bs4/download/4.0  Name Last modified Size Description Parent Directory  -  beautifulsoup4-4.0.0b3.tar.gz 2012-02-03 21:40 44K  beautifulsoup4-4.0.0b4.tar.gz 2012-02-08 15:32 104K  beautifulsoup4-4.0.0b5.tar.gz 2012-02-09 21:28 107K  beautifulsoup4-4.0.0b6.tar.gz 2012-02-16 13:28 108K  beautifulsoup4-4.0.0b7.tar.gz 2012-02-23 13:13 107K  beautifulsoup4-4.0.0b8.tar.gz 2012-02-24 15:49 110K  beautifulsoup4-4.0.0b9.tar.gz 2012-02-28 15:37 110K  beautifulsoup4-4.0.0b10.tar.gz 2012-03-02 13:34 114K  beautifulsoup4-4.0.1.tar.gz 2012-03-14 14:40 115K  beautifulsoup4-4.0.2.tar.gz 2012-03-26 13:51 115K  beautifulsoup4-4.0.3.tar.gz 2012-04-03 14:34 116K  beautifulsoup4-4.0.4.tar.gz 2012-04-16 15:18 118K  beautifulsoup4-4.0.5.tar.gz 2012-04-27 14:17 121K  beautifulsoup4-4.1.0.tar.gz 2012-05-29 17:31 126K   Apache/2.4.18 (Ubuntu) OpenSSL/1.0.2g mod_wsgi/4.3.0 Python/2.7.12 Server at www.crummy.com Port 443   Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
Menu Like the tool?
Help making it better!
Your donation helps!
lxml lxml Introduction Support the project Documentation Download Mailing list Bug tracker License Old Versions Docutils System Messages Project income report Legal Notice for Donations Why lxml?
lxml.html Parsing HTML HTML Element Methods Running HTML doctests Creating HTML with the E-factory Working with links Forms HTML Diff Examples lxml.cssselect The CSSSelector class The cssselect method Supported Selectors Namespaces BeautifulSoup Parser Parsing with the soupparser Entity handling Using soupparser as a fallback Using only the encoding detection html5lib Parser Differences to regular HTML parsing Function Reference Extending lxml Document loading and URL resolving XML Catalogs URI Resolvers Document loading in context I/O access control in XSLT Python extensions for XPath and XSLT XPath Extension functions XSLT extension elements Using custom Element classes in lxml Background on Element proxies Element initialization Setting up a class lookup scheme Generating XML with custom classes Implementing namespaces Sax support Building a tree from SAX events Producing SAX events from an ElementTree or Element Interfacing with pulldom/minidom The public C-API of lxml.etree Passing generated trees through Python Writing external modules in Cython Writing external modules in C Developing lxml How to build lxml from source Cython Github, git and hg Building the sources Running the tests and reporting errors Building an egg or wheel Building lxml on MacOS-X Static linking on Windows Building Debian packages from SVN sources How to read the source of lxml What is Cython?
Where to start?
lxml.etree Python modules lxml.objectify lxml.html Release Changelog Credits Main contributors Special thanks goes to: Sitemap Like the tool?
lxml - XML and HTML with Python   » lxml takes all the pain out of XML.
«  Stephan Richter   lxml is the most feature-rich
and easy-to-use library
for processing XML and HTML
in the Python language.
Introduction  The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt .
It is unique in that it combines the speed and
XML feature completeness of these libraries with the simplicity of a
native Python API, mostly compatible but superior to the well-known ElementTree API.
The latest release works with all CPython versions
from 3.6 to 3.12.
See the introduction for more information about
background and goals of the lxml project.
Support the project  lxml has been downloaded from the Python Package Index millions of times and is also available directly in many package
distributions, e.g.
for Linux or macOS.
Most people who use lxml do so because they like using it.
You can show us that you like it by blogging about your experience
with it and linking to the project website.
If you are using lxml for your work and feel like giving a bit of
your own benefit back to support the project, consider sending us
money through GitHub Sponsors, Tidelift or PayPal that we can use
to buy us free time for the maintenance of this great library, to
fix bugs in the software, review and integrate code contributions,
to improve its features and documentation, or to just take a deep
breath and have a cup of tea every once in a while.
Please read the Legal Notice below, at the bottom of this page.
Thank you for your support.
Support lxml through GitHub Sponsors  via a Tidelift subscription  or via PayPal:   Please contact Stefan Behnel for other ways to support the lxml project,
as well as commercial consulting, customisations and trainings on lxml and
fast Python XML processing.
Note that we are not accepting donations in crypto currencies.
Much of the development and hosting for lxml is done in a carbon-neutral way
or with compensated and very low emissions.
Crypto currencies do not fit into that ambition.
AppVeyor and GitHub Actions support the lxml project with their build and CI servers.
Jetbrains supports the lxml project by donating free licenses of their PyCharm IDE .
Another supporter of the lxml project is COLOGNE Webdesign .
Documentation  The HTML documentation from this web site is part of
the normal source download .
Tutorials:  the lxml.etree tutorial for XML processing  John Shipman's tutorial on Python XML processing with lxml  Fredrik Lundh's tutorial for ElementTree    ElementTree:  ElementTree API  compatibility and differences of lxml.etree  ElementTree performance characteristics and comparison    lxml.etree:  lxml.etree specific API documentation  the generated API documentation as a reference  parsing and validating XML  XPath and XSLT support  Python XPath extension functions for XPath and XSLT  custom XML element classes for custom XML APIs (see EuroPython 2008 talk )  a SAX compliant API for interfacing with other XML tools  a C-level API for interfacing with external C/Cython modules    lxml.objectify:  lxml.objectify API documentation  a brief comparison of objectify and etree     lxml.etree follows the ElementTree API as much as possible, building
it on top of the native libxml2 tree.
If you are new to ElementTree,
start with the lxml.etree tutorial for XML processing .
See also the
ElementTree compatibility overview and the ElementTree performance page comparing lxml to the original ElementTree and cElementTree implementations.
Right after the lxml.etree tutorial for XML processing and the ElementTree documentation, the next place to look is the lxml.etree
specific API documentation.
It describes how lxml extends the
ElementTree API to expose libxml2 and libxslt specific XML
functionality, such as XPath , Relax NG , XML Schema , XSLT , and c14n (including c14n 2.0 ).
Python code can be called from XPath expressions and XSLT
stylesheets through the use of XPath extension functions .
lxml
also offers a SAX compliant API , that works with the SAX support in
the standard library.
There is a separate module lxml.objectify that implements a data-binding
API on top of lxml.etree.
In addition to the ElementTree API, lxml also features a sophisticated
API for custom XML element classes .
This is a simple way to write
arbitrary XML driven APIs on top of lxml.
lxml.etree also has a C-level API that can be used to efficiently extend lxml.etree in
external C modules, including fast custom element class support.
Download  The best way to download lxml is to visit lxml at the Python Package
Index (PyPI).
It has the source
that compiles on various platforms.
The source distribution is signed
with this key .
The latest version is lxml 5.2.0 , released 2024-03-30
( changes for 5.2.0 ).
Older versions are listed below.
Please take a look at the installation instructions !
This complete website (including the generated API documentation) is
part of the source distribution, so if you want to download the
documentation for offline use, take the source archive and copy the doc/html directory out of the source tree.
The latest installable developer sources are available from Github.
It's also possible to check out
the latest development version of lxml from Github directly, using a command
like this:  git clone https://github.com/lxml/lxml.git lxml  You can browse the source repository and its history through
the web.
Please read how to build lxml from source first.
The latest CHANGES of the developer version are also
accessible.
You can check there if a bug you found has been fixed
or a feature you want has been implemented in the latest trunk version.
Mailing list  Questions?
Suggestions?
Code to contribute?
We have a mailing list .
You can also search the archive for past questions and discussions.
Bug tracker  lxml uses the launchpad bug tracker .
If you are sure you found a
bug in lxml, please file a bug report there.
If you are not sure
whether some unexpected behaviour of lxml is a bug or not, please
check the documentation and ask on the mailing list first.
Do not
forget to search the archive !
License  The lxml library is shipped under a BSD license .
libxml2 and libxslt2
itself are shipped under the MIT license .
There should therefore be no
obstacle to using lxml in your codebase.
Old Versions  See the websites of lxml 5.1 , 5.0 , 4.9 , 4.8 , 4.7 , 4.6 , 4.5 , 4.4 , 4.3 , 4.2 , 4.1 , 4.0 , 3.8 , 3.7 , 3.6 , 3.5 , 3.4 , 3.3 , 3.2 , 3.1 , 3.0 , 2.3 , 2.2 , 2.1 , 2.0 , 1.3   lxml 5.2.0 , released 2024-03-30 ( changes for 5.2.0 )  lxml 5.1.1 , released 2024-03-28 ( changes for 5.1.1 )  lxml 5.1.0 , released 2024-01-05 ( changes for 5.1.0 )  lxml 5.0.2 , released 2024-03-28 ( changes for 5.0.2 )  lxml 5.0.1 , released 2024-01-05 ( changes for 5.0.1 )  lxml 5.0.0 , released 2023-12-29 ( changes for 5.0.0 )  older releases     Docutils System Messages   System Message: ERROR/3 ( ./doc/main.txt , line 268); backlink Unknown target name: "lxml 5.1.0".
Project income report  lxml has about 80 million downloads per month on PyPI.
Total project income in 2023: EUR 2776.56  (231.38 € / month, 2.89 € / 1,000,000 downloads)  Tidelift: EUR 2738.46  Paypal: EUR 38.10    Total project income in 2022: EUR 2566.38  (213.87 € / month, 3.56 € / 1,000,000 downloads)  Tidelift: EUR 2539.38  Paypal: EUR 24.32    Total project income in 2021: EUR 4640.37  (386.70 € / month)  Tidelift: EUR 4066.66  Paypal: EUR 223.71  other: EUR 350.00    Total project income in 2020: EUR 6065,86  (506.49 € / month)  Tidelift: EUR 4064.77  Paypal: EUR 1401.09  other: EUR 600.00    Total project income in 2019: EUR 717.52  (59.79 € / month)  Tidelift: EUR 360.30  Paypal: EUR 157.22  other: EUR 200.00       Legal Notice for Donations  Any donation that you make to the lxml project is voluntary and
is not a fee for any services, goods, or advantages.
By making
a donation to the lxml project, you acknowledge that we have the
right to use the money you donate in any lawful way and for any
lawful purpose we see fit and we are not obligated to disclose
the way and purpose to any party unless required by applicable
law.
Although lxml is free software, to the best of our knowledge
the lxml project does not have any tax exempt status.
The lxml
project is neither a registered non-profit corporation nor a
registered charity in any country.
Your donation may or may not
be tax-deductible; please consult your tax advisor in this matter.
We will not publish or disclose your name and/or e-mail address
without your consent, unless required by applicable law.
Your
donation is non-refundable.
Generated on: 2024-03-30.
         html5lib                  Overview       Repositories        Projects        Packages       People            More       Overview    Repositories    Projects    Packages    People                   Popular repositories             html5lib-python  html5lib-python   Public    Standards-compliant library for parsing and serializing HTML documents and fragments in Python     Python      1.1k     277           html5lib-tests  html5lib-tests   Public    Testsuite data for html5lib, including the de-facto standard HTML parsing tests.
Python      182     104           html5lib-php  html5lib-php   Public    PHP port of html5lib, currently unmaintained.
PHP      95     69           html5lib-ruby  html5lib-ruby   Public    Ruby port of html5lib, currently unmaintained.
Ruby      7     3           gcode-import  gcode-import   Public    Automatically exported from code.google.com/p/html5lib.
Purely archival.
HTML      7     8           html5lib.github.io  html5lib.github.io   Public    html5lib now has its very own website!
4             Repositories                   Type       Select type            All        Public        Sources        Forks        Archived        Mirrors        Templates         Language       Select language            All        HTML        PHP        Python        Ruby         Sort       Select order            Last updated        Name        Stars                Showing 6 of 6 repositories        html5lib-python  Public  Standards-compliant library for parsing and serializing HTML documents and fragments in Python                          Python      1,093     MIT     277     78     5  Updated Feb 27, 2024        html5lib-tests  Public  Testsuite data for html5lib, including the de-facto standard HTML parsing tests.
Python      182     MIT     104     25     7  Updated Sep 30, 2023        gcode-import  Public  Automatically exported from code.google.com/p/html5lib.
HTML      7     8     13     0  Updated Mar 27, 2016        html5lib.github.io  Public  html5lib now has its very own website!
4     0     1     0  Updated May 19, 2014        html5lib-ruby  Public  Ruby port of html5lib, currently unmaintained.
Ruby      7     3     0     0  Updated Apr 9, 2013        html5lib-php  Public  PHP port of html5lib, currently unmaintained.
PHP      95     69     2     0  Updated Apr 9, 2013                    People                             Top languages  Loading…        Most used topics  Loading…          Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
Extensible Markup Language (XML) 1.0 (Fifth Edition)  W3C Recommendation 26 November 2008   Note: On 7 February 2013, this specification was modified in place to replace broken links to RFC4646 and RFC4647.
The previous errata for this document, are also available.
See also translations .
This document is also available in these non-normative formats: XML and XHTML with color-coded revision indicators .
Copyright © 2008 W3C ® ( MIT , ERCIM , Keio ), All Rights Reserved.
W3C liability , trademark and document use rules apply.
Abstract The Extensible Markup Language (XML) is a subset of SGML that is completely
described in this document.
Its goal is to enable generic SGML to be served,
received, and processed on the Web in the way that is now possible with HTML.
XML has been designed for ease of implementation and for interoperability
with both SGML and HTML.
Status of this Document This section describes the status of this document at the time of its publication.
Other documents may supersede this document.
A list of current W3C publications and the
				latest revision of this technical report can be found in the W3C technical reports index at
				http://www.w3.org/TR/.
This document specifies a syntax created by subsetting an existing, widely
				used international text processing standard (Standard Generalized Markup Language,
				ISO 8879:1986(E) as amended and corrected) for use on the World Wide Web.
It is a product of the XML Core Working Group as part of the XML Activity .
The English version of this specification is the only normative version.
However,
				for translations of this document, see http://www.w3.org/2003/03/Translations/byTechnology?technology=xml .
This document is a W3C Recommendation .
This fifth edition is not a new version of XML.
As a convenience to readers,
				it incorporates the changes dictated by the accumulated errata (available at http://www.w3.org/XML/xml-V10-4e-errata ) to the Fourth
				Edition of XML 1.0, dated 16 August 2006 .
In particular, erratum [E09] relaxes the restrictions on element and attribute names, thereby providing in XML 1.0 the major end user benefit
				currently achievable only by using XML
1.1.
As a consequence, many possible
 documents which were not well-formed according to previous editions of this
 specification are now well-formed, and previously invalid documents
using the newly-allowed name characters in, for example, ID
attributes, are now valid.
This edition supersedes the previous W3C Recommendation
				of 16 August 2006 .
For the convenience of readers,
				an XHTML version with color-coded revision indicators is
				also provided; this version highlights each change due to an erratum published in the errata
list for the previous edition, together with a link to the particular
				erratum in that list.
Most of the
errata in the list provide a rationale for the change.
The errata
list for this fifth edition is available at http://www.w3.org/XML/xml-V10-5e-errata .
An implementation report is available at http://www.w3.org/XML/2008/01/xml10-5e-implementation.html .
A Test Suite is maintained to help assessing conformance to this specification.
This document has been reviewed by W3C Members, by software developers, and by other W3C groups and interested parties, and is endorsed by the Director as a W3C Recommendation.
It is a stable document and may be used as reference material or cited from another document.
W3C's role in making the Recommendation is to draw attention to the specification and to promote its widespread deployment.
This enhances the functionality and interoperability of the Web.
W3C maintains a public list of
				any patent disclosures made in connection with the deliverables of
				the group; that page also includes instructions for disclosing a patent.
An individual who has actual knowledge of a patent which the individual
				believes contains Essential
				Claim(s) must disclose the information in accordance with section 6 of the W3C Patent Policy .
Table of Contents 1 Introduction 1.1 Origin and Goals 1.2 Terminology 2 Documents 2.1 Well-Formed XML Documents 2.2 Characters 2.3 Common Syntactic Constructs 2.4 Character Data and Markup 2.5 Comments 2.6 Processing Instructions 2.7 CDATA Sections 2.8 Prolog and Document Type Declaration 2.9 Standalone Document Declaration 2.10 White Space Handling 2.11 End-of-Line Handling 2.12 Language Identification 3 Logical Structures 3.1 Start-Tags, End-Tags, and Empty-Element Tags 3.2 Element Type Declarations 3.2.1 Element Content 3.2.2 Mixed Content 3.3 Attribute-List Declarations 3.3.1 Attribute Types 3.3.2 Attribute Defaults 3.3.3 Attribute-Value Normalization 3.4 Conditional Sections 4 Physical Structures 4.1 Character and Entity References 4.2 Entity Declarations 4.2.1 Internal Entities 4.2.2 External Entities 4.3 Parsed Entities 4.3.1 The Text Declaration 4.3.2 Well-Formed Parsed Entities 4.3.3 Character Encoding in Entities 4.4 XML Processor Treatment of Entities and References 4.4.1 Not Recognized 4.4.2 Included 4.4.3 Included If Validating 4.4.4 Forbidden 4.4.5 Included in Literal 4.4.6 Notify 4.4.7 Bypassed 4.4.8 Included as PE 4.4.9 Error 4.5 Construction of Entity Replacement Text 4.6 Predefined Entities 4.7 Notation Declarations 4.8 Document Entity 5 Conformance 5.1 Validating and Non-Validating Processors 5.2 Using XML Processors 6 Notation   Appendices A References A.1 Normative References A.2 Other References B Character Classes C XML and SGML (Non-Normative) D Expansion of Entity and Character References (Non-Normative) E Deterministic Content Models (Non-Normative) F Autodetection of Character Encodings (Non-Normative) F.1 Detection Without External Encoding Information F.2 Priorities in the Presence of External Encoding Information G W3C XML Working Group (Non-Normative) H W3C XML Core Working Group (Non-Normative) I Production Notes (Non-Normative) J Suggestions for XML Names (Non-Normative)   1 Introduction Extensible Markup Language, abbreviated XML, describes a class of data
objects called XML documents and partially
describes the behavior of computer programs which process them.
XML is an
application profile or restricted form of SGML, the Standard Generalized Markup
Language [ISO 8879] .
By construction, XML documents are conforming
SGML documents.
XML documents are made up of storage units called entities ,
which contain either parsed or unparsed data.
Parsed data is made up of characters , some of which form character
data , and some of which form markup .
Markup encodes a description of the document's storage layout and logical
structure.
XML provides a mechanism to impose constraints on the storage layout
and logical structure.
[ Definition : A software module called
an XML processor is used to read XML documents and provide access
to their content and structure.]
[ Definition : It
is assumed that an XML processor is doing its work on behalf of another module,
called the application .]
This specification describes
the required behavior of an XML processor in terms of how it must read XML
data and the information it must provide to the application.
1.1 Origin and Goals XML was developed by an XML Working Group (originally known as the SGML
Editorial Review Board) formed under the auspices of the World Wide Web Consortium
(W3C) in 1996.
It was chaired by Jon Bosak of Sun Microsystems with the active
participation of an XML Special Interest Group (previously known as the SGML
Working Group) also organized by the W3C.
The membership of the XML Working
Group is given in an appendix.
Dan Connolly served as the Working Group's contact with
the W3C.
The design goals for XML are: XML shall be straightforwardly usable over the Internet.
XML shall support a wide variety of applications.
XML shall be compatible with SGML.
It shall be easy to write programs which process XML documents.
The number of optional features in XML is to be kept to the absolute
minimum, ideally zero.
XML documents should be human-legible and reasonably clear.
The XML design should be prepared quickly.
The design of XML shall be formal and concise.
XML documents shall be easy to create.
Terseness in XML markup is of minimal importance.
This specification, together with associated standards (Unicode [Unicode] and ISO/IEC 10646 [ISO/IEC 10646] for characters, Internet BCP 47  [IETF BCP 47]  and the Language Subtag Registry [IANA-LANGCODES] for language
					identification tags), provides
all the information necessary to understand XML Version 1.0 and
construct computer programs to process it.
This version of the XML specification may be distributed freely, as long as
all text and legal notices remain intact.
1.2 Terminology The terminology used to describe XML documents is defined in the body of
this specification.
The key words MUST , MUST NOT , REQUIRED , SHALL , SHALL NOT , SHOULD , SHOULD NOT , RECOMMENDED , MAY , and OPTIONAL , when EMPHASIZED ,
are to be interpreted as described in [IETF RFC 2119] .
In addition, the terms defined
in the following list are used in building
those definitions and in describing the actions of an XML processor: error [ Definition : A violation of the rules of this specification;
results are undefined.
Unless otherwise specified, failure to observe a prescription of this specification indicated by one of the keywords MUST , REQUIRED , MUST NOT , SHALL and SHALL NOT is an error.
Conforming software MAY detect and report an error
and MAY recover from it.]
fatal error [ Definition : An error which a conforming XML processor  MUST detect and report to the application.
After encountering a fatal error, the processor MAY continue processing the
data to search for further errors and MAY report such errors to the application.
In order to support correction of errors, the processor MAY make unprocessed
data from the document (with intermingled character data and markup) available
to the application.
Once a fatal error is detected, however, the processor MUST NOT continue normal processing (i.e., it MUST NOT continue to pass character
data and information about the document's logical structure to the application
in the normal way).]
at user option [ Definition : Conforming software MAY or MUST (depending on the modal verb in the sentence) behave as described;
if it does, it MUST provide users a means to enable or disable the behavior
described.]
validity constraint [ Definition : A rule which applies to
all valid XML documents.
Violations of validity
constraints are errors; they MUST , at user option, be reported by validating XML processors .]
well-formedness constraint [ Definition : A rule which applies
to all well-formed XML documents.
Violations
of well-formedness constraints are fatal errors .]
match [ Definition : (Of strings or names:) Two strings
or names being compared are identical.
Characters with multiple possible
representations in ISO/IEC 10646 (e.g.
characters with both precomposed and
base+diacritic forms) match only if they have the same representation in both
strings.
No
case folding is performed.
(Of strings and rules in the grammar:) A string
matches a grammatical production if it belongs to the language generated by
that production.
(Of content and content models:) An element matches its declaration
when it conforms in the fashion described in the constraint [VC: Element Valid ] .]
for compatibility [ Definition : Marks
a sentence describing a feature of XML included solely to ensure
that XML remains compatible with SGML.]
for interoperability [ Definition : Marks
a sentence describing a non-binding recommendation included to increase
the chances that XML documents can be processed by the existing installed
base of SGML processors which predate the WebSGML Adaptations Annex to ISO 8879.]
2 Documents [ Definition :  A data object is an XML
document if it is well-formed ,
as defined in this specification.
In addition, the XML document is valid if it meets certain further constraints.]
Each XML document has both a logical and a physical structure.
Physically,
the document is composed of units called entities .
An entity may refer to other entities to
cause their inclusion in the document.
A document begins in a "root"
or document entity .
Logically, the document
is composed of declarations, elements, comments, character references, and
processing instructions, all of which are indicated in the document by explicit
markup.
The logical and physical structures MUST nest properly, as described
in 4.3.2 Well-Formed Parsed Entities .
2.1 Well-Formed XML Documents [ Definition :  A textual object is a well-formed XML document if:] Taken as a whole, it matches the production labeled document .
It meets all the well-formedness constraints given in this specification.
Each of the parsed entities which is referenced directly or indirectly within the document is well-formed .
Document [1] document ::=  prolog  element  Misc * Matching the document production implies that: It contains one or more elements .
[ Definition : There is exactly one element,
called the root , or document element, no part of which appears
in the content of any other element.]
For
all other elements, if the start-tag is in
the content of another element, the end-tag is in the content of the same element.
More simply stated, the elements,
delimited by start- and end-tags, nest properly within each other.
[ Definition : As a consequence of this,
for each non-root element C in the document, there is one other element P in the document such that C is in the content of P , but
is not in the content of any other element that is in the content of P .
P is referred to as the parent of C , and C as
a child of P .]
2.2 Characters [ Definition : A parsed entity contains text ,
a sequence of characters , which may
represent markup or character data.]
[ Definition : A character is an atomic unit of text as specified by ISO/IEC 10646:2000 [ISO/IEC 10646] .
Legal characters are tab, carriage
return, line feed, and the legal characters
of Unicode and ISO/IEC 10646.
The
versions of these standards cited in A.1 Normative References were
current at the time this document was prepared.
New characters may be added
to these standards by amendments or new editions.
Consequently, XML processors MUST accept any character in the range specified for Char .
]
Character Range [2] Char ::= #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF] /* any Unicode character, excluding the surrogate blocks, FFFE, and FFFF.
*/ The mechanism for encoding character code points into bit patterns may
vary from entity to entity.
All XML processors MUST accept the UTF-8 and UTF-16
encodings of Unicode [Unicode] ;
the mechanisms for signaling which of the two is in use,
or for bringing other encodings into play, are discussed later, in 4.3.3 Character Encoding in Entities .
Note: Document authors are encouraged to avoid
"compatibility characters", as defined
in section 2.3 of [Unicode] .
The characters defined in the following ranges are also
discouraged.
They are either control characters or permanently undefined Unicode
characters: [#x7F-#x84], [#x86-#x9F], [#xFDD0-#xFD E F],
[#x1FFFE-#x1FFFF], [#x2FFFE-#x2FFFF], [#x3FFFE-#x3FFFF],
[#x4FFFE-#x4FFFF], [#x5FFFE-#x5FFFF], [#x6FFFE-#x6FFFF],
[#x7FFFE-#x7FFFF], [#x8FFFE-#x8FFFF], [#x9FFFE-#x9FFFF],
[#xAFFFE-#xAFFFF], [#xBFFFE-#xBFFFF], [#xCFFFE-#xCFFFF],
[#xDFFFE-#xDFFFF], [#xEFFFE-#xEFFFF], [#xFFFFE-#xFFFFF],
[#x10FFFE-#x10FFFF].
2.3 Common Syntactic Constructs This section defines some symbols used widely in the grammar.
S (white space) consists of one or more space (#x20)
characters, carriage returns, line feeds, or tabs.
White Space [3] S ::= (#x20 | #x9 | #xD | #xA)+ Note: The presence of #xD in the above production is
	maintained purely for backward compatibility with the First Edition .
As explained in 2.11 End-of-Line Handling ,
	all #xD characters literally present in an XML document
	are either removed or replaced by #xA characters before
	any other processing is done.
The only way to get a #xD character to match this production is to
  use a character reference in an entity value literal.
An Nmtoken (name token) is any mixture of name
characters.
[ Definition : A Name is an Nmtoken with a restricted set of initial characters.]
Disallowed initial characters for Names include digits, diacritics, the full stop and the hyphen.
Names beginning with the string " xml ",
or with any string which would match (('X'|'x') ('M'|'m') ('L'|'l')) ,
are reserved for standardization in this or future versions of this specification.
Note: The
Namespaces in XML Recommendation [XML Names] assigns a meaning
to names containing colon characters.
Therefore, authors should not use the
colon in XML names except for namespace purposes, but XML processors must
accept the colon as a name character.
The first character of a Name  MUST be a NameStartChar , and any
					other characters MUST be NameChars ; this mechanism is used to
					prevent names from beginning with European (ASCII) digits or with
					basic combining characters.
Almost all characters are permitted in
					names, except those which either are or reasonably could be used as
					delimiters.
The intention is to be inclusive rather than exclusive,
					so that writing systems not yet encoded in Unicode can be used in
					XML names.
See J Suggestions for XML Names for suggestions on the creation of
					names.
Document authors are encouraged to use names which are
					meaningful words or combinations of words in natural languages, and
					to avoid symbolic or white space characters in names.
Note that
					COLON, HYPHEN-MINUS, FULL STOP (period), LOW LINE (underscore), and
					MIDDLE DOT are explicitly permitted.
The ASCII symbols and punctuation marks, along with a fairly
					large group of Unicode symbol characters, are excluded from names
					because they are more useful as delimiters in contexts where XML
					names are used outside XML documents; providing this group gives
					those contexts hard guarantees about what cannot be part of
					an XML name.
The character #x037E, GREEK QUESTION MARK, is excluded
					because when normalized it becomes a semicolon, which could change
					the meaning of entity references.
Names and Tokens [4] NameStartChar ::= ":" | [A-Z] | "_" | [a-z] | [#xC0-#xD6] | [#xD8-#xF6] | [#xF8-#x2FF] | [#x370-#x37D] | [#x37F-#x1FFF] | [#x200C-#x200D] | [#x2070-#x218F] | [#x2C00-#x2FEF] | [#x3001-#xD7FF] | [#xF900-#xFDCF] | [#xFDF0-#xFFFD] | [#x10000-#xEFFFF] [4a] NameChar ::= NameStartChar | "-" | "."
| [0-9] | #xB7 | [#x0300-#x036F] | [#x203F-#x2040] [5] Name ::= NameStartChar ( NameChar )* [6] Names ::= Name (#x20 Name )* [7] Nmtoken ::= ( NameChar )+ [8] Nmtokens ::= Nmtoken (#x20 Nmtoken )* Note: The Names and Nmtokens productions are used to define the validity
of tokenized attribute values after normalization (see 3.3.1 Attribute Types ).
Literal data is any quoted string not containing the quotation mark used
as a delimiter for that string.
Literals are used for specifying the content
of internal entities ( EntityValue ), the values
of attributes ( AttValue ), and external identifiers
( SystemLiteral ).
Note that a SystemLiteral can be parsed without scanning for markup.
Literals [9] EntityValue ::= '"' ([^%&"] | PEReference | Reference )* '"' |  "'" ([^%&'] | PEReference | Reference )* "'" [10] AttValue ::= '"' ([^<&"] | Reference )*
'"' |  "'" ([^<&'] | Reference )*
"'" [11] SystemLiteral ::= ('"' [^"]* '"') | ("'" [^']* "'") [12] PubidLiteral ::= '"' PubidChar * '"'
| "'" ( PubidChar - "'")* "'" [13] PubidChar ::= #x20 | #xD | #xA | [a-zA-Z0-9] | [-'()+,./:=?;!
2.4 Character Data and Markup  Text consists of intermingled character data and markup.
[ Definition : Markup takes the form of start-tags , end-tags , empty-element tags , entity references , character
references , comments , CDATA section delimiters, document
type declarations , processing instructions , XML declarations , text declarations ,
and any white space that is at the top level of the document entity (that
is, outside the document element and not inside any other markup).]
[ Definition : All text that is not markup
constitutes the character data of the document.]
The ampersand character (&) and the left angle bracket (<) MUST NOT appear
in their literal form, except when used as markup delimiters, or
within a comment , a processing
instruction , or a CDATA section .
If they are needed elsewhere, they MUST be escaped using either numeric character references or the strings " &amp; " and " &lt; "
respectively.
The right angle bracket (>) may be represented using the string " &gt; ",
and MUST , for compatibility , be escaped
using either " &gt; " or a character reference when it
appears in the string " ]]> " in content, when
that string is not marking the end of a CDATA
section .
In the content of elements, character data is any string of characters
which does not contain the start-delimiter of any markup and does not include the CDATA-section-close
delimiter, " ]]> ".
In a CDATA section,
character data is any string of characters not including the CDATA-section-close
delimiter, " ]]> ".
To allow attribute values to contain both single and double quotes, the
apostrophe or single-quote character (') may be represented as " &apos; ",
and the double-quote character (") as " &quot; ".
Character Data [14] CharData ::= [^<&]* - ([^<&]* ']]>' [^<&]*)  2.5 Comments [ Definition : Comments may appear
anywhere in a document outside other markup ;
in addition, they may appear within the document type declaration at places
allowed by the grammar.
They are not part of the document's character
data ; an XML processor MAY , but need not, make it possible for an
application to retrieve the text of comments.
For
compatibility , the string " -- " (double-hyphen) MUST NOT occur within comments.]
Parameter
entity references MUST NOT be recognized within comments.
Comments [15] Comment ::= '<!--' (( Char - '-') | ('-'
( Char - '-')))* '-->' An example of a comment: <!-- declarations for <head> & <body> --> Note
that the grammar does not allow a comment ending in ---> .
The
following example is not well-formed.
<!-- B+, B, or B--->  2.6 Processing Instructions [ Definition : Processing instructions (PIs) allow documents to contain instructions for applications.]
Processing Instructions [16] PI ::= '<?'
PITarget ( S ( Char * - ( Char * '?>' Char *)))?
'?>' [17] PITarget ::=  Name - (('X' | 'x') ('M' |
'm') ('L' | 'l')) PIs are not part of the document's character
data , but MUST be passed through to the application.
The PI begins
with a target ( PITarget ) used to identify the application
to which the instruction is directed.
The target names " XML ", " xml ",
and so on are reserved for standardization in this or future versions of this
specification.
The XML Notation mechanism
may be used for formal declaration of PI targets.
Parameter
entity references MUST NOT be recognized within processing instructions.
2.7 CDATA Sections [ Definition : CDATA sections may occur anywhere character data may occur; they are used to escape blocks
of text containing characters which would otherwise be recognized as markup.
CDATA sections begin with the string " <!
[CDATA[ "
and end with the string " ]]> ":]  CDATA Sections [18] CDSect ::=  CDStart  CData  CDEnd  [19] CDStart ::= '<!
[CDATA[' [20] CData ::= ( Char * - ( Char *
']]>' Char *)) [21] CDEnd ::= ']]>' Within a CDATA section, only the CDEnd string is
recognized as markup, so that left angle brackets and ampersands may occur
in their literal form; they need not (and cannot) be escaped using " &lt; "
and " &amp; ".
CDATA sections cannot nest.
An example of a CDATA section, in which " <greeting> "
and " </greeting> " are recognized as character data , not markup : <!
[CDATA[<greeting>Hello, world!</greeting>]]>  2.8 Prolog and Document Type Declaration [ Definition : XML documents SHOULD begin with an XML declaration which specifies the version of
XML being used.]
For example, the following is a complete XML document, well-formed but not valid : <?xml version="1.0"?>
<greeting>Hello, world!</greeting> and so is this: <greeting>Hello, world!</greeting> The function of the markup in an XML document is to describe its storage and
logical structure and to associate attribute
name-value pairs with its logical structures.
XML provides a mechanism, the document
type declaration , to define constraints on the logical structure
and to support the use of predefined storage units.
[ Definition : An XML document is valid if it has an associated
document type declaration and if the document complies with the constraints
expressed in it.]
The document type declaration MUST appear before the first element in the document.
Prolog [22] prolog ::=  XMLDecl ?
Misc *
( doctypedecl  Misc *)?
[23] XMLDecl ::= '<?xml' VersionInfo  EncodingDecl ?
SDDecl ?
S ?
'?>' [24] VersionInfo ::=  S 'version' Eq ("'" VersionNum "'" | '"' VersionNum '"') [25] Eq ::=  S ?
'=' S ?
[26] VersionNum ::= '1.'
[0-9]+ [27] Misc ::=  Comment | PI | S  Even though the VersionNum production matches
					any version number of the form '1.x', XML 1.0 documents SHOULD NOT specify a version number other than '1.0'.
Note: When an XML 1.0 processor encounters a document that specifies
						a 1.x version number other than '1.0', it will process it as
						a 1.0 document.
This means that an XML 1.0 processor will accept
						1.x documents provided they do not use any non-1.0 features.
[ Definition : The XML document
type declaration contains or points to markup
declarations that provide a grammar for a class of documents.
This
grammar is known as a document type definition, or DTD .
The document
type declaration can point to an external subset (a special kind of external entity ) containing markup declarations,
or can contain the markup declarations directly in an internal subset, or
can do both.
The DTD for a document consists of both subsets taken together.]
[ Definition :  A markup declaration is an element type declaration , an attribute-list declaration , an entity
declaration , or a notation declaration .]
These declarations may be contained in whole or in part within parameter
entities , as described in the well-formedness and validity constraints
below.
For further
information, see 4 Physical Structures .
Document Type Definition [28] doctypedecl ::= '<!DOCTYPE' S  Name ( S  ExternalID )?
('[' intSubset ']' S ?
)?
'>' [VC: Root Element Type] [WFC: External Subset] [28a] DeclSep ::=  PEReference | S  [WFC: PE Between Declarations] [28b] intSubset ::= ( markupdecl | DeclSep )* [29] markupdecl ::=  elementdecl | AttlistDecl | EntityDecl | NotationDecl | PI | Comment  [VC: Proper Declaration/PE Nesting] [WFC: PEs in Internal Subset] Note
that it is possible to construct a well-formed document containing a doctypedecl that neither points to an external subset nor contains an internal subset.
The markup declarations may be made up in whole or in part of the replacement text of parameter
entities .
The productions later in this specification for individual
nonterminals ( elementdecl , AttlistDecl ,
and so on) describe the declarations after all the parameter
entities have been included .
Parameter
entity references are recognized anywhere in the DTD (internal and external
subsets and external parameter entities), except in literals, processing instructions,
comments, and the contents of ignored conditional sections (see 3.4 Conditional Sections ).
They are also recognized in entity value literals.
The use of parameter entities
in the internal subset is restricted as described below.
Validity constraint: Root Element Type The Name in the document type declaration MUST match the element type of the root element .
Validity constraint: Proper Declaration/PE Nesting Parameter-entity replacement text  MUST be properly nested with markup declarations.
That is to say, if either
the first character or the last character of a markup declaration ( markupdecl above) is contained in the replacement text for a parameter-entity
reference , both MUST be contained in the same replacement text.
Well-formedness constraint: PEs in Internal Subset In
the internal DTD subset, parameter-entity references  MUST NOT occur within markup declarations; they may occur where markup declarations can occur.
(This does not apply to references that occur in external parameter entities
or to the external subset.)
Well-formedness constraint: External Subset The external subset, if any, MUST match the production for extSubset .
Well-formedness constraint: PE Between Declarations The replacement text of a parameter entity reference
in a DeclSep  MUST match the production extSubsetDecl .
Like the internal subset, the external subset and any external parameter
entities referenced
in a DeclSep  MUST consist of a series of
complete markup declarations of the types allowed by the non-terminal symbol markupdecl , interspersed with white space or parameter-entity references .
However, portions of
the contents of the external subset or of these
external parameter entities may conditionally be ignored by using the conditional section construct; this is not
allowed in the internal subset but is
allowed in external parameter entities referenced in the internal subset.
External Subset [30] extSubset ::=  TextDecl ?
extSubsetDecl  [31] extSubsetDecl ::= ( markupdecl | conditionalSect | DeclSep )* The external subset and external parameter entities also differ from the
internal subset in that in them, parameter-entity
references are permitted within markup declarations,
not only between markup declarations.
An example of an XML document with a document type declaration: <?xml version="1.0"?>
<!DOCTYPE greeting SYSTEM "hello.dtd">
<greeting>Hello, world!</greeting> The system identifier " hello.dtd "
gives the address (a URI reference) of a DTD for the document.
The declarations can also be given locally, as in this example: <?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE greeting [
  <!ELEMENT greeting (#PCDATA)>
]>
<greeting>Hello, world!</greeting> If both the external and internal subsets are used, the internal subset MUST be considered to occur before the external subset.
This has the effect that entity and attribute-list declarations in the internal
subset take precedence over those in the external subset.
2.9 Standalone Document Declaration Markup declarations can affect the content of the document, as passed from
an XML processor to an application; examples
are attribute defaults and entity declarations.
The standalone document declaration,
which may appear as a component of the XML declaration, signals whether or
not there are such declarations which appear external to the document
entity or in parameter entities.
[ Definition : An external
markup declaration is defined as a markup declaration occurring in
the external subset or in a parameter entity (external or internal, the latter
being included because non-validating processors are not required to read
them).]
Standalone Document Declaration [32] SDDecl ::=  S 'standalone' Eq (("'" ('yes' | 'no') "'") | ('"' ('yes' | 'no') '"')) [VC: Standalone Document Declaration] In a standalone document declaration, the value "yes" indicates
that there are no external markup declarations which
affect the information passed from the XML processor to the application.
The
value "no" indicates that there are or may be such external
markup declarations.
Note that the standalone document declaration only denotes
the presence of external declarations ; the presence, in a document,
of references to external entities , when those entities are internally
declared, does not change its standalone status.
If there are no external markup declarations, the standalone document declaration
has no meaning.
If there are external markup declarations but there is no
standalone document declaration, the value "no" is assumed.
Any XML document for which standalone="no" holds can be converted
algorithmically to a standalone document, which may be desirable for some
network delivery applications.
Validity constraint: Standalone Document Declaration The
standalone document declaration MUST have the value "no" if
any external markup declarations contain declarations of: attributes with default values,
if elements to which these attributes apply appear in the document without
specifications of values for these attributes, or entities (other than amp , lt , gt , apos , quot ), if references to those entities appear in the document, or attributes with
tokenized types, where the
attribute appears in the document with a value such that normalization will produce a different value from that which would be produced
in the absence of the declaration, or element types with element content ,
if white space occurs directly within any instance of those types.
An example XML declaration with a standalone document declaration: <?xml version="1.0" standalone='yes'?>  2.10 White Space Handling In editing XML documents, it is often convenient to use "white space"
(spaces, tabs, and blank lines)
to set apart the markup for greater readability.
Such white space is typically
not intended for inclusion in the delivered version of the document.
On the
other hand, "significant" white space that should be preserved
in the delivered version is common, for example in poetry and source code.
An XML processor  MUST always pass
all characters in a document that are not markup through to the application.
A validating XML processor  MUST also
inform the application which of these characters constitute white space appearing
in element content .
A special attribute named xml:space may be attached to an element to signal an intention that in that element,
white space should be preserved by applications.
In valid documents, this
attribute, like any other, MUST be declared if it is used.
When declared, it MUST be given as an enumerated
type whose values
are one or both of "default" and "preserve".
For example: <!ATTLIST poem  xml:space (default|preserve) 'preserve'>

<!ATTLIST pre xml:space (preserve) #FIXED 'preserve'> The value "default" signals that applications' default white-space
processing modes are acceptable for this element; the value "preserve"
indicates the intent that applications preserve all the white space.
This
declared intent is considered to apply to all elements within the content
of the element where it is specified, unless overridden with
another instance of the xml:space attribute.
This specification does not give meaning to any value of xml:space other than "default" and "preserve".
It is an error for other values to be specified; the XML processor MAY report the error or MAY recover by ignoring the attribute specification or by reporting the (erroneous) value to the application.
Applications may ignore or reject erroneous values.
The root element of any document is considered
to have signaled no intentions as regards application space handling, unless
it provides a value for this attribute or the attribute is declared with a
default value.
2.11 End-of-Line Handling XML parsed entities are often stored
in computer files which, for editing convenience, are organized into lines.
These lines are typically separated by some combination of the characters
CARRIAGE RETURN (#xD) and LINE FEED (#xA).
To
simplify the tasks of applications , the XML
processor  MUST behave as if it normalized all line breaks in external parsed
entities (including the document entity) on input, before parsing, by translating
both the two-character sequence #xD #xA and any #xD that is not followed by
#xA to a single #xA character.
2.12 Language Identification In document processing, it is often useful to identify the natural or formal
language in which the content is written.
A special attribute named xml:lang may be inserted in documents to specify the language
used in the contents and attribute values of any element in an XML document.
In valid documents, this attribute, like any other, MUST be declared if it is used.
The
values of the attribute are language identifiers as defined by [IETF BCP 47] , Tags
for the Identification of Languages ; in addition, the empty string may be specified.
(Productions 33 through 38 have been removed.)
For example: <p xml:lang="en">The quick brown fox jumps over the lazy dog.</p>
<p xml:lang="en-GB">What colour is it?</p>
<p xml:lang="en-US">What color is it?</p>
<sp who="Faust" desc='leise' xml:lang="de">
  <l>Habe nun, ach!
Philosophie,</l>
  <l>Juristerei, und Medizin</l>
  <l>und leider auch Theologie</l>
  <l>durchaus studiert mit heißem Bemüh'n.</l>
</sp> The language specified by xml:lang applies to the element where it is specified
		 (including the values of its attributes), and  to all elements in its content unless
     overridden with another instance of xml:lang .
In particular, the empty value of xml:lang is used on an element B to override
		 a specification of xml:lang on an enclosing element A, without specifying another language.
Within B,
		 it is considered that there is no language information available, just as if xml:lang had not been specified
		 on B or any of its ancestors.
Applications determine which of an element's attribute values
     and which parts of its character content, if any, are treated as language-dependent values described by xml:lang .
Note: Language information may also be provided by external transport protocols (e.g.
HTTP or
  MIME).
When available, this information may be used by XML applications, but the more local
  information provided by xml:lang should be considered to override it.
A simple declaration for xml:lang might take the form xml:lang CDATA #IMPLIED but specific default values may also be given, if appropriate.
In a collection
of French poems for English students, with glosses and notes in English, the xml:lang attribute might be declared this way: <!ATTLIST poem   xml:lang CDATA 'fr'>
<!ATTLIST gloss  xml:lang CDATA 'en'>
<!ATTLIST note   xml:lang CDATA 'en'>  3 Logical Structures [ Definition : Each XML
document contains one or more elements , the boundaries
of which are either delimited by start-tags and end-tags , or, for empty elements, by an empty-element tag .
Each
element has a type, identified by name, sometimes called its "generic
identifier" (GI), and may have a set of attribute specifications.]
Each attribute specification has a name and a value .
Element [39] element ::=  EmptyElemTag  | STag  content  ETag  [WFC: Element Type Match] [VC: Element Valid] This specification does not constrain the
			application semantics, use, or (beyond syntax)
names of the element types and attributes, except that names beginning with
a match to (('X'|'x')('M'|'m')('L'|'l')) are reserved for standardization
in this or future versions of this specification.
Well-formedness constraint: Element Type Match The Name in an element's end-tag MUST match the element type in the start-tag.
Validity constraint: Element Valid An element is valid
if there is a declaration matching elementdecl where the Name matches the element type, and one of
the following holds: The declaration matches EMPTY and the element has no content (not even entity
references, comments, PIs or white space).
The declaration matches children and the
sequence of child elements belongs
to the language generated by the regular expression in the content model,
with optional white space, comments and
PIs (i.e.
markup matching production [27] Misc ) between the
start-tag and the first child element, between child elements, or between
the last child element and the end-tag.
Note that a CDATA section containing
only white space or a reference
to an entity whose replacement text is character references expanding to white
space do not
match the nonterminal S , and
hence cannot appear in these positions; however, a
reference to an internal entity with a literal value consisting of character
references expanding to white space does match S , since its
replacement text is the white space resulting from expansion of the character
references.
The declaration matches Mixed , and the content
(after replacing
any entity references with their replacement text) consists of character data (including CDATA sections ), comments , PIs and child elements whose types match names in the
content model.
The declaration matches ANY , and the content (after replacing
any entity references with their replacement text)
consists of character data, CDATA
sections , comments , PIs and child elements whose types have been declared.
3.1 Start-Tags, End-Tags, and Empty-Element Tags [ Definition : The beginning of every non-empty
XML element is marked by a start-tag .]
Start-tag [40] STag ::= '<' Name ( S  Attribute )* S ?
'>' [WFC: Unique Att Spec] [41] Attribute ::=  Name  Eq  AttValue  [VC: Attribute Value Type] [WFC: No External Entity References] [WFC: No < in Attribute Values] The Name in the start- and end-tags gives the element's type .
[ Definition :  The Name - AttValue pairs are referred to as the attribute specifications of the
element], [ Definition : with the Name in each pair referred to as the attribute name ]
and [ Definition : the content of the AttValue (the text between the ' or " delimiters) as the attribute value .]
Note
that the order of attribute specifications in a start-tag or empty-element
tag is not significant.
Well-formedness constraint: Unique Att Spec An attribute name MUST NOT appear more than once in the same start-tag or empty-element tag.
Validity constraint: Attribute Value Type The attribute MUST have been declared; the value MUST be of the type declared for it.
(For attribute
types, see 3.3 Attribute-List Declarations .)
Well-formedness constraint: No External Entity References Attribute
values MUST NOT contain direct or indirect entity references to external entities.
Well-formedness constraint: No < in Attribute Values The replacement text of any entity
referred to directly or indirectly in an attribute value MUST NOT contain a < .
An example of a start-tag: <termdef id="dt-dog" term="dog"> [ Definition : The end of every element that begins
with a start-tag MUST be marked by an end-tag containing a name
that echoes the element's type as given in the start-tag:]  End-tag [42] ETag ::= '</' Name  S ?
'>' An example of an end-tag: </termdef> [ Definition : The text between the start-tag and end-tag is called the element's content :]  Content of Elements [43] content ::=  CharData ?
(( element | Reference | CDSect | PI | Comment ) CharData ?
)* [ Definition : An element
with no content is said to be empty .]
The representation
of an empty element is either a start-tag immediately followed by an end-tag,
or an empty-element tag.
[ Definition : An empty-element
tag takes a special form:]  Tags for Empty Elements [44] EmptyElemTag ::= '<' Name ( S  Attribute )* S ?
'/>' [WFC: Unique Att Spec] Empty-element tags may be used for any element which has no content, whether
or not it is declared using the keyword EMPTY .
For
interoperability , the empty-element tag SHOULD be used, and SHOULD only be used, for elements which are declared
EMPTY.
Examples of empty elements: <IMG align="left"
 src="http://www.w3.org/Icons/WWW/w3c_home" />
<br></br>
<br/>  3.2 Element Type Declarations The element structure of an XML document may, for validation purposes, be constrained using element type and attribute-list declarations.
An element type declaration constrains the element's content .
Element type declarations often constrain which element types can appear
as children of the element.
At user
option, an XML processor MAY issue a warning when a declaration mentions an
element type for which no declaration is provided, but this is not an error.
[ Definition : An element
type declaration takes the form:]  Element Type Declaration [45] elementdecl ::= '<!ELEMENT' S  Name  S  contentspec  S ?
'>' [VC: Unique Element Type Declaration] [46] contentspec ::= 'EMPTY' | 'ANY' | Mixed | children  where the Name gives the element type being declared.
Validity constraint: Unique Element Type Declaration An element type MUST NOT be declared more than once.
Examples of element type declarations: <!ELEMENT br EMPTY>
<!ELEMENT p (#PCDATA|emph)* >
<!ELEMENT %name.para; %content.para; >
<!ELEMENT container ANY>  3.2.1 Element Content [ Definition : An element type has element content when elements
of that type MUST contain only child elements (no character data), optionally separated by white space (characters
matching the nonterminal S ).]
[ Definition : In this case, the constraint includes a content
model , a simple grammar governing the allowed types of the
child elements and the order in which they are allowed to appear.]
The grammar is built on content particles ( cp s), which
consist of names, choice lists of content particles, or sequence lists of
content particles:  Element-content Models [47] children ::= ( choice | seq )
('?'
| '*' | '+')?
[48] cp ::= ( Name | choice | seq ) ('?'
[49] choice ::= '(' S ?
cp ( S ?
'|' S ?
cp )+ S ?
')'
[VC: Proper Group/PE Nesting] [50] seq ::= '(' S ?
',' S ?
cp )* S ?
')'
[VC: Proper Group/PE Nesting] where each Name is the type of an element which
may appear as a child .
Any content
particle in a choice list may appear in the element
content at the location where the choice list appears in the grammar;
content particles occurring in a sequence list MUST each appear in the element content in the order given in the list.
The optional character following a name or list governs whether the element
or the content particles in the list may occur one or more ( + ),
zero or more ( * ), or zero or one times ( ?
).
The
absence of such an operator means that the element or content particle MUST appear exactly once.
This syntax and meaning are identical to those used in
the productions in this specification.
The content of an element matches a content model if and only if it is
possible to trace out a path through the content model, obeying the sequence,
choice, and repetition operators and matching each element in the content
against an element type in the content model.
For
compatibility , it is an error if the content model
allows an element to match more than one occurrence of an element type in the
content model.
For more information, see E Deterministic Content Models .
Validity constraint: Proper Group/PE Nesting Parameter-entity replacement text  MUST be properly nested with parenthesized
groups.
That is to say, if either of the opening or closing parentheses in
a choice , seq , or Mixed construct is contained in the replacement text for a parameter
entity , both MUST be contained in the same replacement text.
For interoperability , if a parameter-entity reference
appears in a choice , seq , or Mixed construct, its replacement text SHOULD contain at
least one non-blank character, and neither the first nor last non-blank character
of the replacement text SHOULD be a connector ( | or , ).
Examples of element-content models: <!ELEMENT spec (front, body, back?
)>
<!ELEMENT div1 (head, (p | list | note)*, div2*)>
<!ELEMENT dictionary-body (%div.mix; | %dict.mix;)*>  3.2.2 Mixed Content [ Definition : An element type has mixed content when elements of that type may contain character
data, optionally interspersed with child elements.]
In this case, the types of the child elements may be constrained,
but not their order or their number of occurrences:  Mixed-content Declaration [51] Mixed ::= '(' S ?
'#PCDATA' ( S ?
Name )* S ?
')*' | '(' S ?
'#PCDATA' S ?
')'
[VC: Proper Group/PE Nesting] [VC: No Duplicate Types] where the Name s give the types of elements that
may appear as children.
The
keyword #PCDATA derives historically from the term "parsed
character data."
Validity constraint: No Duplicate Types The
same name MUST NOT appear more than once in a single mixed-content declaration.
Examples of mixed content declarations: <!ELEMENT p (#PCDATA|a|ul|b|i|em)*>
<!ELEMENT p (#PCDATA | %font; | %phrase; | %special; | %form;)* >
<!ELEMENT b (#PCDATA)>  3.3 Attribute-List Declarations  Attributes are used to associate name-value
pairs with elements .
Attribute specifications MUST NOT appear outside of start-tags and empty-element tags ; thus, the productions used to
recognize them appear in 3.1 Start-Tags, End-Tags, and Empty-Element Tags .
Attribute-list declarations
may be used: To define the set of attributes pertaining to a given element type.
To establish type constraints for these attributes.
To provide default values for
attributes.
[ Definition : Attribute-list
declarations specify the name, data type, and default value (if any)
of each attribute associated with a given element type:]  Attribute-list Declaration [52] AttlistDecl ::= '<!ATTLIST' S  Name  AttDef * S ?
'>' [53] AttDef ::=  S  Name  S  AttType  S  DefaultDecl  The Name in the AttlistDecl rule is the type of an element.
At user option, an XML processor MAY issue
a warning if attributes are declared for an element type not itself declared,
but this is not an error.
The Name in the AttDef rule is the name of the attribute.
When more than one AttlistDecl is provided
for a given element type, the contents of all those provided are merged.
When
more than one definition is provided for the same attribute of a given element
type, the first declaration is binding and later declarations are ignored.
For interoperability, writers of DTDs may choose
to provide at most one attribute-list declaration for a given element type,
at most one attribute definition for a given attribute name in an attribute-list
declaration, and at least one attribute definition in each attribute-list
declaration.
For interoperability, an XML processor MAY at user option
issue a warning when more than one attribute-list declaration is provided
for a given element type, or more than one attribute definition is provided
for a given attribute, but this is not an error.
3.3.1 Attribute Types XML attribute types are of three kinds: a string type, a set of tokenized
types, and enumerated types.
The string type may take any literal string as
a value; the tokenized types are more constrained.
The validity constraints noted in the grammar are applied after the attribute
value has been normalized as described in 3.3.3 Attribute-Value Normalization .
Attribute Types [54] AttType ::=  StringType | TokenizedType | EnumeratedType  [55] StringType ::= 'CDATA' [56] TokenizedType ::= 'ID' [VC: ID] [VC: One ID per Element Type] [VC: ID Attribute Default] | 'IDREF' [VC: IDREF] | 'IDREFS' [VC: IDREF] | 'ENTITY' [VC: Entity Name] | 'ENTITIES' [VC: Entity Name] | 'NMTOKEN' [VC: Name Token] | 'NMTOKENS' [VC: Name Token] Validity constraint: ID Values of type ID  MUST match the Name production.
A name MUST NOT appear more than once
in an XML document as a value of this type; i.e., ID values MUST uniquely
identify the elements which bear them.
Validity constraint: One ID per Element Type An element type MUST NOT have more than one ID attribute specified.
Validity constraint: ID Attribute Default An ID attribute MUST have a declared default of #IMPLIED or #REQUIRED .
Validity constraint: IDREF Values of type IDREF  MUST match the Name production, and values of type IDREFS  MUST match Names ; each Name  MUST match the value of an ID attribute on some element in the XML document;
i.e.
IDREF values MUST match the value of some ID attribute.
Validity constraint: Entity Name Values of type ENTITY  MUST match the Name production, values of type ENTITIES  MUST match Names ; each Name  MUST match the name of an unparsed entity declared in the DTD .
Validity constraint: Name Token Values of type NMTOKEN  MUST match the Nmtoken production; values of type NMTOKENS  MUST match Nmtokens .
[ Definition : Enumerated attributes have a list of allowed values in their declaration
						].
They MUST take one of those values.
There are two kinds of enumerated attribute types:  Enumerated Attribute Types [57] EnumeratedType ::=  NotationType | Enumeration  [58] NotationType ::= 'NOTATION' S '(' S ?
Name ( S ?
Name )* S ?
')'
[VC: Notation Attributes] [VC: One Notation Per Element Type] [VC: No Notation on Empty Element] [VC: No Duplicate Tokens] [59] Enumeration ::= '(' S ?
Nmtoken ( S ?
Nmtoken )* S ?
')'
[VC: Enumeration] [VC: No Duplicate Tokens] A NOTATION attribute identifies a notation ,
declared in the DTD with associated system and/or public identifiers, to be
used in interpreting the element to which the attribute is attached.
Validity constraint: Notation Attributes Values of this type MUST match one of the notation names
included in the declaration; all notation names in the declaration MUST be
declared.
Validity constraint: One Notation Per Element Type An element type MUST NOT have more than one NOTATION attribute specified.
Validity constraint: No Notation on Empty Element  For compatibility ,
an attribute of type NOTATION  MUST NOT be declared on an element
declared EMPTY .
Validity constraint: No Duplicate Tokens The notation names in a single NotationType attribute declaration, as well as the NmToken s in a single Enumeration attribute declaration, MUST all be distinct.
Validity constraint: Enumeration Values of this type MUST match
one of the Nmtoken tokens in the declaration.
For interoperability, the same Nmtoken  SHOULD NOT occur more than once in the enumerated
attribute types of a single element type.
3.3.2 Attribute Defaults An attribute declaration provides information
on whether the attribute's presence is REQUIRED , and if not, how an XML processor
is to react if a declared attribute is absent in a document.
Attribute Defaults [60] DefaultDecl ::= '#REQUIRED' | '#IMPLIED' | (('#FIXED' S )?
AttValue ) [VC: Required Attribute] [VC: Attribute Default Value Syntactically Correct] [WFC: No < in Attribute Values] [VC: Fixed Attribute Default] [WFC: No External Entity References] In an attribute declaration, #REQUIRED means that the attribute MUST always be provided, #IMPLIED that no default value is provided.
[ Definition : If
the declaration is neither #REQUIRED nor #IMPLIED , then
the AttValue value contains the declared default value; the #FIXED keyword states that the attribute MUST always have
the default value.
When an XML processor encounters
an element
without a specification for an attribute for which it has read a default
value declaration, it MUST report the attribute with the declared default
value to the application.]
Validity constraint: Required Attribute If the default
declaration is the keyword #REQUIRED , then the attribute MUST be
specified for all elements of the type in the attribute-list declaration.
Validity constraint: Attribute Default Value Syntactically Correct The declared default value MUST meet the syntactic
constraints of the declared attribute type.
That is, the default value of an attribute: of type IDREF or ENTITY must match the Name production; of type IDREFS or ENTITIES must match the Names production; of type NMTOKEN must match the Nmtoken production; of type NMTOKENS must match the Nmtokens production; of an enumerated type (either a NOTATION type or an enumeration ) must match one of the enumerated values.
Note that only the
syntactic constraints of the type are required here; other constraints (e.g.
that the value be the name of a declared unparsed entity, for an attribute of
type ENTITY) will be reported by a validating
parser only if an element without a specification for this attribute
actually occurs.
Validity constraint: Fixed Attribute Default If an attribute
has a default value declared with the #FIXED keyword, instances of
that attribute MUST match the default value.
Examples of attribute-list declarations: <!ATTLIST termdef
          id      ID      #REQUIRED
          name    CDATA   #IMPLIED>
<!ATTLIST list
          type    (bullets|ordered|glossary)  "ordered">
<!ATTLIST form
          method  CDATA   #FIXED "POST">  3.3.3 Attribute-Value Normalization Before the value of an attribute is passed to the application or checked
for validity, the XML processor MUST normalize the attribute value by applying
the algorithm below, or by using some other method such that the value passed
to the application is the same as that produced by the algorithm.
All line breaks MUST have been normalized on input to #xA as described
in 2.11 End-of-Line Handling , so the rest of this algorithm operates
on text normalized in this way.
Begin with a normalized value consisting of the empty string.
For each character, entity reference, or character reference in the
unnormalized attribute value, beginning with the first and continuing to the
last, do the following: For a character reference, append the referenced character to the
normalized value.
For an entity reference, recursively apply step 3 of this algorithm
to the replacement text of the entity.
For a white space character (#x20, #xD, #xA, #x9), append a space
character (#x20) to the normalized value.
For another character, append the character to the normalized value.
If the attribute type is not CDATA, then the XML processor MUST further
process the normalized attribute value by discarding any leading and trailing
space (#x20) characters, and by replacing sequences of space (#x20) characters
by a single space (#x20) character.
Note that if the unnormalized attribute value contains a character reference
to a white space character other than space (#x20), the normalized value contains
the referenced character itself (#xD, #xA or #x9).
This contrasts with the
case where the unnormalized value contains a white space character (not a
reference), which is replaced with a space character (#x20) in the normalized
value and also contrasts with the case where the unnormalized value contains
an entity reference whose replacement text contains a white space character;
being recursively processed, the white space character is replaced with a
space character (#x20) in the normalized value.
All attributes for which no declaration has been read SHOULD be treated
by a non-validating processor as if declared CDATA .
It is an error if an attribute
value contains a reference to an
entity for which no declaration has been read.
Following are examples of attribute normalization.
Given the following
declarations: <!ENTITY d "&#xD;">
<!ENTITY a "&#xA;">
<!ENTITY da "&#xD;&#xA;"> the attribute specifications in the left column below would be normalized
to the character sequences of the middle column if the attribute a is declared NMTOKENS and to those of the right columns if a is declared CDATA .
Attribute specification a is NMTOKENS a is CDATA  a="

xyz"   x y z   #x20 #x20 x y z   a="&d;&d;A&a;&#x20;&a;B&da;"   A #x20 B   #x20 #x20 A #x20 #x20 #x20 B #x20 #x20   a=
"&#xd;&#xd;A&#xa;&#xa;B&#xd;&#xa;"   #xD #xD A #xA #xA B #xD #xA   #xD #xD A #xA #xA B #xD #xA  Note that the last example is invalid (but well-formed) if a is declared to be of type NMTOKENS .
3.4 Conditional Sections [ Definition : Conditional
sections are portions of the document type
declaration external subset or
of external parameter entities which are included in, or excluded from,
the logical structure of the DTD based on the keyword which governs them.]
Conditional Section [61] conditionalSect ::=  includeSect | ignoreSect  [62] includeSect ::= '<!
[' S ?
'INCLUDE' S ?
'[' extSubsetDecl ']]>' [VC: Proper Conditional Section/PE Nesting] [63] ignoreSect ::= '<!
'IGNORE' S ?
'[' ignoreSectContents *
']]>' [VC: Proper Conditional Section/PE Nesting] [64] ignoreSectContents ::=  Ignore ('<!
[' ignoreSectContents ']]>' Ignore )* [65] Ignore ::=  Char * - ( Char *
('<!
[' | ']]>') Char *) Validity constraint: Proper Conditional Section/PE Nesting If any of the " <!
[ ",
" [ ", or " ]]> " of a conditional section is contained
in the replacement text for a parameter-entity reference, all of them MUST be contained in the same replacement text.
Like the internal and external DTD subsets, a conditional section may contain
one or more complete declarations, comments, processing instructions, or nested
conditional sections, intermingled with white space.
If the keyword of the conditional section is INCLUDE , then the
contents of the conditional section MUST be processed as part of the DTD.
If the keyword of
the conditional section is IGNORE , then the contents of the conditional
section MUST  NOT be processed as part of the DTD.
If a conditional section with a keyword of INCLUDE occurs within
a larger conditional section with a keyword of IGNORE , both the outer
and the inner conditional sections MUST be ignored.
The contents
of an ignored conditional section MUST be parsed by ignoring all characters after
the " [ " following the keyword, except conditional section starts
" <!
[ " and ends " ]]> ", until the matching conditional
section end is found.
Parameter entity references MUST NOT be recognized in this
process.
If the keyword of the conditional section is a parameter-entity reference,
the parameter entity MUST be replaced by its content before the processor
decides whether to include or ignore the conditional section.
An example: <!ENTITY % draft 'INCLUDE' >
<!ENTITY % final 'IGNORE' >

<!
[%draft;[
<!ELEMENT book (comments*, title, body, supplements?
)>
]]>
<!
[%final;[
<!ELEMENT book (title, body, supplements?
)>
]]>  4 Physical Structures [ Definition : An XML document may consist of one
or many storage units.
These
are called entities ; they all have content and are
all (except for the document entity and
the external DTD subset ) identified by
entity name .]
Each XML document has one entity
called the document entity , which serves
as the starting point for the XML processor and may contain the whole document.
Entities may be either parsed or unparsed.
[ Definition : The contents of a parsed
entity are referred to as its replacement
text ; this text is considered an
integral part of the document.]
[ Definition : An unparsed entity is a resource whose contents may or may not be text ,
and if text, may
be other than XML.
Each unparsed entity has an associated notation , identified by name.
Beyond a requirement
that an XML processor make the identifiers for the entity and notation available
to the application, XML places no constraints on the contents of unparsed
entities.]
Parsed entities are invoked by name using entity references; unparsed entities
by name, given in the value of ENTITY or ENTITIES attributes.
[ Definition : General entities are entities for use within the document content.
In this specification, general
entities are sometimes referred to with the unqualified term entity when this leads to no ambiguity.]
[ Definition : Parameter
entities are parsed entities for use within the DTD.]
These two types of entities use different forms of reference and are recognized
in different contexts.
Furthermore, they occupy different namespaces; a parameter
entity and a general entity with the same name are two distinct entities.
4.1 Character and Entity References [ Definition :  A character
reference refers to a specific character in the ISO/IEC 10646 character
set, for example one not directly accessible from available input devices.]
Character Reference [66] CharRef ::= '&#' [0-9]+ ';' | '&#x' [0-9a-fA-F]+ ';' [WFC: Legal Character] Well-formedness constraint: Legal Character Characters referred
to using character references MUST match the production for Char .
If the character reference begins with " &#x ",
the digits and letters up to the terminating ; provide a hexadecimal
representation of the character's code point in ISO/IEC 10646.
If it begins
just with " &# ", the digits up to the terminating ; provide a decimal representation of the character's code point.
[ Definition : An entity reference refers to the content of a named entity.]
[ Definition : References to parsed general entities use
ampersand ( & ) and semicolon ( ; ) as delimiters.]
[ Definition : Parameter-entity references use percent-sign ( % ) and semicolon ( ; ) as delimiters.]
Entity Reference [67] Reference ::=  EntityRef | CharRef  [68] EntityRef ::= '&' Name ';' [WFC: Entity Declared] [VC: Entity Declared] [WFC: Parsed Entity] [WFC: No Recursion] [69] PEReference ::= '%' Name ';' [VC: Entity Declared] [WFC: No Recursion] [WFC: In DTD] Well-formedness constraint: Entity Declared In a document
without any DTD, a document with only an internal DTD subset which contains
no parameter entity references, or a document with " standalone='yes' ", for
an entity reference that does not occur within the external subset or a parameter
entity, the Name given in the entity reference MUST  match that in an entity
declaration that does not occur within the external subset or a
parameter entity, except that well-formed documents need not declare
any of the following entities: amp , lt , gt , apos , quot .
The
declaration of a general entity MUST precede any reference to it which appears
in a default value in an attribute-list declaration.
Note that non-validating processors are not
obligated to read and process entity declarations occurring in parameter entities or in
the external subset; for such documents,
the rule that an entity must be declared is a well-formedness constraint only
if standalone='yes' .
Validity constraint: Entity Declared In a document with an external subset or parameter  entity references ,
						if the document is not standalone (either " standalone='no' "
						is specified or there is no standalone declaration), then the Name given in the entity reference MUST  match that in an entity
declaration .
For interoperability, valid documents SHOULD declare
the entities amp , lt , gt , apos , quot , in the form specified in 4.6 Predefined Entities .
The declaration of a parameter entity MUST precede any reference to it.
Similarly,
the declaration of a general entity MUST precede any attribute-list
declaration containing a default value with a direct or indirect reference
to that general entity.
Well-formedness constraint: Parsed Entity An entity reference MUST
NOT contain the name of an unparsed entity .
Unparsed entities may be referred to only in attribute
values declared to be of type ENTITY or ENTITIES .
Well-formedness constraint: No Recursion A parsed entity MUST NOT contain a recursive reference to itself, either directly or indirectly.
Well-formedness constraint: In DTD Parameter-entity references MUST NOT appear outside
 the DTD .
Examples of character and entity references: Type <key>less-than</key> (&#x3C;) to save options.
This document was prepared on &docdate; and
is classified &security-level;.
Example of a parameter-entity reference: <!-- declare the parameter entity "ISOLat2"...
-->
<!ENTITY % ISOLat2
         SYSTEM "http://www.xml.com/iso/isolat2-xml.entities" >
<!-- ...
now reference it.
-->
%ISOLat2;  4.2 Entity Declarations [ Definition :  Entities are declared
thus:]  Entity Declaration [70] EntityDecl ::=  GEDecl | PEDecl  [71] GEDecl ::= '<!ENTITY' S  Name  S  EntityDef  S ?
'>' [72] PEDecl ::= '<!ENTITY' S '%' S  Name  S  PEDef  S ?
'>' [73] EntityDef ::=  EntityValue | ( ExternalID  NDataDecl ?)
[74] PEDef ::=  EntityValue | ExternalID  The Name identifies the entity in an entity
reference or, in the case of an unparsed entity, in the value of
an ENTITY or ENTITIES attribute.
If the same entity is declared
more than once, the first declaration encountered is binding; at user option,
an XML processor MAY issue a warning if entities are declared multiple times.
4.2.1 Internal Entities [ Definition : If the
entity definition is an EntityValue , the defined
entity is called an internal entity .
There is no separate physical
storage object, and the content of the entity is given in the declaration.]
Note that some processing of entity and character references in the literal entity value may be required to produce
the correct replacement text : see 4.5 Construction of Entity Replacement Text .
An internal entity is a parsed entity .
Example of an internal entity declaration: <!ENTITY Pub-Status "This is a pre-release of the
 specification.
">  4.2.2 External Entities [ Definition : If the entity is not internal,
it is an external entity , declared as follows:]  External Entity Declaration [75] ExternalID ::= 'SYSTEM' S  SystemLiteral  | 'PUBLIC' S  PubidLiteral  S  SystemLiteral  [76] NDataDecl ::=  S 'NDATA' S  Name  [VC: Notation Declared] If the NDataDecl is present, this is a general unparsed entity ; otherwise it is a parsed entity.
Validity constraint: Notation Declared The Name  MUST match the declared name of a notation .
[ Definition : The SystemLiteral is called the entity's system
identifier .
It is meant to be converted to a URI reference
(as defined in [IETF RFC 3986] ),
as part of the
process of dereferencing it to obtain input for the XML processor to construct the
entity's replacement text.]
It is an error for a fragment identifier
(beginning with a # character) to be part of a system identifier.
Unless otherwise provided by information outside the scope of this specification
(e.g.
a special XML element type defined by a particular DTD, or a processing
instruction defined by a particular application specification), relative URIs
are relative to the location of the resource within which the entity declaration
occurs.
This is defined to
be the external entity containing the '<' which starts the declaration, at the
point when it is parsed as a declaration.
A URI might thus be relative to the document
entity , to the entity containing the external
DTD subset , or to some other external parameter
entity .
Attempts to
retrieve the resource identified by a URI may be redirected at the parser
level (for example, in an entity resolver) or below (at the protocol level,
for example, via an HTTP Location: header).
In the absence of additional
information outside the scope of this specification within the resource,
the base URI of a resource is always the URI of the actual resource returned.
In other words, it is the URI of the resource retrieved after all redirection
has occurred.
System
identifiers (and other XML strings meant to be used as URI references) may contain
characters that, according to [IETF RFC 3986] ,
must be escaped before a URI can be used to retrieve the referenced resource.
The
characters to be escaped are the control characters #x0 to #x1F and #x7F (most of
which cannot appear in XML), space #x20, the delimiters '<' #x3C, '>' #x3E and
'"' #x22, the unwise characters '{' #x7B, '}' #x7D, '|' #x7C, '\' #x5C, '^' #x5E and
'`' #x60, as well as all characters above #x7F.
Since escaping is not always a fully
reversible process, it MUST be performed only when absolutely necessary and as late
as possible in a processing chain.
In particular, neither the process of converting
a relative URI to an absolute one nor the process of passing a URI reference to a
process or software component responsible for dereferencing it SHOULD trigger escaping.
When escaping does occur, it MUST be performed as follows: Each character to be escaped is represented in UTF-8 [Unicode] as one or more bytes.
The resulting bytes are escaped with
the URI escaping mechanism (that is, converted to %  HH ,
where HH is the hexadecimal notation of the byte value).
The original character is replaced by the resulting character sequence.
Note: In a future edition of this specification, the XML Core Working Group intends to replace the preceding paragraph
							and list of steps with a normative reference to an upcoming revision of IETF RFC 3987, which will define
							"Legacy Extended IRIs (LEIRIs)".
When this revision is available, it is the intent of the XML Core WG to use it to replace
							language similar to the above in any future revisions of XML-related specifications under its purview.
[ Definition :  In addition to a system
identifier, an external identifier may include a public identifier .]
An XML processor attempting to retrieve the entity's content may use
any combination of
the public and system identifiers as well as additional information outside the
scope of this specification to try to generate an alternative URI reference.
If the processor is unable to do so, it MUST use the URI
reference specified in the system literal.
Before a match is attempted,
all strings of white space in the public identifier MUST be normalized to
single space characters (#x20), and leading and trailing white space MUST be removed.
Examples of external entity declarations: <!ENTITY open-hatch
         SYSTEM "http://www.textuality.com/boilerplate/OpenHatch.xml">
<!ENTITY open-hatch
         PUBLIC "-//Textuality//TEXT Standard open-hatch boilerplate//EN"
         "http://www.textuality.com/boilerplate/OpenHatch.xml">
<!ENTITY hatch-pic
         SYSTEM "../grafix/OpenHatch.gif"
         NDATA gif >  4.3 Parsed Entities  4.3.1 The Text Declaration External parsed entities SHOULD each begin with a text declaration .
Text Declaration [77] TextDecl ::= '<?xml' VersionInfo ?
EncodingDecl  S ?
'?>' The text declaration MUST be provided literally, not by reference
					to a parsed entity.
The text declaration MUST NOT appear at any
					position other than the beginning of an external parsed entity.
The text declaration
          in an external parsed entity is not considered part of its replacement text .
4.3.2 Well-Formed Parsed Entities The document entity is well-formed if it matches the production labeled document .
An external general parsed entity is well-formed
if it matches the production labeled extParsedEnt .
All
external parameter entities are well-formed by definition.
Note: Only parsed entities that are referenced directly or indirectly within the document are required to be well-formed.
Well-Formed External Parsed Entity [78] extParsedEnt ::=  TextDecl ?
content  An internal general parsed entity is well-formed if its replacement text
matches the production labeled content .
All internal
parameter entities are well-formed by definition.
A consequence of well-formedness in general
entities is that the logical and physical
structures in an XML document are properly nested; no start-tag , end-tag , empty-element tag , element , comment , processing instruction , character
reference , or entity reference can begin in one entity and end in another.
4.3.3 Character Encoding in Entities Each external parsed entity in an XML document may use a different encoding
for its characters.
All XML processors MUST be able to read entities in both
the UTF-8 and UTF-16 encodings.
The terms "UTF-8"
and "UTF-16" in this specification do not apply to related character encodings, including but not limited to UTF-16BE, UTF-16LE, or CESU-8.
Entities encoded in UTF-16 MUST and entities
encoded in UTF-8 MAY begin with the Byte Order Mark described by
Annex H of [ISO/IEC 10646:2000] , section 16.8 of [Unicode] (the ZERO WIDTH NO-BREAK SPACE character, #xFEFF).
This is an encoding signature,
not part of either the markup or the character data of the XML document.
XML
processors MUST be able to use this character to differentiate between UTF-8
and UTF-16 encoded documents.
If the replacement text of an external entity is to
						begin with the character U+FEFF, and no text declaration
						is present, then a Byte Order Mark MUST be present,
						whether the entity is encoded in UTF-8 or UTF-16.
Although an XML processor is required to read only entities in the UTF-8
and UTF-16 encodings, it is recognized that other encodings are used around
the world, and it may be desired for XML processors to read entities that
use them.
In
the absence of external character encoding information (such as MIME headers),
parsed entities which are stored in an encoding other than UTF-8 or UTF-16 MUST begin with a text declaration (see 4.3.1 The Text Declaration ) containing
an encoding declaration:  Encoding Declaration [80] EncodingDecl ::=  S 'encoding' Eq ('"' EncName '"' | "'" EncName "'" ) [81] EncName ::= [A-Za-z] ([A-Za-z0-9._] | '-')* /* Encoding
name contains only Latin characters */ In the document entity , the encoding
declaration is part of the XML declaration .
The EncName is the name of the encoding used.
In an encoding declaration, the values " UTF-8 ", " UTF-16 ",
" ISO-10646-UCS-2 ", and " ISO-10646-UCS-4 " SHOULD be used
for the various encodings and transformations of Unicode / ISO/IEC 10646,
the values " ISO-8859-1 ", " ISO-8859-2 ",
...
" ISO-8859-  n " (where n is the part number) SHOULD be used for the parts of ISO 8859, and
the values " ISO-2022-JP ", " Shift_JIS ",
and " EUC-JP " SHOULD be used for the various encoded
forms of JIS X-0208-1997.
It
is RECOMMENDED that character encodings registered (as charset s)
with the Internet Assigned Numbers Authority [IANA-CHARSETS] ,
other than those just listed, be referred to using their registered names;
other encodings SHOULD use names starting with an "x-" prefix.
XML processors SHOULD match character encoding names in a case-insensitive
way and SHOULD either interpret an IANA-registered name as the encoding registered
at IANA for that name or treat it as unknown (processors are, of course, not
required to support all IANA-registered encodings).
In the absence of information provided by an external transport protocol
(e.g.
HTTP or MIME), it is a fatal error for
an entity including an encoding declaration to be presented to the XML processor
in an encoding other than that named in the declaration, or for an entity which
begins with neither a Byte Order Mark
nor an encoding declaration to use an encoding other than UTF-8.
Note that
since ASCII is a subset of UTF-8, ordinary ASCII entities do not strictly
need an encoding declaration.
It is a fatal error for a TextDecl to occur other
than at the beginning of an external entity.
It is a fatal error when an XML processor
encounters an entity with an encoding that it is unable to process.
It
is a fatal error if an XML entity is determined (via default, encoding declaration,
or higher-level protocol) to be in a certain encoding but contains byte
sequences that are not legal in that encoding.
Specifically, it is a
fatal error if an entity encoded in UTF-8 contains any ill-formed code unit sequences,
as defined in section 3.9 of Unicode [Unicode] .
Unless an encoding
is determined by a higher-level protocol, it is also a fatal error if an XML entity
contains no encoding declaration and its content is not legal UTF-8 or UTF-16.
Examples of text declarations containing encoding declarations: <?xml encoding='UTF-8'?>
<?xml encoding='EUC-JP'?>  4.4 XML Processor Treatment of Entities and References The table below summarizes the contexts in which character references,
entity references, and invocations of unparsed entities might appear and the REQUIRED behavior of an XML processor in each case.
The labels in the leftmost column describe the recognition context: Reference in Content as a reference anywhere after the start-tag and before the end-tag of an element; corresponds
to the nonterminal content .
Reference in Attribute Value as a reference within either the value of an attribute in a start-tag ,
or a default value in an attribute declaration ;
corresponds to the nonterminal AttValue .
Occurs as Attribute Value as a Name , not a reference, appearing either as
the value of an attribute which has been declared as type ENTITY ,
or as one of the space-separated tokens in the value of an attribute which
has been declared as type ENTITIES .
Reference in Entity Value as a reference within a parameter or internal entity's literal
entity value in the entity's declaration; corresponds to the nonterminal EntityValue .
Reference in DTD as a reference within either the internal or external subsets of the DTD , but outside of an EntityValue , AttValue , PI , Comment , SystemLiteral , PubidLiteral ,
or the contents of an ignored conditional section (see 3.4 Conditional Sections ).
Entity
Type Character Parameter Internal General External Parsed
General Unparsed Reference
in Content  Not recognized   Included   Included
if validating   Forbidden   Included  Reference in Attribute Value  Not recognized   Included
in literal   Forbidden   Forbidden   Included  Occurs as Attribute
Value  Not recognized   Forbidden   Forbidden   Notify   Not recognized  Reference in EntityValue  Included in literal   Bypassed   Bypassed   Error   Included  Reference in DTD  Included as PE   Forbidden   Forbidden   Forbidden   Forbidden   4.4.1 Not Recognized Outside the DTD, the % character has no special significance;
thus, what would be parameter entity references in the DTD are not recognized
as markup in content .
Similarly, the names of unparsed
entities are not recognized except when they appear in the value of an appropriately
declared attribute.
4.4.2 Included [ Definition : An entity is included when its replacement text is retrieved
and processed, in place of the reference itself, as though it were part of
the document at the location the reference was recognized.]
The replacement
text may contain both character data and (except for parameter entities) markup ,
which MUST be recognized in the usual way.
(The string " AT&amp;T; "
expands to " AT&T; " and the remaining ampersand
is not recognized as an entity-reference delimiter.)
A character reference
is included when the indicated character is processed in place
of the reference itself.
4.4.3 Included If Validating When an XML processor recognizes a reference to a parsed entity, in order
to validate the document, the processor MUST  include its replacement text.
If
the entity is external, and the processor is not attempting to validate the
XML document, the processor MAY , but need
not, include the entity's replacement text.
If a non-validating processor
does not include the replacement text, it MUST inform the application that
it recognized, but did not read, the entity.
This rule is based on the recognition that the automatic inclusion provided
by the SGML and XML entity mechanism, primarily designed to support modularity
in authoring, is not necessarily appropriate for other applications, in particular
document browsing.
Browsers, for example, when encountering an external parsed
entity reference, might choose to provide a visual indication of the entity's
presence and retrieve it for display only on demand.
4.4.4 Forbidden The following are forbidden, and constitute fatal
errors : the appearance of a reference to an unparsed
entity , except in the EntityValue in an entity declaration.
the appearance of any character or general-entity reference in the
DTD except within an EntityValue or AttValue .
a reference to an external entity in an attribute value.
4.4.5 Included in Literal When an entity reference appears in
an attribute value, or a parameter entity reference appears in a literal entity
value, its replacement text  MUST be processed
in place of the reference itself as though it were part of the document at
the location the reference was recognized, except that a single or double
quote character in the replacement text MUST always be treated as a normal data
character and MUST NOT terminate the literal.
For example, this is well-formed: <!ENTITY % YN '"Yes"' >
<!ENTITY WhatHeSaid "He said %YN;" > while this is not: <!ENTITY EndAttr "27'" >
<element attribute='a-&EndAttr;>  4.4.6 Notify When the name of an unparsed entity appears as a token in the value of an attribute of declared type ENTITY or ENTITIES , a validating processor MUST inform the application of
the system and public (if any) identifiers for both the entity and its associated notation .
4.4.7 Bypassed When a general entity reference appears in the EntityValue in an entity declaration, it MUST be bypassed and left as is.
4.4.8 Included as PE Just as with external parsed entities, parameter entities need only be included if validating .
When a parameter-entity
reference is recognized in the DTD and included, its replacement
text  MUST be enlarged by the attachment of one leading and one following
space (#x20) character; the intent is to constrain the replacement text of
parameter entities to contain an integral number of grammatical tokens in
the DTD.
This
behavior MUST NOT apply to parameter entity references within entity values;
these are described in 4.4.5 Included in Literal .
4.4.9 Error It is an error for a reference to
		an unparsed entity to appear in the EntityValue in an
		entity declaration.
4.5 Construction of Entity Replacement Text In discussing the treatment of entities, it is useful to distinguish
two forms of the entity's value.
[ Definition : For an
internal entity, the literal
entity value is the quoted string actually present in the entity declaration,
corresponding to the non-terminal EntityValue .]
[ Definition : For an external entity, the literal
entity value is the exact text contained in the entity.]
[ Definition : For an
internal entity, the replacement text is the content of the entity, after replacement of character references and
parameter-entity references.]
[ Definition : For
an external entity, the replacement text is the content of the entity,
after stripping the text declaration (leaving any surrounding whitespace) if there
is one but without any replacement of character references or parameter-entity
references.]
The literal entity value as given in an internal entity declaration ( EntityValue ) may contain character, parameter-entity,
and general-entity references.
Such references MUST be contained entirely
within the literal entity value.
The actual replacement text that is included (or included in literal ) as described above MUST contain the replacement
text of any parameter entities referred to, and MUST contain the character
referred to, in place of any character references in the literal entity value;
however, general-entity references MUST be left as-is, unexpanded.
For example,
given the following declarations: <!ENTITY % pub    "&#xc9;ditions Gallimard" >
<!ENTITY   rights "All rights reserved" >
<!ENTITY   book   "La Peste: Albert Camus,
&#xA9; 1947 %pub;.
&rights;" > then the replacement text for the entity " book "
is: La Peste: Albert Camus,
© 1947 Éditions Gallimard.
&rights; The general-entity reference " &rights; " would
be expanded should the reference " &book; " appear
in the document's content or an attribute value.
These simple rules may have complex interactions; for a detailed discussion
of a difficult example, see D Expansion of Entity and Character References .
4.6 Predefined Entities [ Definition : Entity and character references may
both be used to escape the left angle bracket, ampersand, and
other delimiters.
A set of general entities ( amp , lt , gt , apos , quot ) is specified for
this purpose.
Numeric character references may also be used; they are expanded
immediately when recognized and MUST be treated as character data, so the
numeric character references " &#60; " and " &#38; " may be used to escape < and & when they occur
in character data.]
All XML processors MUST recognize these entities whether they are declared
or not.
For interoperability , valid XML
documents SHOULD declare these entities, like any others, before using them.
If
the entities lt or amp are declared, they MUST be
declared as internal entities whose replacement text is a character reference
to the respective
character (less-than sign or ampersand) being escaped; the double
escaping is REQUIRED for these entities so that references to them produce
a well-formed result.
If the entities gt , apos ,
or quot are declared, they MUST be declared as internal entities
whose replacement text is the single character being escaped (or a character
reference to that character; the double escaping here is OPTIONAL but harmless).
For example: <!ENTITY lt     "&#38;#60;">
<!ENTITY gt     "&#62;">
<!ENTITY amp    "&#38;#38;">
<!ENTITY apos   "&#39;">
<!ENTITY quot   "&#34;">  4.7 Notation Declarations [ Definition : Notations identify
by name the format of unparsed entities ,
the format of elements which bear a notation attribute, or the application
to which a processing instruction is addressed.]
[ Definition : Notation declarations provide a name for the notation, for use in entity and attribute-list declarations
and in attribute specifications, and an external identifier for the notation
which may allow an XML processor or its client application to locate a helper
application capable of processing data in the given notation.]
Notation Declarations [82] NotationDecl ::= '<!NOTATION' S  Name  S ( ExternalID | PublicID ) S ?
'>' [VC: Unique Notation Name] [83] PublicID ::= 'PUBLIC' S  PubidLiteral  Validity constraint: Unique Notation Name A given Name  MUST NOT be declared in more than one notation declaration.
XML processors MUST provide applications with the name and external identifier(s)
of any notation declared and referred to in an attribute value, attribute
definition, or entity declaration.
They MAY additionally resolve the external
identifier into the system identifier , file
name, or other information needed to allow the application to call a processor
for data in the notation described.
(It is not an error, however, for XML
documents to declare and refer to notations for which notation-specific applications
are not available on the system where the XML processor or application is
running.)
4.8 Document Entity [ Definition : The document entity serves as the root of the entity tree and a starting-point for an XML processor .]
This specification does
not specify how the document entity is to be located by an XML processor;
unlike other entities, the document entity has no name and might well appear
on a processor input stream without any identification at all.
5 Conformance  5.1 Validating and Non-Validating Processors Conforming XML processors fall into
two classes: validating and non-validating.
Validating and non-validating processors alike MUST report violations of
this specification's well-formedness constraints in the content of the document entity and any other parsed
entities that they read.
[ Definition : Validating
processors  MUST ,
at user option, report violations of the constraints expressed by
the declarations in the DTD , and failures
to fulfill the validity constraints given in this specification.]
To accomplish this, validating XML processors MUST read and process the entire
DTD and all external parsed entities referenced in the document.
Non-validating processors are REQUIRED to check only the document
entity , including the entire internal DTD subset, for well-formedness.
[ Definition :  While they are not required
to check the document for validity, they are REQUIRED to process all the declarations they read in the internal DTD subset and in any parameter
entity that they read, up to the first reference to a parameter entity that
they do not read; that is to say, they MUST use the information
in those declarations to normalize attribute values, include the replacement
text of internal entities, and supply default
attribute values .]
Except when standalone="yes" , they MUST NOT  process  entity
declarations or attribute-list declarations encountered after a reference to a parameter entity that is not read, since
the entity may have contained overriding declarations; when standalone="yes" , processors MUST process these declarations.
Note that when processing invalid documents with a non-validating
processor the application may not be presented with consistent
information.
For example, several requirements for uniqueness
within the document may not be met, including more than one element
with the same id, duplicate declarations of elements or notations
with the same name, etc.
In these cases the behavior of the parser
with respect to reporting such information to the application is
undefined.
5.2 Using XML Processors The behavior of a validating XML processor is highly predictable; it must
read every piece of a document and report all well-formedness and validity
violations.
Less is required of a non-validating processor; it need not read
any part of the document other than the document entity.
This has two effects
that may be important to users of XML processors: Certain well-formedness errors, specifically those that require reading
external entities, may fail to be detected by a non-validating processor.
Examples
include the constraints entitled Entity Declared , Parsed Entity , and No
Recursion , as well as some of the cases described as forbidden in 4.4 XML Processor Treatment of Entities and References .
The information passed from the processor to the application may
vary, depending on whether the processor reads parameter and external entities.
For example, a non-validating processor may fail to normalize attribute values, include the replacement
text of internal entities, or supply default
attribute values , where doing so depends on having read declarations
in external or parameter entities , or in the internal subset after an unread 
parameter entity reference .
For maximum reliability in interoperating between different XML processors,
applications which use non-validating processors SHOULD NOT rely on any behaviors
not required of such processors.
Applications which require DTD facilities not related to validation (such
as the declaration of default attributes and internal entities that are or may be specified in
external entities) SHOULD use validating XML processors.
6 Notation The formal grammar of XML is given in this specification using a simple
Extended Backus-Naur Form (EBNF) notation.
Each rule in the grammar defines
one symbol, in the form symbol ::= expression Symbols are written with an initial capital letter if they are the
start symbol of a regular language, otherwise with an initial lowercase letter.
Literal strings are quoted.
Within the expression on the right-hand side of a rule, the following expressions
are used to match strings of one or more characters:  #xN  where N is a hexadecimal integer, the expression matches the character
whose number
(code point) in ISO/IEC 10646 is N .
The number of leading zeros in the #xN form is insignificant.
[a-zA-Z] , [#xN-#xN]  matches any Char with a value in the range(s) indicated (inclusive).
[abc] , [#xN#xN#xN]  matches any Char with a value among the characters
enumerated.
Enumerations and ranges can be mixed in one set of brackets.
[^a-z] , [^#xN-#xN]  matches any Char with a value outside the range
indicated.
[^abc] , [^#xN#xN#xN]  matches any Char with a value not among the characters given.
Enumerations
and ranges of forbidden values can be mixed in one set of brackets.
"string"  matches a literal string matching that
given inside the double quotes.
'string'  matches a literal string matching that
given inside the single quotes.
These symbols may be combined to match more complex patterns as follows,
where A and B represent simple expressions: ( expression )  expression is treated as a unit and may be combined as described
in this list.
A?
matches A or nothing; optional A .
A B  matches A followed by B .
This
operator has higher precedence than alternation; thus A B | C D is identical to (A B) | (C D) .
A | B  matches A or B .
A - B  matches any string that matches A but does not match B .
A+  matches one or more occurrences of A .
Concatenation
has higher precedence than alternation; thus A+ | B+ is identical
to (A+) | (B+) .
A*  matches zero or more occurrences of A .
Concatenation
has higher precedence than alternation; thus A* | B* is identical
to (A*) | (B*) .
Other notations used in the productions are:  /* ...
*/  comment.
[ wfc: ...
]  well-formedness constraint; this identifies by name a constraint on well-formed documents associated with a production.
[ vc: ...
]  validity constraint; this identifies by name a constraint on valid documents associated with a production.
A References  A.1 Normative References IANA-CHARSETS (Internet
Assigned Numbers Authority) Official Names for Character Sets ,
ed.
Keld Simonsen et al.
(See http://www.iana.org/assignments/character-sets.)
IETF RFC 2119 IETF
(Internet Engineering Task Force).
RFC 2119: Key words for use in RFCs to Indicate Requirement Levels .
Scott Bradner, 1997.
(See http://www.ietf.org/rfc/rfc2119.txt.)
IETF BCP 47 IETF
  (Internet Engineering Task Force).
BCP 47, consisting of RFC 4646: Tags for Identifying Languages , and RFC 4647: Matching of Language Tags ,
						A.
Phillips, M.
Davis.
2006.
IETF RFC 3986 IETF (Internet Engineering Task Force).
RFC 3986: Uniform Resource Identifier (URI): Generic Syntax .
T.
Berners-Lee, R.
Fielding, L.
Masinter.
2005.
(See http://www.ietf.org/rfc/rfc3986.txt.)
ISO/IEC 10646 ISO (International
Organization for Standardization).
ISO/IEC 10646-1:2000.
Information
technology — Universal Multiple-Octet Coded Character Set (UCS) —
Part 1: Architecture and Basic Multilingual Plane and ISO/IEC 10646-2:2001.
Information technology — Universal Multiple-Octet Coded Character Set (UCS) — Part 2:
Supplementary Planes , as, from time to time, amended, replaced by a new edition or
expanded by the addition of new parts.
[Geneva]: International Organization for Standardization.
(See http://www.iso.org/iso/home.htm for the latest version.)
ISO/IEC 10646:2000 ISO (International
Organization for Standardization).
Information
technology — Universal Multiple-Octet Coded Character Set (UCS) —
Part 1: Architecture and Basic Multilingual Plane.
[Geneva]: International
Organization for Standardization, 2000.
Unicode The Unicode Consortium.
The Unicode
Standard, Version 5.0.0,  defined by: The Unicode Standard, Version 5.0 (Boston, MA,
Addison-Wesley, 2007.
ISBN 0-321-48091-0) .
UnicodeNormal The Unicode
Consortium.
Unicode normalization forms .
Mark Davis and
Martin Durst.
2008.
(See http://unicode.org/reports/tr15/.)
A.2 Other References Aho/Ullman Aho, Alfred V., Ravi Sethi, and Jeffrey D.
Ullman.
Compilers: Principles, Techniques, and Tools .
Reading: Addison-Wesley, 1986, rpt.
corr.
1988.
Brüggemann-Klein Brüggemann-Klein,
Anne.
Formal Models in Document Processing .
Habilitationsschrift.
Faculty
of Mathematics at the University of Freiburg, 1993.
(See ftp://ftp.informatik.uni-freiburg.de/documents/papers/brueggem/habil.ps.)
Brüggemann-Klein and Wood Brüggemann-Klein,
Anne, and Derick Wood.
Deterministic Regular Languages .
Universität Freiburg, Institut für Informatik, Bericht 38, Oktober 1991.
Extended
abstract in A.
Finkel, M.
Jantzen, Hrsg., STACS 1992, S.
173-184.
Springer-Verlag,
Berlin 1992.
Lecture Notes in Computer Science 577.
Full version titled One-Unambiguous
Regular Languages in Information and Computation 140 (2): 229-253,
February 1998.
Clark James Clark.
Comparison of SGML and XML .
(See http://www.w3.org/TR/NOTE-sgml-xml-971215.)
IANA-LANGCODES (Internet
Assigned Numbers Authority) Registry of Language Tags (See http://www.iana.org/assignments/language-subtag-registry.)
IETF RFC 2141 IETF
(Internet Engineering Task Force).
RFC 2141: URN Syntax , ed.
R.
Moats.
1997.
(See http://www.ietf.org/rfc/rfc2141.txt.)
IETF RFC 3023 IETF
(Internet Engineering Task Force).
RFC 3023: XML Media Types .
eds.
M.
Murata, S.
St.Laurent, D.
Kohn.
2001.
(See http://www.ietf.org/rfc/rfc3023.txt.)
IETF RFC 2781 IETF
(Internet Engineering Task Force).
RFC 2781: UTF-16, an encoding
of ISO 10646 , ed.
P.
Hoffman, F.
Yergeau.
2000.
(See http://www.ietf.org/rfc/rfc2781.txt.)
ISO 639 (International Organization for Standardization).
ISO 639:1988 (E).
Code for the representation of names of languages.
[Geneva]: International
Organization for Standardization, 1988.
ISO 3166 (International Organization for Standardization).
ISO 3166-1:1997
(E).
Codes for the representation of names of countries and their subdivisions —
Part 1: Country codes [Geneva]: International Organization for
Standardization, 1997.
ISO 8879 ISO (International Organization for Standardization).
ISO
8879:1986(E).
Information processing — Text and Office Systems —
Standard Generalized Markup Language (SGML).
First edition —
1986-10-15.
[Geneva]: International Organization for Standardization, 1986.
ISO/IEC 10744 ISO (International Organization for
Standardization).
ISO/IEC 10744-1992 (E).
Information technology —
Hypermedia/Time-based Structuring Language (HyTime).
[Geneva]:
International Organization for Standardization, 1992.
Extended Facilities
Annexe.
[Geneva]: International Organization for Standardization, 1996.
WEBSGML ISO
(International Organization for Standardization).
ISO 8879:1986
TC2.
Information technology — Document Description and Processing Languages .
[Geneva]: International Organization for Standardization, 1998.
(See http://www.sgmlsource.com/8879/n0029.htm.)
XML Names Tim Bray,
Dave Hollander, and Andrew Layman, editors.
Namespaces in XML .
Textuality, Hewlett-Packard, and Microsoft.
World Wide Web Consortium, 1999.
(See http://www.w3.org/TR/xml-names/.)
B Character Classes Because of changes to productions [4] and [5] , the productions in
				this Appendix are now orphaned and not used anymore in determining
				name characters.
This Appendix may be removed in a future edition of 
				this specification; other specifications that wish to refer to the productions herein should
				do so by means of a reference to the relevant production(s) in the Fourth Edition of this specification.
Following the characteristics defined in the Unicode standard, characters
are classed as base characters (among others, these contain the alphabetic
characters of the Latin alphabet), ideographic characters, and combining characters (among
others, this class contains most diacritics).
Digits and extenders are also
distinguished.
Characters [84] Letter ::=  BaseChar | Ideographic  [85] BaseChar ::= [#x0041-#x005A] | [#x0061-#x007A] | [#x00C0-#x00D6]
| [#x00D8-#x00F6] | [#x00F8-#x00FF] | [#x0100-#x0131] | [#x0134-#x013E]
| [#x0141-#x0148] | [#x014A-#x017E] | [#x0180-#x01C3] | [#x01CD-#x01F0]
| [#x01F4-#x01F5] | [#x01FA-#x0217] | [#x0250-#x02A8] | [#x02BB-#x02C1]
| #x0386 | [#x0388-#x038A] | #x038C | [#x038E-#x03A1]
| [#x03A3-#x03CE] | [#x03D0-#x03D6] | #x03DA | #x03DC
| #x03DE | #x03E0 | [#x03E2-#x03F3] | [#x0401-#x040C]
| [#x040E-#x044F] | [#x0451-#x045C] | [#x045E-#x0481] | [#x0490-#x04C4]
| [#x04C7-#x04C8] | [#x04CB-#x04CC] | [#x04D0-#x04EB] | [#x04EE-#x04F5]
| [#x04F8-#x04F9] | [#x0531-#x0556] | #x0559 | [#x0561-#x0586]
| [#x05D0-#x05EA] | [#x05F0-#x05F2] | [#x0621-#x063A] | [#x0641-#x064A]
| [#x0671-#x06B7] | [#x06BA-#x06BE] | [#x06C0-#x06CE] | [#x06D0-#x06D3]
| #x06D5 | [#x06E5-#x06E6] | [#x0905-#x0939] | #x093D
| [#x0958-#x0961] | [#x0985-#x098C] | [#x098F-#x0990] | [#x0993-#x09A8]
| [#x09AA-#x09B0] | #x09B2 | [#x09B6-#x09B9] | [#x09DC-#x09DD]
| [#x09DF-#x09E1] | [#x09F0-#x09F1] | [#x0A05-#x0A0A] | [#x0A0F-#x0A10]
| [#x0A13-#x0A28] | [#x0A2A-#x0A30] | [#x0A32-#x0A33] | [#x0A35-#x0A36]
| [#x0A38-#x0A39] | [#x0A59-#x0A5C] | #x0A5E | [#x0A72-#x0A74]
| [#x0A85-#x0A8B] | #x0A8D | [#x0A8F-#x0A91] | [#x0A93-#x0AA8]
| [#x0AAA-#x0AB0] | [#x0AB2-#x0AB3] | [#x0AB5-#x0AB9] | #x0ABD
| #x0AE0 | [#x0B05-#x0B0C] | [#x0B0F-#x0B10] | [#x0B13-#x0B28]
| [#x0B2A-#x0B30] | [#x0B32-#x0B33] | [#x0B36-#x0B39] | #x0B3D
| [#x0B5C-#x0B5D] | [#x0B5F-#x0B61] | [#x0B85-#x0B8A] | [#x0B8E-#x0B90]
| [#x0B92-#x0B95] | [#x0B99-#x0B9A] | #x0B9C | [#x0B9E-#x0B9F]
| [#x0BA3-#x0BA4] | [#x0BA8-#x0BAA] | [#x0BAE-#x0BB5] | [#x0BB7-#x0BB9]
| [#x0C05-#x0C0C] | [#x0C0E-#x0C10] | [#x0C12-#x0C28] | [#x0C2A-#x0C33]
| [#x0C35-#x0C39] | [#x0C60-#x0C61] | [#x0C85-#x0C8C] | [#x0C8E-#x0C90]
| [#x0C92-#x0CA8] | [#x0CAA-#x0CB3] | [#x0CB5-#x0CB9] | #x0CDE
| [#x0CE0-#x0CE1] | [#x0D05-#x0D0C] | [#x0D0E-#x0D10] | [#x0D12-#x0D28]
| [#x0D2A-#x0D39] | [#x0D60-#x0D61] | [#x0E01-#x0E2E] | #x0E30
| [#x0E32-#x0E33] | [#x0E40-#x0E45] | [#x0E81-#x0E82] | #x0E84
| [#x0E87-#x0E88] | #x0E8A | #x0E8D | [#x0E94-#x0E97]
| [#x0E99-#x0E9F] | [#x0EA1-#x0EA3] | #x0EA5 | #x0EA7
| [#x0EAA-#x0EAB] | [#x0EAD-#x0EAE] | #x0EB0 | [#x0EB2-#x0EB3]
| #x0EBD | [#x0EC0-#x0EC4] | [#x0F40-#x0F47] | [#x0F49-#x0F69]
| [#x10A0-#x10C5] | [#x10D0-#x10F6] | #x1100 | [#x1102-#x1103]
| [#x1105-#x1107] | #x1109 | [#x110B-#x110C] | [#x110E-#x1112]
| #x113C | #x113E | #x1140 | #x114C | #x114E | #x1150
| [#x1154-#x1155] | #x1159 | [#x115F-#x1161] | #x1163
| #x1165 | #x1167 | #x1169 | [#x116D-#x116E] | [#x1172-#x1173]
| #x1175 | #x119E | #x11A8 | #x11AB | [#x11AE-#x11AF]
| [#x11B7-#x11B8] | #x11BA | [#x11BC-#x11C2] | #x11EB
| #x11F0 | #x11F9 | [#x1E00-#x1E9B] | [#x1EA0-#x1EF9]
| [#x1F00-#x1F15] | [#x1F18-#x1F1D] | [#x1F20-#x1F45] | [#x1F48-#x1F4D]
| [#x1F50-#x1F57] | #x1F59 | #x1F5B | #x1F5D | [#x1F5F-#x1F7D]
| [#x1F80-#x1FB4] | [#x1FB6-#x1FBC] | #x1FBE | [#x1FC2-#x1FC4]
| [#x1FC6-#x1FCC] | [#x1FD0-#x1FD3] | [#x1FD6-#x1FDB] | [#x1FE0-#x1FEC]
| [#x1FF2-#x1FF4] | [#x1FF6-#x1FFC] | #x2126 | [#x212A-#x212B]
| #x212E | [#x2180-#x2182] | [#x3041-#x3094] | [#x30A1-#x30FA]
| [#x3105-#x312C] | [#xAC00-#xD7A3] [86] Ideographic ::= [#x4E00-#x9FA5] | #x3007 | [#x3021-#x3029] [87] CombiningChar ::= [#x0300-#x0345] | [#x0360-#x0361] | [#x0483-#x0486]
| [#x0591-#x05A1] | [#x05A3-#x05B9] | [#x05BB-#x05BD] | #x05BF
| [#x05C1-#x05C2] | #x05C4 | [#x064B-#x0652] | #x0670
| [#x06D6-#x06DC] | [#x06DD-#x06DF] | [#x06E0-#x06E4] | [#x06E7-#x06E8]
| [#x06EA-#x06ED] | [#x0901-#x0903] | #x093C | [#x093E-#x094C]
| #x094D | [#x0951-#x0954] | [#x0962-#x0963] | [#x0981-#x0983]
| #x09BC | #x09BE | #x09BF | [#x09C0-#x09C4] | [#x09C7-#x09C8]
| [#x09CB-#x09CD] | #x09D7 | [#x09E2-#x09E3] | #x0A02
| #x0A3C | #x0A3E | #x0A3F | [#x0A40-#x0A42] | [#x0A47-#x0A48]
| [#x0A4B-#x0A4D] | [#x0A70-#x0A71] | [#x0A81-#x0A83] | #x0ABC
| [#x0ABE-#x0AC5] | [#x0AC7-#x0AC9] | [#x0ACB-#x0ACD] | [#x0B01-#x0B03]
| #x0B3C | [#x0B3E-#x0B43] | [#x0B47-#x0B48] | [#x0B4B-#x0B4D]
| [#x0B56-#x0B57] | [#x0B82-#x0B83] | [#x0BBE-#x0BC2] | [#x0BC6-#x0BC8]
| [#x0BCA-#x0BCD] | #x0BD7 | [#x0C01-#x0C03] | [#x0C3E-#x0C44]
| [#x0C46-#x0C48] | [#x0C4A-#x0C4D] | [#x0C55-#x0C56] | [#x0C82-#x0C83]
| [#x0CBE-#x0CC4] | [#x0CC6-#x0CC8] | [#x0CCA-#x0CCD] | [#x0CD5-#x0CD6]
| [#x0D02-#x0D03] | [#x0D3E-#x0D43] | [#x0D46-#x0D48] | [#x0D4A-#x0D4D]
| #x0D57 | #x0E31 | [#x0E34-#x0E3A] | [#x0E47-#x0E4E]
| #x0EB1 | [#x0EB4-#x0EB9] | [#x0EBB-#x0EBC] | [#x0EC8-#x0ECD]
| [#x0F18-#x0F19] | #x0F35 | #x0F37 | #x0F39 | #x0F3E
| #x0F3F | [#x0F71-#x0F84] | [#x0F86-#x0F8B] | [#x0F90-#x0F95]
| #x0F97 | [#x0F99-#x0FAD] | [#x0FB1-#x0FB7] | #x0FB9
| [#x20D0-#x20DC] | #x20E1 | [#x302A-#x302F] | #x3099
| #x309A [88] Digit ::= [#x0030-#x0039] | [#x0660-#x0669] | [#x06F0-#x06F9]
| [#x0966-#x096F] | [#x09E6-#x09EF] | [#x0A66-#x0A6F] | [#x0AE6-#x0AEF]
| [#x0B66-#x0B6F] | [#x0BE7-#x0BEF] | [#x0C66-#x0C6F] | [#x0CE6-#x0CEF]
| [#x0D66-#x0D6F] | [#x0E50-#x0E59] | [#x0ED0-#x0ED9] | [#x0F20-#x0F29] [89] Extender ::= #x00B7 | #x02D0 | #x02D1 | #x0387 | #x0640
| #x0E46 | #x0EC6 | #x3005 | [#x3031-#x3035] | [#x309D-#x309E]
| [#x30FC-#x30FE] The character classes defined here can be derived from the Unicode 2.0
character database as follows: Name start characters must have one of the categories Ll, Lu, Lo,
Lt, Nl.
Name characters other than Name-start characters must have one of
the categories Mc, Me, Mn, Lm, or Nd.
Characters in the compatibility area (i.e.
with character code greater
than #xF900 and less than #xFFFE) are not allowed in XML names.
Characters which have a font or compatibility decomposition (i.e.
those with a "compatibility formatting tag" in field 5 of the
database -- marked by field 5 beginning with a "<") are not
allowed.
The following characters are treated as name-start characters rather
than name characters, because the property file classifies them as Alphabetic:
[#x02BB-#x02C1], #x0559, #x06E5, #x06E6.
Characters #x20DD-#x20E0 are excluded (in accordance with Unicode 2.0,
section 5.14).
Character #x00B7 is classified as an extender, because the property
list so identifies it.
Character #x0387 is added as a name character, because #x00B7 is
its canonical equivalent.
Characters ':' and '_' are allowed as name-start characters.
Characters '-' and '.'
are allowed as name characters.
C XML and SGML (Non-Normative) XML
is designed to be a subset of SGML, in that every XML document should also
be a conforming SGML document.
For a detailed comparison of the additional
restrictions that XML places on documents beyond those of SGML, see [Clark] .
D Expansion of Entity and Character References (Non-Normative) This appendix contains some examples illustrating the sequence of entity-
and character-reference recognition and expansion, as specified in 4.4 XML Processor Treatment of Entities and References .
If the DTD contains the declaration <!ENTITY example "<p>An ampersand (&#38;#38;) may be escaped
numerically (&#38;#38;#38;) or with a general entity
(&amp;amp;).</p>" > then the XML processor will recognize the character references when it
parses the entity declaration, and resolve them before storing the following
string as the value of the entity " example ": <p>An ampersand (&#38;) may be escaped
numerically (&#38;#38;) or with a general entity
(&amp;amp;).</p> A reference in the document to " &example; "
will cause the text to be reparsed, at which time the start- and end-tags
of the p element will be recognized and the three references will
be recognized and expanded, resulting in a p element with the following
content (all data, no delimiters or markup): An ampersand (&) may be escaped
numerically (&#38;) or with a general entity
(&amp;).
A more complex example will illustrate the rules and their effects fully.
In the following example, the line numbers are solely for reference.
1 <?xml version='1.0'?>
2 <!DOCTYPE test [
3 <!ELEMENT test (#PCDATA) >
4 <!ENTITY % xx '&#37;zz;'>
5 <!ENTITY % zz '&#60;!ENTITY tricky "error-prone" >' >
6 %xx;
7 ]>
8 <test>This sample shows a &tricky; method.</test> This produces the following: in line 4, the reference to character 37 is expanded immediately,
and the parameter entity " xx " is stored in the symbol
table with the value " %zz; ".
Since the replacement
text is not rescanned, the reference to parameter entity " zz "
is not recognized.
(And it would be an error if it were, since " zz "
is not yet declared.)
in line 5, the character reference " &#60; "
is expanded immediately and the parameter entity " zz "
is stored with the replacement text " <!ENTITY tricky "error-prone"
> ", which is a well-formed entity declaration.
in line 6, the reference to " xx " is recognized,
and the replacement text of " xx " (namely " %zz; ")
is parsed.
The reference to " zz " is recognized in
its turn, and its replacement text (" <!ENTITY tricky "error-prone"
> ") is parsed.
The general entity " tricky "
has now been declared, with the replacement text " error-prone ".
in line 8, the reference to the general entity " tricky "
is recognized, and it is expanded, so the full content of the test element is the self-describing (and ungrammatical) string This sample
shows a error-prone method.
In the following example <!DOCTYPE foo [ 
<!ENTITY x "&lt;"> 
]> 
<foo attr="&x;"/> the replacement text of x is the four characters "&lt;" because
				references to general entities in entity values are bypassed .
The replacement text of lt is a character reference to
				the less-than character, for example the five characters "&#60;"
				(see 4.6 Predefined Entities ).
Since neither of these contains a less-than character
				the result is well-formed.
If the definition of x had been <!ENTITY x "&#60;"> then the document would not have been well-formed, because the
				replacement text of x would be the single character "<" which
				is not permitted in attribute values (see WFC: No < in Attribute Values ).
E Deterministic Content Models (Non-Normative) As
noted in 3.2.1 Element Content , it is required that content
models in element type declarations be deterministic.
This requirement is for compatibility with SGML (which calls deterministic
content models "unambiguous"); XML processors built
using SGML systems may flag non-deterministic content models as errors.
For example, the content model ((b, c) | (b, d)) is non-deterministic,
because given an initial b the XML processor
cannot know which b in the model is being matched without looking
ahead to see which element follows the b .
In this case, the two references
to b can be collapsed into a single reference, making the model read (b,
(c | d)) .
An initial b now clearly matches only a single name
in the content model.
The processor doesn't need to look ahead to see what follows; either c or d would be accepted.
More formally: a finite state automaton may be constructed from the content
model using the standard algorithms, e.g.
algorithm 3.5 in section 3.9 of
Aho, Sethi, and Ullman [Aho/Ullman] .
In many such algorithms, a follow
set is constructed for each position in the regular expression (i.e., each
leaf node in the syntax tree for the regular expression); if any position
has a follow set in which more than one following position is labeled with
the same element type name, then the content model is in error and may be
reported as an error.
Algorithms exist which allow many but not all non-deterministic content
models to be reduced automatically to equivalent deterministic models; see
Brüggemann-Klein 1991 [Brüggemann-Klein] .
F Autodetection of Character Encodings (Non-Normative) The XML encoding declaration functions as an internal label on each entity,
indicating which character encoding is in use.
Before an XML processor can
read the internal label, however, it apparently has to know what character
encoding is in use—which is what the internal label is trying to indicate.
In the general case, this is a hopeless situation.
It is not entirely hopeless
in XML, however, because XML limits the general case in two ways: each implementation
is assumed to support only a finite set of character encodings, and the XML
encoding declaration is restricted in position and content in order to make
it feasible to autodetect the character encoding in use in each entity in
normal cases.
Also, in many cases other sources of information are available
in addition to the XML data stream itself.
Two cases may be distinguished,
depending on whether the XML entity is presented to the processor without,
or with, any accompanying (external) information.
We will consider these cases in turn.
F.1 Detection Without External Encoding Information Because each XML entity not accompanied by external
encoding information and not in UTF-8 or UTF-16 encoding must
begin with an XML encoding declaration, in which the first characters must
be ' <?xml ', any conforming processor can detect, after two
to four octets of input, which of the following cases apply.
In reading this
list, it may help to know that in UCS-4, '<' is " #x0000003C "
and '?'
is " #x0000003F ", and the Byte Order Mark
required of UTF-16 data streams is " #xFEFF ".
The notation ## is used to denote any byte value except that two consecutive ## s cannot be both 00.
With a Byte Order Mark:  00 00 FE
FF  UCS-4, big-endian machine (1234 order)  FF
FE 00 00  UCS-4, little-endian machine (4321 order)  00 00 FF FE  UCS-4, unusual octet order (2143)  FE FF 00 00  UCS-4, unusual octet order (3412)  FE FF ## ##  UTF-16, big-endian  FF FE ## ##  UTF-16, little-endian  EF BB BF  UTF-8 Without a Byte Order Mark:  00 00 00 3C  UCS-4 or other encoding with a 32-bit code unit and ASCII
characters encoded as ASCII values, in respectively big-endian (1234), little-endian
(4321) and two unusual byte orders (2143 and 3412).
The encoding declaration
must be read to determine which of UCS-4 or other supported 32-bit encodings
applies.
3C 00 00 00   00 00 3C 00   00 3C 00 00   00 3C 00 3F  UTF-16BE or big-endian ISO-10646-UCS-2
or other encoding with a 16-bit code unit in big-endian order and ASCII characters
encoded as ASCII values (the encoding declaration must be read to determine
which)  3C 00 3F 00  UTF-16LE or little-endian
ISO-10646-UCS-2 or other encoding with a 16-bit code unit in little-endian
order and ASCII characters encoded as ASCII values (the encoding declaration
must be read to determine which)  3C 3F 78 6D  UTF-8, ISO 646, ASCII, some part of ISO 8859, Shift-JIS, EUC, or any other
7-bit, 8-bit, or mixed-width encoding which ensures that the characters of
ASCII have their normal positions, width, and values; the actual encoding
declaration must be read to detect which of these applies, but since all of
these encodings use the same bit patterns for the relevant ASCII characters,
the encoding declaration itself may be read reliably  4C
6F A7 94  EBCDIC (in some flavor; the full encoding declaration
must be read to tell which code page is in use) Other UTF-8 without an encoding declaration, or else the data stream is mislabeled
(lacking a required encoding declaration), corrupt, fragmentary, or enclosed
in a wrapper of some kind Note: In cases above which do not require reading the encoding declaration to
determine the encoding, section 4.3.3 still requires that the encoding declaration,
if present, be read and that the encoding name be checked to match the actual
encoding of the entity.
Also, it is possible that new character encodings
will be invented that will make it necessary to use the encoding declaration
to determine the encoding, in cases where this is not required at present.
This level of autodetection is enough to read the XML encoding declaration
and parse the character-encoding identifier, which is still necessary to distinguish
the individual members of each family of encodings (e.g.
to tell UTF-8 from
8859, and the parts of 8859 from each other, or to distinguish the specific
EBCDIC code page in use, and so on).
Because the contents of the encoding declaration are restricted to characters
from the ASCII repertoire (however encoded),
a processor can reliably read the entire encoding declaration as soon as it
has detected which family of encodings is in use.
Since in practice, all widely
used character encodings fall into one of the categories above, the XML encoding
declaration allows reasonably reliable in-band labeling of character encodings,
even when external sources of information at the operating-system or transport-protocol
level are unreliable.
Character encodings such as UTF-7
that make overloaded usage of ASCII-valued bytes may fail to be reliably detected.
Once the processor has detected the character encoding in use, it can act
appropriately, whether by invoking a separate input routine for each case,
or by calling the proper conversion function on each character of input.
Like any self-labeling system, the XML encoding declaration will not work
if any software changes the entity's character set or encoding without updating
the encoding declaration.
Implementors of character-encoding routines should
be careful to ensure the accuracy of the internal and external information
used to label the entity.
F.2 Priorities in the Presence of External Encoding Information The second possible case occurs when the XML entity is accompanied by encoding
information, as in some file systems and some network protocols.
When multiple
sources of information are available, their relative priority and the preferred
method of handling conflict should be specified as part of the higher-level
protocol used to deliver XML.
In particular, please refer
to [IETF RFC 3023] or its successor, which defines the text/xml and application/xml MIME types and provides some useful guidance.
In the interests of interoperability, however, the following rule is recommended.
If an XML entity is in a file, the Byte-Order Mark and encoding declaration are used
(if present) to determine the character encoding.
G W3C XML Working Group (Non-Normative) This specification was prepared and approved for publication by the W3C
XML Working Group (WG).
WG approval of this specification does not necessarily
imply that all WG members voted for its approval.
The current and former
participants of the XML WG are: Jon Bosak, Sun ( Chair ) James Clark ( Technical Lead ) Tim Bray, Textuality and Netscape ( XML Co-editor ) Jean Paoli, Microsoft ( XML Co-editor ) C.
M.
Sperberg-McQueen, U.
of Ill.
( XML Co-editor ) Dan Connolly, W3C ( W3C Liaison ) Paula Angerstein, Texcel Steve DeRose, INSO Dave Hollander, HP Eliot Kimber, ISOGEN Eve Maler, ArborText Tom Magliery, NCSA Murray Maloney, SoftQuad, Grif SA, Muzmo and Veo Systems MURATA Makoto (FAMILY Given), Fuji Xerox Information Systems Joel Nava, Adobe Conleth O'Connell, Vignette Peter Sharpe, SoftQuad John Tigue, DataChannel  H W3C XML Core Working Group (Non-Normative) The fifth edition of this specification was prepared by the W3C XML Core
Working Group (WG).
The participants in the WG at the time of publication of this
edition were: John Cowan, Google Andrew Fang, PTC-Arbortext Paul Grosso, PTC-Arbortext ( Co-Chair ) Konrad Lanz, A-SIT Glenn Marcy, IBM Henry Thompson, W3C ( Staff Contact ) Richard Tobin, University of Edinburgh Daniel Veillard Norman Walsh, Mark Logic ( Co-Chair ) François Yergeau  I Production Notes (Non-Normative) This edition was encoded in a
slightly modified version of the XMLspec DTD, v2.10 .
The XHTML versions were produced with a combination of the xmlspec.xsl , diffspec.xsl ,
and REC-xml.xsl XSLT stylesheets.
J Suggestions for XML Names (Non-Normative) The following suggestions define what is believed to be best
				practice in the construction of XML names used as element names,
				attribute names, processing instruction targets, entity names,
				notation names, and the values of attributes of type ID, and are
				intended as guidance for document authors and schema designers.
All references to Unicode are understood with respect to
				a particular version of the Unicode Standard greater than or equal
				to 5.0; which version should be used is left to the discretion of
				the document author or schema designer.
The first two suggestions are directly derived from the rules
				given for identifiers in Standard Annex #31 (UAX #31) of the Unicode Standard, version 5.0 [Unicode] , and
				exclude all control characters, enclosing nonspacing marks,
				non-decimal numbers, private-use characters, punctuation characters
				(with the noted exceptions), symbol characters, unassigned
				codepoints, and white space characters.
The other suggestions
				are mostly derived from Appendix B in previous editions of this specification.
The first character of any name should have a Unicode property
						of ID_Start, or else be '_' #x5F.
Characters other than the first should have a Unicode property
						of ID_Continue, or be one of the characters listed in the table
						entitled "Characters for Natural Language Identifiers" in UAX
						#31, with the exception of "'" #x27 and "’" #x2019.
Characters in names should be expressed using
Normalization Form C as defined in [UnicodeNormal] .
Ideographic characters which have a canonical decomposition
						(including those in the ranges [#xF900-#xFAFF] and
						[#x2F800-#x2FFFD], with 12 exceptions) should not be used in names.
Characters which have a compatibility decomposition (those with
						a "compatibility formatting tag" in field 5 of the Unicode
						Character Database -- marked by field 5 beginning with a "<")
						should not be used in names.
This suggestion does not apply
						to characters which
						despite their compatibility decompositions are in regular use in
						their scripts, for
example #x0E33 THAI CHARACTER SARA AM or #x0EB3 LAO CHARACTER AM.
Combining characters meant for use with symbols only (including
						those in the ranges [#x20D0-#x20EF] and [#x1D165-#x1D1AD]) should
						not be used in names.
The interlinear annotation characters ([#xFFF9-#xFFFB]) should
						not be used in names.
Variation selector characters should not be used in names.
Names which are nonsensical, unpronounceable, hard to read, or
						easily confusable with other names should not be employed.
Soup Sieve       GitHub         Home         Home     API      F.A.Q.
Beautiful Soup Differences           CSS Selectors         CSS Selectors     Basic Selectors      Combinators and Lists      Pseudo Classes      Non-Applicable Pseudo Classes          About      About     Contributing & Support      Development      Security Vulnerabilities      Changelog      License                 Table of contents    Overview    Installation    Usage          Quick Start  Overview  Soup Sieve is a CSS selector library designed to be used with Beautiful Soup 4 .
It aims to provide selecting, matching, and filtering using modern CSS selectors.
Soup Sieve currently provides selectors from the CSS level 1 specifications up through the latest CSS level 4 drafts and beyond (though some are not yet implemented).
Soup Sieve was written with the intent to replace Beautiful Soup's builtin select feature, and as of Beautiful Soup version 4.7.0, it now is .
Soup Sieve can also be imported in order to use its API directly for more controlled, specialized parsing.
Soup Sieve has implemented most of the CSS selectors up through the latest CSS draft specifications, though there are a number that don't make sense in a non-browser environment.
Selectors that cannot provide meaningful functionality simply do not match anything.
Some of the supported selectors are:   .
classes  # ids  [ attributes = value ]  parent  child  parent  >  child  sibling  ~  sibling  sibling  +  sibling  : not ( element .
class ,  element2 .
class )  : is ( element .
class )  parent : has (>  child )  and many more   Installation  You must have Beautiful Soup already installed:  pip install beautifulsoup4  In most cases, assuming you've installed version 4.7.0, that should be all you need to do, but if you've installed via some alternative method, and Soup Sieve is not automatically installed, you can install it directly:  pip install soupsieve  If you want to manually install it from source, first ensure that build is installed:  pip install build  Then navigate to the root of the project and build the wheel and install (replacing <ver> with the current version):  python -m build -w
pip install dist/soupsive-<ver>-py3-none-any.whl  Usage  To use Soup Sieve, you must create a BeautifulSoup object:  >>> import  bs4  >>> text  =  """  ...
<div>  ...
<!-- These are animals -->  ...
<p class="a">Cat</p>  ...
<p class="b">Dog</p>  ...
<p class="c">Mouse</p>  ...
</div>  ...
"""  >>> soup  =  bs4 .
BeautifulSoup ( text ,  'html5lib' )   For most people, using the Beautiful Soup 4.7.0+ API may be more than sufficient.
Beautiful Soup offers two methods that employ Soup Sieve: select and select_one .
Beautiful Soup's select API is identical to Soup Sieve's, except that you don't have to hand it the tag object, the calling object passes itself to Soup Sieve:  >>> soup  =  bs4 .
BeautifulSoup ( text ,  'html5lib' )  >>> soup .
select_one ( 'p:is(.a, .b, .c)' )  <p class="a">Cat</p>   >>> soup  =  bs4 .
select ( 'p:is(.a, .b, .c)' )  [<p class="a">Cat</p>, <p class="b">Dog</p>, <p class="c">Mouse</p>]   You can also use the Soup Sieve API directly to get access to the full range of possibilities that Soup Sieve offers.
You can select a single tag:  >>> import  soupsieve  as  sv  >>> sv .
select_one ( 'p:is(.a, .b, .c)' ,  soup )  <p class="a">Cat</p>   You can select all tags:  >>> import  soupsieve  as  sv  >>> sv .
select ( 'p:is(.a, .b, .c)' ,  soup )  [<p class="a">Cat</p>, <p class="b">Dog</p>, <p class="c">Mouse</p>]   You can select the closest ancestor:  >>> import  soupsieve  as  sv  >>> el  =  sv .
select_one ( '.c' ,  soup )  >>> sv .
closest ( 'div' ,  el )  <div>  <!-- These are animals -->  <p class="a">Cat</p>  <p class="b">Dog</p>  <p class="c">Mouse</p>  </div>   You can filter a tag's Children (or an iterable of tags):  >>> sv .
filter ( 'p:not(.b)' ,  soup .
div )  [<p class="a">Cat</p>, <p class="c">Mouse</p>]   You can match a single tag:  >>> els  =  sv .
select ( 'p:is(.a, .b, .c)' ,  soup )  >>> sv .
match ( 'p:not(.b)' ,  els [ 0 ])  True  >>> sv .
match ( 'p:not(.b)' ,  els [ 1 ])  False   Or even just extract comments:  >>> sv .
comments ( soup )  [' These are animals ']   Selectors do not have to be constrained to one line either.
You can span selectors over multiple lines just like you would in a CSS file.
>>> selector  =  """  ...
.a,  ...
.b,  ...
.c  ...
"""  >>> sv .
select ( selector ,  soup )  [<p class="a">Cat</p>, <p class="b">Dog</p>, <p class="c">Mouse</p>]   You can even use comments to annotate a particularly complex selector.
>>> selector  =  """  ...
/* This isn't complicated, but we're going to annotate it anyways.
...
This is the a class */  ...
.a,  ...
/* This is the b class */  ...
.b,  ...
/* This is the c class */  ...
.c  ...
"""  >>> sv .
select ( selector ,  soup )  [<p class="a">Cat</p>, <p class="b">Dog</p>, <p class="c">Mouse</p>]   If you've ever used Python's Re library for regular expressions, you may know that it is often useful to pre-compile a regular expression pattern, especially if you plan to use it more than once.
The same is true for Soup Sieve's matchers, though is not required.
If you have a pattern that you want to use more than once, it may be wise to pre-compile it early on:  >>> selector  =  sv .
compile ( 'p:is(.a, .b, .c)' )  >>> selector .
filter ( soup .
div )  [<p class="a">Cat</p>, <p class="b">Dog</p>, <p class="c">Mouse</p>]   A compiled object has all the same methods, though the parameters will be slightly different as they don't need things like the pattern or flags once compiled.
See API documentation for more info.
Compiled patterns are cached, so if for any reason you need to clear the cache, simply issue the purge command.
>>> sv .
purge ()     Last update: August 31, 2023                 Back to top          Soup Sieve       GitHub         Home         Home     API      F.A.Q.
Beautiful Soup Differences           CSS Selectors         CSS Selectors     Basic Selectors      Combinators and Lists      Pseudo Classes      Non-Applicable Pseudo Classes          About      About     Contributing & Support      Development      Security Vulnerabilities      Changelog      License                 Table of contents    Implementation Specifics    Selector Context Key    Selector Terminology     Selector    Simple Selector    Compound Selector    Complex Selector    Selector List             General Details  Implementation Specifics  The CSS selectors are based off of the CSS specification and includes not only stable selectors, but may also include selectors currently under development from the draft specifications.
Primarily support has been added for selectors that were feasible to implement and most likely to get practical use.
In addition to the selectors in the specification, Soup Sieve also supports a couple non-standard selectors.
Soup Sieve aims to allow users to target XML/HTML elements with CSS selectors.
It implements many pseudo classes, but it does not currently implement any pseudo elements and has no plans to do so.
Soup Sieve also will not match anything for pseudo classes that are only relevant in a live, browser environment, but it will gracefully handle them if they've been implemented; such pseudo classes are non-applicable in the Beautiful Soup environment and are noted in Non-Applicable Pseudo Classes .
When speaking about namespaces, they only apply to XML, XHTML, or when dealing with recognized foreign tags in HTML5.
Currently, Beautiful Soup's html5lib parser is the only parser that will return the appropriate namespaces for a HTML5 document.
If you are using XHTML, you have to use the Beautiful Soup's lxml-xml parser (or xml for short) to get the appropriate namespaces in an XHTML document.
In addition to using the correct parser, you must provide a dictionary of namespaces to Soup Sieve in order to use namespace selectors.
See the documentation on namespaces to learn more.
While an effort is made to mimic CSS selector behavior, there may be some differences or quirks, please report issues if any are found.
Selector Context Key   Symbol  Name  Description    HTML  Some selectors are very specific to HTML and either have no meaningful representation in XML, or such functionality has not been implemented.
Selectors that are HTML only will be noted with , and will match nothing if used in XML.
Custom  Soup Sieve has implemented a couple non-standard selectors.
These can contain useful selectors that were rejected from the official CSS specifications, selectors implemented by other systems such as JQuery, or even selectors specifically created for Soup Sieve.
If a selector is considered non standard, it will be marked with .
Experimental  All selectors that are from the current working draft of CSS4 are considered experimental and are marked with .
Additionally, if there are other immature selectors, they may be marked as experimental as well.
Experimental may mean we are not entirely sure if our implementation is correct, that things may still be in flux as they are part of a working draft, or even both.
If at anytime a working draft drops a selector from the current draft, it will most likely also be removed here, most likely with a deprecation path, except where there may be a conflict that requires a less graceful transition.
One exception is in the rare case that the selector is found to be far too useful despite being rejected.
In these cases, we may adopt them as "custom" selectors.
Additional Reading  If usage of a selector is not clear in this documentation, you can find more information by reading these specification documents:  CSS Level 3 Specification : Contains the latest official document outlying official behaviors of CSS selectors.
CSS Level 4 Working Draft : Contains the latest published working draft of the CSS level 4 selectors which outlines the experimental new selectors and experimental behavioral changes.
HTML5 : The HTML 5.0 specification document.
Defines the semantics regarding HTML.
HTML Living Standard : The HTML Living Standard document.
Defines semantics regarding HTML.
Selector Terminology  Certain terminology is used throughout this document when describing selectors.
In order to fully understand the syntax a selector may implement, it is important to understand a couple of key terms.
Selector  Selector is used to describe any selector whether it is a simple , compound , or complex selector.
Simple Selector  A simple selector represents a single condition on an element.
It can be a type selector , universal selector , ID selector , class selector , attribute selector , or pseudo class selector .
Compound Selector  A compound selector is a sequence of simple selectors.
They do not contain any combinators .
If a universal or type selector is used, they must come first, and only one instance of either a universal or type selector can be used, both cannot be used at the same time.
Complex Selector  A complex selector consists of multiple simple or compound selectors joined with combinators .
Selector List  A selector list is a list of selectors joined with a comma ( , ).
A selector list is used to specify that a match is valid if any of the selectors in a list matches.
Last update: August 31, 2023                 Back to top   Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
HTML Living Standard — Last Updated 15 April 2024   ← 12 Web storage — Table of Contents — 13.2 Parsing HTML documents → 13 The HTML syntax 13.1 Writing HTML documents 13.1.1 The DOCTYPE 13.1.2 Elements 13.1.2.1 Start tags 13.1.2.2 End tags 13.1.2.3 Attributes 13.1.2.4 Optional tags 13.1.2.5 Restrictions on content models 13.1.2.6 Restrictions on the contents of raw text and escapable raw text elements 13.1.3 Text 13.1.3.1 Newlines 13.1.4 Character references 13.1.5 CDATA sections 13.1.6 Comments 13  The HTML syntax  This section only describes the rules for resources labeled with an HTML
  MIME type .
Rules for XML resources are discussed in the section below entitled " The
  XML syntax ".
13.1 Writing HTML documents  This section only applies to documents, authoring tools, and markup generators.
In
  particular, it does not apply to conformance checkers; conformance checkers must use the
  requirements given in the next section ("parsing HTML documents").
Documents must consist of the following parts, in the given
  order:  Optionally, a single U+FEFF BYTE ORDER MARK (BOM) character.
Any number of comments and ASCII
   whitespace .
A DOCTYPE .
The document element , in the form of an html  element .
The various types of content mentioned above are described in the next few sections.
In addition, there are some restrictions on how character encoding declarations are to be serialized, as discussed in the
  section on that topic.
ASCII whitespace before the html element, at the start of the html element and before the head element, will be dropped when the
   document is parsed; ASCII whitespace  after the html element
   will be parsed as if it were at the end of the body element.
Thus, ASCII
   whitespace around the document element does not round-trip.
It is suggested that newlines be inserted after the DOCTYPE, after any comments that are
   before the document element, after the html element's start tag (if it is not omitted ), and after any comments that are inside the html element but before the head element.
Many strings in the HTML syntax (e.g.
the names of elements and their attributes) are
  case-insensitive, but only for ASCII upper alphas and ASCII lower alphas .
For convenience, in this section this
  is just referred to as "case-insensitive".
13.1.1 The DOCTYPE  A DOCTYPE is a 
  required preamble.
DOCTYPEs are required for legacy reasons.
When omitted, browsers tend to use a
  different rendering mode that is incompatible with some specifications.
Including the DOCTYPE in a
  document ensures that the browser makes a best-effort attempt at following the relevant
  specifications.
A DOCTYPE must consist of the following components, in this order:  A string that is an ASCII case-insensitive match for the string " <!DOCTYPE ".
One or more ASCII whitespace .
A string that is an ASCII case-insensitive match for the string " html ".
Optionally, a DOCTYPE legacy string .
Zero or more ASCII whitespace .
A U+003E GREATER-THAN SIGN character (>).
In other words, <!DOCTYPE html> , case-insensitively.
For the purposes of HTML generators that cannot output HTML markup with the short DOCTYPE
  " <!DOCTYPE html> ", a DOCTYPE legacy string may be inserted
  into the DOCTYPE (in the position defined above).
This string must consist of:  One or more ASCII whitespace .
A string that is an ASCII case-insensitive match for the string " SYSTEM ".
A U+0022 QUOTATION MARK or U+0027 APOSTROPHE character (the quote mark ).
The literal string " about:legacy-compat ".
A matching U+0022 QUOTATION MARK or U+0027 APOSTROPHE character (i.e.
the same character as in the earlier step labeled quote mark ).
In other words, <!DOCTYPE html SYSTEM "about:legacy-compat"> or <!DOCTYPE html SYSTEM 'about:legacy-compat'> , case-insensitively except for the
  part in single or double quotes.
The DOCTYPE legacy string should not be used unless the document is generated from
  a system that cannot output the shorter string.
13.1.2 Elements  There are six different kinds of elements : void
  elements , the template element , raw text
  elements , escapable raw text elements , foreign elements , and normal elements .
Void elements area , base , br , col , embed , hr , img , input , link , meta , source , track , wbr The template element template Raw text elements script , style Escapable raw text elements textarea , title Foreign elements Elements from the MathML namespace and the SVG namespace .
Normal elements All other allowed HTML elements are normal elements.
Tags are used to delimit the start and end of elements in the
  markup.
Raw text , escapable raw text , and normal elements have
  a start tag to indicate where they begin, and an end tag to indicate where they end.
The start and end tags of
  certain normal elements can be omitted , as
  described below in the section on optional tags .
Those
  that cannot be omitted must not be omitted.
Void elements only have a start tag; end
  tags must not be specified for void elements .
Foreign elements must
  either have a start tag and an end tag, or a start tag that is marked as self-closing, in which
  case they must not have an end tag.
The contents of the element must be placed between
  just after the start tag (which might be implied, in certain
  cases ) and just before the end tag (which again, might be
  implied in certain cases ).
The exact allowed contents of each individual element depend on
  the content model of that element, as described earlier in
  this specification.
Elements must not contain content that their content model disallows.
In
  addition to the restrictions placed on the contents by those content models, however, the five
  types of elements have additional syntactic requirements.
Void elements can't have any contents (since there's no end tag, no content can be
  put between the start tag and the end tag).
The template element can have template contents , but such template contents are not children of the template element itself.
Instead, they are stored in a DocumentFragment associated with a different Document — without a browsing context — so
  as to avoid the template contents interfering with the main Document .
The markup for the template contents of a template element is placed
  just after the template element's start tag and just before template element's end tag (as with other elements), and may consist of any text , character references , elements , and comments , but
  the text must not contain the character U+003C LESS-THAN SIGN (<) or an ambiguous ampersand .
Raw text elements can have text , though it has restrictions described below.
Escapable raw text elements can have text and character references , but the text must not contain an ambiguous ampersand .
There are also further restrictions described below.
Foreign elements whose start tag is marked as self-closing can't have any contents
  (since, again, as there's no end tag, no content can be put between the start tag and the end
  tag).
Foreign elements whose start tag is not marked as self-closing can
  have text , character
  references , CDATA sections , other elements , and comments , but
  the text must not contain the character U+003C LESS-THAN SIGN (<) or an ambiguous ampersand .
The HTML syntax does not support namespace declarations, even in foreign
   elements .
For instance, consider the following HTML fragment:  < p >  < svg >  < metadata >  <!-- this is invalid -->  < cdr:license  xmlns:cdr = "https://www.example.com/cdr/metadata"  name = "MIT" />  </ metadata >  </ svg >  </ p >  The innermost element, cdr:license , is actually in the SVG namespace, as
   the " xmlns:cdr " attribute has no effect (unlike in XML).
In fact, as the
   comment in the fragment above says, the fragment is actually non-conforming.
This is because SVG 2 does not define any elements called " cdr:license " in
   the SVG namespace.
Normal elements can have text , character references , other elements , and comments , but
  the text must not contain the character U+003C LESS-THAN SIGN (<) or an ambiguous ampersand .
Some normal elements also have yet more restrictions on what content they are
  allowed to hold, beyond the restrictions imposed by the content model and those described in this
  paragraph.
Those restrictions are described below.
Tags contain a tag name , giving the element's name.
HTML
  elements all have names that only use ASCII
  alphanumerics .
In the HTML syntax, tag names, even those for foreign elements ,
  may be written with any mix of lower- and uppercase letters that, when converted to all-lowercase,
  matches the element's tag name; tag names are case-insensitive.
13.1.2.1 Start tags  Start tags must have the following format:  The first character of a start tag must be a U+003C LESS-THAN SIGN character (<).
The next few characters of a start tag must be the element's tag name .
If there are to be any attributes in the next step, there must first be one or more ASCII whitespace .
Then, the start tag may have a number of attributes, the syntax for which is described below.
Attributes must be
   separated from each other by one or more ASCII whitespace .
After the attributes, or after the tag name if there
   are no attributes, there may be one or more ASCII whitespace .
(Some attributes are
   required to be followed by a space.
See the attributes
   section below.)
Then, if the element is one of the void elements , or if the element is a foreign element , then there may be a single U+002F SOLIDUS
   character (/), which on foreign elements marks the start tag as self-closing.
On void elements , it does not mark the start tag as self-closing but instead is
   unnecessary and has no effect of any kind.
For such void elements, it should be used only with
   caution — especially since, if directly preceded by an unquoted attribute
   value , it becomes part of the attribute value rather than being discarded by the
   parser.
Finally, start tags must be closed by a U+003E GREATER-THAN SIGN character (>).
13.1.2.2 End tags  End tags must have the following format:  The first character of an end tag must be a U+003C LESS-THAN SIGN character (<).
The second character of an end tag must be a U+002F SOLIDUS character (/).
The next few characters of an end tag must be the element's tag
   name .
After the tag name, there may be one or more ASCII whitespace .
Finally, end tags must be closed by a U+003E GREATER-THAN SIGN character (>).
13.1.2.3 Attributes  Attributes for an element are expressed inside the
  element's start tag.
Attributes have a name and a value.
Attribute names must consist of one or more characters other than controls ,
  U+0020 SPACE, U+0022 ("), U+0027 ('), U+003E (>), U+002F (/), U+003D (=), and noncharacters .
In the HTML syntax, attribute names, even those for foreign elements , may be written with any mix of ASCII lower and ASCII upper alphas .
Attribute values are a mixture of text and character references ,
  except with the additional restriction that the text cannot contain an ambiguous ampersand .
Attributes can be specified in four different ways:  Empty attribute syntax  Just the attribute name .
The value is implicitly
    the empty string.
In the following example, the disabled attribute is
     given with the empty attribute syntax:  < input  disabled >   If an attribute using the empty attribute syntax is to be followed by another attribute, then
    there must be ASCII whitespace separating the two.
Unquoted attribute value syntax  The attribute name , followed by zero or more ASCII whitespace , followed by a single U+003D EQUALS SIGN character, followed by
    zero or more ASCII whitespace , followed by the attribute value , which, in addition to the requirements
    given above for attribute values, must not contain any literal ASCII whitespace ,
    any U+0022 QUOTATION MARK characters ("), U+0027 APOSTROPHE characters ('), U+003D
    EQUALS SIGN characters (=), U+003C LESS-THAN SIGN characters (<), U+003E GREATER-THAN SIGN
    characters (>), or U+0060 GRAVE ACCENT characters (`), and must not be the empty string.
In the following example, the value attribute is given
     with the unquoted attribute value syntax:  < input  value = yes >   If an attribute using the unquoted attribute syntax is to be followed by another attribute or
    by the optional U+002F SOLIDUS character (/) allowed in step 6 of the start tag syntax above, then there must be ASCII
    whitespace separating the two.
Single-quoted attribute value syntax  The attribute name , followed by zero or more ASCII whitespace , followed by a single U+003D EQUALS SIGN character, followed by
    zero or more ASCII whitespace , followed by a single U+0027 APOSTROPHE character
    ('), followed by the attribute value , which, in
    addition to the requirements given above for attribute values, must not contain any literal
    U+0027 APOSTROPHE characters ('), and finally followed by a second single U+0027 APOSTROPHE
    character (').
In the following example, the type attribute is given
     with the single-quoted attribute value syntax:  < input  type = 'checkbox' >   If an attribute using the single-quoted attribute syntax is to be followed by another
    attribute, then there must be ASCII whitespace separating the two.
Double-quoted attribute value syntax  The attribute name , followed by zero or more ASCII whitespace , followed by a single U+003D EQUALS SIGN character, followed by
    zero or more ASCII whitespace , followed by a single U+0022 QUOTATION MARK character
    ("), followed by the attribute value , which, in
    addition to the requirements given above for attribute values, must not contain any literal
    U+0022 QUOTATION MARK characters ("), and finally followed by a second single U+0022 QUOTATION
    MARK character (").
In the following example, the name attribute is given with
     the double-quoted attribute value syntax:  < input  name = "be evil" >   If an attribute using the double-quoted attribute syntax is to be followed by another
    attribute, then there must be ASCII whitespace separating the two.
There must never be two or more attributes on the same start tag whose names are an ASCII
  case-insensitive match for each other.
When a foreign element has one of the namespaced
  attributes given by the local name and namespace of the first and second cells of a row from the
  following table, it must be written using the name given by the third cell from the same row.
Local name Namespace Attribute name  actuate   XLink namespace   xlink:actuate   arcrole   XLink namespace   xlink:arcrole   href   XLink namespace   xlink:href   role   XLink namespace   xlink:role   show   XLink namespace   xlink:show   title   XLink namespace   xlink:title   type   XLink namespace   xlink:type   lang   XML namespace   xml:lang   space   XML namespace   xml:space   xmlns   XMLNS namespace   xmlns   xlink   XMLNS namespace   xmlns:xlink   No other namespaced attribute can be expressed in the HTML syntax .
Whether the attributes in the table above are conforming or not is defined by
  other specifications (e.g.
SVG 2 and MathML ); this section only
  describes the syntax rules if the attributes are serialized using the HTML syntax.
13.1.2.4 Optional tags  Certain tags can be omitted .
Omitting an element's start tag in the
  situations described below does not mean the element is not present; it is implied, but it is
  still there.
For example, an HTML document always has a root html element, even if
  the string <html> doesn't appear anywhere in the markup.
An html element's start tag may be omitted
  if the first thing inside the html element is not a comment .
For example, in the following case it's ok to remove the " <html> "
   tag:  <!DOCTYPE HTML>  < html >  < head >  < title > Hello </ title >  </ head >  < body >  < p > Welcome to this example.
</ p >  </ body >  </ html >  Doing so would make the document look like this:  <!DOCTYPE HTML>  < head >  < title > Hello </ title >  </ head >  < body >  < p > Welcome to this example.
</ p >  </ body >  </ html >  This has the exact same DOM.
In particular, note that whitespace around the document
   element is ignored by the parser.
The following example would also have the exact same
   DOM:  <!DOCTYPE HTML> < head >  < title > Hello </ title >  </ head >  < body >  < p > Welcome to this example.
</ p >  </ body >  </ html >  However, in the following example, removing the start tag moves the comment to before the html element:  <!DOCTYPE HTML>  < html >  <!-- where is this comment in the DOM?
-->  < head >  < title > Hello </ title >  </ head >  < body >  < p > Welcome to this example.
</ p >  </ body >  </ html >  With the tag removed, the document actually turns into the same as this:  <!DOCTYPE HTML>  <!-- where is this comment in the DOM?
-->  < html >  < head >  < title > Hello </ title >  </ head >  < body >  < p > Welcome to this example.
</ p >  </ body >  </ html >  This is why the tag can only be removed if it is not followed by a comment: removing the tag
   when there is a comment there changes the document's resulting parse tree.
Of course, if the
   position of the comment does not matter, then the tag can be omitted, as if the comment had been
   moved to before the start tag in the first place.
An html element's end tag may be omitted if
  the html element is not immediately followed by a comment .
A head element's start tag may be omitted if
  the element is empty, or if the first thing inside the head element is an
  element.
A head element's end tag may be omitted if
  the head element is not immediately followed by ASCII whitespace or a comment .
A body element's start tag may be omitted
  if the element is empty, or if the first thing inside the body element is not ASCII whitespace or a comment , except if the
  first thing inside the body element is a meta , noscript , link , script , style , or template element.
A body element's end tag may be omitted if the body element is not immediately followed by a comment .
Note that in the example above, the head element start and end tags, and the body element start tag, can't be omitted, because they are surrounded by
   whitespace:  <!DOCTYPE HTML>  < html >  < head >  < title > Hello </ title >  </ head >  < body >  < p > Welcome to this example.
</ p >  </ body >  </ html >  (The body and html element end tags could be omitted without
   trouble; any spaces after those get parsed into the body element anyway.)
Usually, however, whitespace isn't an issue.
If we first remove the whitespace we don't care
   about:  <!DOCTYPE HTML> < html >< head >< title > Hello </ title ></ head >< body >< p > Welcome to this example.
</ p ></ body ></ html >  Then we can omit a number of tags without affecting the DOM:  <!DOCTYPE HTML> < title > Hello </ title >< p > Welcome to this example.
</ p >  At that point, we can also add some whitespace back:  <!DOCTYPE HTML>  < title > Hello </ title >  < p > Welcome to this example.
</ p >  This would be equivalent to this document, with the omitted tags shown in their
   parser-implied positions; the only whitespace text node that results from this is the newline at
   the end of the head element:  <!DOCTYPE HTML>  < html >< head > < title > Hello </ title >  </ head >< body > < p > Welcome to this example.
</ p > </ body ></ html >   An li element's end tag may be omitted if the li element is immediately followed by another li element or if there is
  no more content in the parent element.
A dt element's end tag may be omitted if the dt element is immediately followed by another dt element or a dd element.
A dd element's end tag may be omitted if the dd element is immediately followed by another dd element or a dt element, or if there is no more content in the parent element.
A p element's end tag may be omitted if the p element is immediately followed by an address , article , aside , blockquote , details , div , dl , fieldset , figcaption , figure , footer , form , h1 , h2 , h3 , h4 , h5 , h6 , header , hgroup , hr , main , menu , nav , ol , p , pre , search , section , table ,
  or ul element, or if there is no more content in the parent element and the parent
  element is an HTML element that is not an a , audio , del , ins , map , noscript ,
  or video element, or an autonomous custom element .
We can thus simplify the earlier example further: <!DOCTYPE HTML> < title > Hello </ title >< p > Welcome to this example.
An rt element's end tag may be omitted if the rt element is immediately followed by an rt or rp element,
  or if there is no more content in the parent element.
An rp element's end tag may be omitted if the rp element is immediately followed by an rt or rp element,
  or if there is no more content in the parent element.
An optgroup element's end tag may be omitted
  if the optgroup element  is
  immediately followed by another optgroup element, if it is immediately followed by an hr element, or if  there is no more content in the parent
  element.
An option element's end tag may be omitted if
  the option element is immediately followed by another option element, if
  it is immediately followed by an optgroup element, if it is immediately followed by
  an hr element, or if there is no more content in the parent element.
A colgroup element's start tag may be
  omitted if the first thing inside the colgroup element is a col element,
  and if the element is not immediately preceded by another colgroup element whose end tag has been omitted.
(It can't be omitted if the element
  is empty.)
A colgroup element's end tag may be omitted
  if the colgroup element is not immediately followed by ASCII whitespace or a comment .
A caption element's end tag may be omitted if
  the caption element is not immediately followed by ASCII whitespace or a comment .
A thead element's end tag may be omitted if
  the thead element is immediately followed by a tbody or tfoot element.
A tbody element's start tag may be omitted
  if the first thing inside the tbody element is a tr element, and if the
  element is not immediately preceded by a tbody , thead , or tfoot element whose end tag has been omitted.
(It
  can't be omitted if the element is empty.)
A tbody element's end tag may be omitted if
  the tbody element is immediately followed by a tbody or tfoot element, or if there is no more content in the parent element.
A tfoot element's end tag may be omitted if
  there is no more content in the parent element.
A tr element's end tag may be omitted if the tr element is immediately followed by another tr element, or if there is
  no more content in the parent element.
A td element's end tag may be omitted if the td element is immediately followed by a td or th element,
  or if there is no more content in the parent element.
A th element's end tag may be omitted if the th element is immediately followed by a td or th element,
  or if there is no more content in the parent element.
The ability to omit all these table-related tags makes table markup much terser.
Take this example:  < table >  < caption > 37547 TEE Electric Powered Rail Car Train Functions (Abbreviated) </ caption >  < colgroup >< col >< col >< col ></ colgroup >  < thead >  < tr >  < th > Function </ th >  < th > Control Unit </ th >  < th > Central Station </ th >  </ tr >  </ thead >  < tbody >  < tr >  < td > Headlights </ td >  < td > ✔ </ td >  < td > ✔ </ td >  </ tr >  < tr >  < td > Interior Lights </ td >  < td > ✔ </ td >  < td > ✔ </ td >  </ tr >  < tr >  < td > Electric locomotive operating sounds </ td >  < td > ✔ </ td >  < td > ✔ </ td >  </ tr >  < tr >  < td > Engineer's cab lighting </ td >  < td ></ td >  < td > ✔ </ td >  </ tr >  < tr >  < td > Station Announcements - Swiss </ td >  < td ></ td >  < td > ✔ </ td >  </ tr >  </ tbody >  </ table >  The exact same table, modulo some whitespace differences, could be marked up as follows:  < table >  < caption > 37547 TEE Electric Powered Rail Car Train Functions (Abbreviated) < colgroup >< col >< col >< col >  < thead >  < tr >  < th > Function < th > Control Unit < th > Central Station < tbody >  < tr >  < td > Headlights < td > ✔ < td > ✔ < tr >  < td > Interior Lights < td > ✔ < td > ✔ < tr >  < td > Electric locomotive operating sounds < td > ✔ < td > ✔ < tr >  < td > Engineer's cab lighting < td >  < td > ✔ < tr >  < td > Station Announcements - Swiss < td >  < td > ✔ </ table >  Since the cells take up much less room this way, this can be made even terser by having each
   row on one line:  < table >  < caption > 37547 TEE Electric Powered Rail Car Train Functions (Abbreviated) < colgroup >< col >< col >< col >  < thead >  < tr >  < th > Function < th > Control Unit < th > Central Station < tbody >  < tr >  < td > Headlights < td > ✔ < td > ✔ < tr >  < td > Interior Lights < td > ✔ < td > ✔ < tr >  < td > Electric locomotive operating sounds < td > ✔ < td > ✔ < tr >  < td > Engineer's cab lighting < td >  < td > ✔ < tr >  < td > Station Announcements - Swiss < td >  < td > ✔ </ table >  The only differences between these tables, at the DOM level, is with the precise position of
   the (in any case semantically-neutral) whitespace.
However , a start tag must never be
  omitted if it has any attributes.
Returning to the earlier example with all the whitespace removed and then all the optional
   tags removed:  <!DOCTYPE HTML> < title > Hello </ title >< p > Welcome to this example.
If the body element in this example had to have a class attribute and the html element had to have a lang attribute, the markup would have to become:  <!DOCTYPE HTML> < html  lang = "en" >< title > Hello </ title >< body  class = "demo" >< p > Welcome to this example.
This section assumes that the document is conforming, in particular, that there
  are no content model violations.
Omitting tags in the fashion
  described in this section in a document that does not conform to the content models described in this specification is likely to result in unexpected DOM differences (this is, in
  part, what the content models are designed to avoid).
13.1.2.5 Restrictions on content models  For historical reasons, certain elements have extra restrictions beyond even the restrictions
  given by their content model.
A table element must not contain tr elements, even though these
  elements are technically allowed inside table elements according to the content
  models described in this specification.
(If a tr element is put inside a table in the markup, it will in fact imply a tbody start tag before
  it.)
A single newline may be placed immediately after the start tag of pre and textarea elements.
This does not affect the processing of the element.
The otherwise optional newline  must be included if the element's contents
  themselves start with a newline (because otherwise the
  leading newline in the contents would be treated like the optional newline, and ignored).
The following two pre blocks are equivalent:  < pre > Hello </ pre >  < pre > Hello </ pre >   13.1.2.6 Restrictions on the contents of raw text and escapable raw text elements  The text in raw text and escapable raw text
  elements must not contain any occurrences of the string " </ "
  (U+003C LESS-THAN SIGN, U+002F SOLIDUS) followed by characters that case-insensitively match the
  tag name of the element followed by one of U+0009 CHARACTER TABULATION (tab), U+000A LINE FEED
  (LF), U+000C FORM FEED (FF), U+000D CARRIAGE RETURN (CR), U+0020 SPACE, U+003E GREATER-THAN SIGN
  (>), or U+002F SOLIDUS (/).
13.1.3 Text  Text is allowed inside elements, attribute values, and comments.
Extra constraints are placed on what is and what is not allowed in text based on where the text is
  to be put, as described in the other sections.
13.1.3.1 Newlines  Newlines in HTML may be represented either as U+000D
  CARRIAGE RETURN (CR) characters, U+000A LINE FEED (LF) characters, or pairs of U+000D CARRIAGE
  RETURN (CR), U+000A LINE FEED (LF) characters in that order.
Where character references are allowed, a character
  reference of a U+000A LINE FEED (LF) character (but not a U+000D CARRIAGE RETURN (CR) character)
  also represents a newline .
13.1.4 Character references  In certain cases described in other sections, text may be
  mixed with character references .
These can be used to escape
  characters that couldn't otherwise legally be included in text .
Character references must start with a U+0026 AMPERSAND character (&).
Following this,
  there are three possible kinds of character references:  Named character references The ampersand must be followed by one of the names given in the named character
   references section, using the same case.
The name must be one that is
   terminated by a U+003B SEMICOLON character (;).
Decimal numeric character reference The ampersand must be followed by a U+0023 NUMBER SIGN character (#), followed by one or more ASCII digits , representing a base-ten integer that corresponds to a code point that
   is allowed according to the definition below.
The digits must then be followed by a U+003B
   SEMICOLON character (;).
Hexadecimal numeric character reference The ampersand must be followed by a U+0023 NUMBER SIGN character (#), which must be followed
   by either a U+0078 LATIN SMALL LETTER X character (x) or a U+0058 LATIN CAPITAL LETTER X
   character (X), which must then be followed by one or more ASCII hex digits ,
   representing a hexadecimal integer that corresponds to a code point that is allowed according to
   the definition below.
The digits must then be followed by a U+003B SEMICOLON character (;).
The numeric character reference forms described above are allowed to reference any code point
  excluding U+000D CR, noncharacters , and controls other than ASCII whitespace .
An ambiguous ampersand is a U+0026 AMPERSAND
  character (&) that is followed by one or more ASCII
  alphanumerics , followed by a U+003B SEMICOLON character (;), where these characters do not
  match any of the names given in the named character references section.
13.1.5 CDATA sections  CDATA sections must consist of the following components, in
  this order:  The string " <!
[CDATA[ ".
Optionally, text , with the additional restriction that the
   text must not contain the string " ]]> ".
The string " ]]> ".
CDATA sections can only be used in foreign content (MathML or SVG).
In this example, a CDATA
   section is used to escape the contents of a MathML ms element:  < p > You can add a string to a number, but this stringifies the number: </ p >  < math >  < ms > <!
[CDATA[x<y]]> </ ms >  < mo > + </ mo >  < mn > 3 </ mn >  < mo > = </ mo >  < ms > <!
[CDATA[x<y3]]> </ ms >  </ math >   13.1.6 Comments  Comments must have the following format:  The string " <!-- ".
Optionally, text , with the additional restriction that the
   text must not start with the string " > ", nor start with the string
   " -> ", nor contain the strings " <!-- ", " --> ", or " --!> ", nor end with the string " <!- ".
The string " --> ".
The text is allowed to end with the string
  " <!
", as in <!--My favorite operators are > and
  <!--> .
← 12 Web storage — Table of Contents — 13.2 Parsing HTML documents →   Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
Search:           PrintFails     PrintFails         FrontPage RecentChanges FindPage HelpContents PrintFails     Page  Immutable Page Comments Info Attachments    More Actions:   Raw Text  Print View  Delete Cache  ------------------------  Check Spelling  Like Pages  Local Site Map  ------------------------  Rename Page  Delete Page  ------------------------  ------------------------  Remove Spam  Revert to this revision  ------------------------  SlideShow          User  Login      Contents  Issue  Windows  Various UNIX consoles  print, write and Unicode in pre-3.0 Python  read and Unicode in pre-3.0 Python   Issue  If you try to print a unicode string to console and get a message like this one: >>> print u"\u03A9" Traceback (most recent call last): File "<stdin>", line 1, in ?
File "C:\Python24\lib\encodings\cp866.py", line 18, in encode return codecs.charmap_encode(input,errors,encoding_map) UnicodeEncodeError: 'charmap' codec can't encode character u'\u1234' in position 0: character maps to <undefined> This means that the python console app can't write the given character to the console's encoding.
More specifically, the python console app created a _io.TextIOWrapperd instance with an encoding that cannot represent the given character.
sys.stdout --> _io.TextIOWrapperd --> (your console) To understand it more clearly, look at: sys.stdout sys.stdout.encoding -- This seems to work on one of my computers (Vista,) but not on another of my computers (XP.)
I haven't looked into differences of situation in detail.
Windows  By default, the console in Microsoft Windows only displays 256 characters (cp437, of "Code page 437" , the original IBM-PC 1981 extended ASCII character set.)
If you try to print an unprintable character you will get UnicodeEncodeError .
Setting the PYTHONIOENCODING environment variable as described above can be used to suppress the error messages.
Setting to "utf-8" is not recommended as this produces an inaccurate, garbled representation of the output to the console.
For best results, use your console's correct default codepage and a suitable error handler other than "strict".
Various UNIX consoles  There is no standard way to query UNIX console for find out what characters it supports but fortunately there is a way to find out what characters are considered to be printable.
Locale category LC_CTYPE defines what characters are printable.
To find out its value type at python prompt:    1  >>> import  locale  2  >>> locale .
getdefaultlocale ()[ 1 ]  3  ' utf-8 '  If you got any other value you won't be able to print all unicode characters.
As soon as you try to print a unprintable character you will get UnicodeEncodeError .
To fix this situation you need to set the environment variable LANG to one of supported by your system unicode locales.
To get the full list of locales use command "locale -a", look for locales that end with string ".utf-8".
If you have set LANG variable but now instead of UnicodeEncodeError you see garbage on your screen you need to set up your terminal to use font unicode font.
Consult terminal manual on how to do it.
print, write and Unicode in pre-3.0 Python  Because file operations are 8-bit clean, reading data from the original stdin will return str 's containing data in the input character set.
Writing these str 's to stdout without any codecs will result in the output identical to the input.
$ echo $LANG en_CA.utf8  $ python -c 'import sys; line = sys.stdin.readline(); print type(line), len(line); print line;' [TYPING: абв ENTER] <type 'str'> 7 абв  $ echo "абв" | python -c 'import sys; line = sys.stdin.readline(); print type(line), len(line); print line;' <type 'str'> 7 абв $ echo "абв" | python -c 'import sys; line = sys.stdin.readline(); print type(line), len(line); print line;' | cat <type 'str'> 7 абв Since programmers need to display unicode strings, the designers of the print statement built the required transformation into it.
When Python finds its output attached to a terminal, it sets the sys.stdout.encoding attribute to the terminal's encoding.
The print statement's handler will automatically encode unicode arguments into str output.
$ python -c 'import sys; print sys.stdout.encoding; print u"\u0411\n"' UTF-8 Б When Python does not detect the desired character set of the output, it sets sys.stdout.encoding to None, and print will invoke the "ascii" codec.
$ python -c 'import sys; print sys.stdout.encoding; print u"\u0411\n"' 2>&1 | cat None Traceback (most recent call last): File "<string>", line 1, in <module> UnicodeEncodeError: 'ascii' codec can't encode character u'\u0411' in position 0: ordinal not in range(128) I (IL) understand the implementation of Python2.5's print statement as follows.
1   # At Python startup.
2   sys .
stdout .
encoding = tty_enc  3   if  tty_enc  is  not  None :  4   class_tty_enc_sw = codecs .
getstreamwriter ( tty_enc )  5   else  6   class_tty_enc_sw = None  7   8   def  print (* args ):  9   if  class_tty_enc_sw  is  not  None :  10   eout = class_tty_enc_sw ( sys .
stdout )  11   else :  12   eout = None  13   for  arg  in  args :  14   sarg = stringify_to_str_or_unicode ( arg )  15   if  type ( sarg ) == str  or  eout  is  None :  16   # Avoid coercion to unicode in eout.write().
17   sys .
write ( sarg )  18   else :  19   eout .
write ( sarg )  At startup, Python will detect the encoding of the standard output and, probably, store the respective StreamWriter class definition.
The print statement stringifies all its arguments to narrow str and wide unicode strings based on the width of the original arguments.
Then print passes narrow strings to sys.stdout directly and wide strings to the instance of StreamWriter wrapped around sys.stdout .
If the user does not replace sys.stdout as shown below and Python does not detect an output encoding, the write method will coerce unicode values to str by invoking the ASCII codec ( DefaultEncoding ).
Python file's write and read methods do not invoke codecs internally.
Python2.5's file open built-in sets the .encoding attribute of the resulting instance to None .
Wrapping sys.stdout into an instance of StreamWriter will allow writing unicode data with sys.stdout.write() and print .
$ python -c 'import sys, codecs, locale; print sys.stdout.encoding; \ sys.stdout = codecs.getwriter(locale.getpreferredencoding())(sys.stdout); \ line = u"\u0411\n"; print type(line), len(line); \ sys.stdout.write(line); print line' UTF-8 <type 'unicode'> 2 Б Б  $ python -c 'import sys, codecs, locale; print sys.stdout.encoding; \ sys.stdout = codecs.getwriter(locale.getpreferredencoding())(sys.stdout); \ line = u"\u0411\n"; print type(line), len(line); \ sys.stdout.write(line); print line' | cat None <type 'unicode'> 2 Б Б The write call executes StreamWriter.write which in turn invokes codec-specific encode and passes the result to the underlying file.
It appears that the print statement will not fail due to the argument type coercion when sys.stdout is wrapped.
My (IL's) understanding of print 's implementation above agrees with that.
read and Unicode in pre-3.0 Python  I (IL) believe reading from stdin does not involve coercion at all because the existing ways to read from stdin such as "for line in sys.stdin" do not convey the expected type of the returned value to the stdin handler.
A function that would complement the print statement might look like this: line = typed_read(unicode)   # Generally, a list of input data types along with an optional parsing format line.
print statement encodes unicode strings to str strings.
One can complement this with decoding of str input data into unicode strings in sys.stdin.read/readline .
For this, we will wrap sys.stdin into a StreamReader instance: $ python -c 'import sys, codecs, locale; \ print sys.stdin.encoding; \ sys.stdin = codecs.getreader(locale.getpreferredencoding())(sys.stdin); \ line = sys.stdin.readline(); print type(line), len(line)' 2>&1 UTF-8 [TYPING: абв ENTER] <type 'unicode'> 4 $ echo "абв" | python -c 'import sys, codecs, locale; \ print sys.stdin.encoding; \ sys.stdin = codecs.getreader(locale.getpreferredencoding())(sys.stdin); \ line = sys.stdin.readline(); print type(line), len(line)' None <type 'unicode'> 4 See also: Unicode   CategoryUnicode  PrintFails  (last edited 2012-11-25 11:32:18 by techtonik )      MoinMoin Powered Python Powered GPL licensed Valid HTML 4.01    Unable to edit the page?
See the FrontPage for instructions.
Created using Sphinx 7.2.6.       cchardet 2.1.7    pip install cchardet    Copy PIP instructions        Latest version   Released: Oct 27, 2020          cChardet is high speed universal character encoding detector.
- binding to uchardet .
Supported Languages/Encodings   International (Unicode)   UTF-8  UTF-16BE / UTF-16LE  UTF-32BE / UTF-32LE / X-ISO-10646-UCS-4-34121 /
X-ISO-10646-UCS-4-21431    Arabic   ISO-8859-6  WINDOWS-1256    Bulgarian   ISO-8859-5  WINDOWS-1251    Chinese   ISO-2022-CN  BIG5  EUC-TW  GB18030  HZ-GB-2312    Croatian:   ISO-8859-2  ISO-8859-13  ISO-8859-16  Windows-1250  IBM852  MAC-CENTRALEUROPE    Czech   Windows-1250  ISO-8859-2  IBM852  MAC-CENTRALEUROPE    Danish   ISO-8859-1  ISO-8859-15  WINDOWS-1252    English   ASCII    Esperanto   ISO-8859-3    Estonian   ISO-8859-4  ISO-8859-13  ISO-8859-13  Windows-1252  Windows-1257    Finnish   ISO-8859-1  ISO-8859-4  ISO-8859-9  ISO-8859-13  ISO-8859-15  WINDOWS-1252    French   ISO-8859-1  ISO-8859-15  WINDOWS-1252    German   ISO-8859-1  WINDOWS-1252    Greek   ISO-8859-7  WINDOWS-1253    Hebrew   ISO-8859-8  WINDOWS-1255    Hungarian:   ISO-8859-2  WINDOWS-1250    Irish Gaelic   ISO-8859-1  ISO-8859-9  ISO-8859-15  WINDOWS-1252    Italian   ISO-8859-1  ISO-8859-3  ISO-8859-9  ISO-8859-15  WINDOWS-1252    Japanese   ISO-2022-JP  SHIFT_JIS  EUC-JP    Korean   ISO-2022-KR  EUC-KR / UHC    Lithuanian   ISO-8859-4  ISO-8859-10  ISO-8859-13    Latvian   ISO-8859-4  ISO-8859-10  ISO-8859-13    Maltese   ISO-8859-3    Polish:   ISO-8859-2  ISO-8859-13  ISO-8859-16  Windows-1250  IBM852  MAC-CENTRALEUROPE    Portuguese   ISO-8859-1  ISO-8859-9  ISO-8859-15  WINDOWS-1252    Romanian:   ISO-8859-2  ISO-8859-16  Windows-1250  IBM852    Russian   ISO-8859-5  KOI8-R  WINDOWS-1251  MAC-CYRILLIC  IBM866  IBM855    Slovak   Windows-1250  ISO-8859-2  IBM852  MAC-CENTRALEUROPE    Slovene   ISO-8859-2  ISO-8859-16  Windows-1250  IBM852  M       Example  # -*- coding: utf-8 -*-  import  cchardet  as  chardet  with  open ( r "src/tests/samples/wikipediaJa_One_Thousand_and_One_Nights_SJIS.txt" ,  "rb" )  as  f :   msg  =  f .
read ()   result  =  chardet .
If you're not sure which to choose, learn more about installing packages .
Source Distribution       cchardet-2.1.7.tar.gz (653.6 kB view hashes ) Uploaded Oct 27, 2020  Source     Built Distributions       cchardet-2.1.7-cp39-cp39-win_amd64.whl (115.1 kB view hashes ) Uploaded Oct 28, 2020  CPython 3.9  Windows x86-64          cchardet-2.1.7-cp39-cp39-win32.whl (111.8 kB view hashes ) Uploaded Oct 28, 2020  CPython 3.9  Windows x86          cchardet-2.1.7-cp39-cp39-manylinux2010_x86_64.whl (265.4 kB view hashes ) Uploaded Oct 28, 2020  CPython 3.9  manylinux: glibc 2.12+ x86-64          cchardet-2.1.7-cp39-cp39-manylinux2010_i686.whl (256.3 kB view hashes ) Uploaded Oct 28, 2020  CPython 3.9  manylinux: glibc 2.12+ i686          cchardet-2.1.7-cp39-cp39-manylinux1_x86_64.whl (265.4 kB view hashes ) Uploaded Oct 28, 2020  CPython 3.9          cchardet-2.1.7-cp39-cp39-manylinux1_i686.whl (256.3 kB view hashes ) Uploaded Oct 28, 2020  CPython 3.9          cchardet-2.1.7-cp39-cp39-macosx_10_9_x86_64.whl (124.3 kB view hashes ) Uploaded Oct 28, 2020  CPython 3.9  macOS 10.9+ x86-64          cchardet-2.1.7-cp38-cp38-win_amd64.whl (115.2 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.8  Windows x86-64          cchardet-2.1.7-cp38-cp38-win32.whl (111.8 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.8  Windows x86          cchardet-2.1.7-cp38-cp38-manylinux2010_x86_64.whl (266.0 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.8  manylinux: glibc 2.12+ x86-64          cchardet-2.1.7-cp38-cp38-manylinux2010_i686.whl (257.1 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.8  manylinux: glibc 2.12+ i686          cchardet-2.1.7-cp38-cp38-manylinux1_x86_64.whl (266.0 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.8          cchardet-2.1.7-cp38-cp38-manylinux1_i686.whl (257.1 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.8          cchardet-2.1.7-cp38-cp38-macosx_10_9_x86_64.whl (124.1 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.8  macOS 10.9+ x86-64          cchardet-2.1.7-cp37-cp37m-win_amd64.whl (114.9 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.7m  Windows x86-64          cchardet-2.1.7-cp37-cp37m-win32.whl (111.5 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.7m  Windows x86          cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263.7 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.7m  manylinux: glibc 2.12+ x86-64          cchardet-2.1.7-cp37-cp37m-manylinux2010_i686.whl (255.1 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.7m  manylinux: glibc 2.12+ i686          cchardet-2.1.7-cp37-cp37m-manylinux1_x86_64.whl (263.7 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.7m          cchardet-2.1.7-cp37-cp37m-manylinux1_i686.whl (255.1 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.7m          cchardet-2.1.7-cp37-cp37m-macosx_10_9_x86_64.whl (124.0 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.7m  macOS 10.9+ x86-64          cchardet-2.1.7-cp36-cp36m-win_amd64.whl (115.1 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.6m  Windows x86-64          cchardet-2.1.7-cp36-cp36m-win32.whl (111.6 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.6m  Windows x86          cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl (263.3 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.6m  manylinux: glibc 2.12+ x86-64          cchardet-2.1.7-cp36-cp36m-manylinux2010_i686.whl (254.4 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.6m  manylinux: glibc 2.12+ i686          cchardet-2.1.7-cp36-cp36m-manylinux1_x86_64.whl (263.3 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.6m          cchardet-2.1.7-cp36-cp36m-manylinux1_i686.whl (254.4 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.6m          cchardet-2.1.7-cp36-cp36m-macosx_10_9_x86_64.whl (124.1 kB view hashes ) Uploaded Oct 27, 2020  CPython 3.6m  macOS 10.9+ x86-64          Close     Hashes for cchardet-2.1.7.tar.gz  Hashes for cchardet-2.1.7.tar.gz   Algorithm  Hash digest      SHA256  c428b6336545053c2589f6caf24ea32276c6664cb86db817e03a94c60afa0eaf   Copy     MD5  8a76472ad09c68c12069203ea9348ee3   Copy     BLAKE2b-256  a85d090c9f0312b7988a9433246c9cf0b566b1ae1374368cfb8ac897218a4f65   Copy       Close        Close     Hashes for cchardet-2.1.7-cp39-cp39-win_amd64.whl  Hashes for cchardet-2.1.7-cp39-cp39-win_amd64.whl   Algorithm  Hash digest      SHA256  24974b3e40fee9e7557bb352be625c39ec6f50bc2053f44a3d1191db70b51675   Copy     MD5  fe14c82276843d48cc6189b9bf499733   Copy     BLAKE2b-256  3bbf14cea23ee6f5ccfc2238235c7a47f88145490e9c58708dc0ca505ad512c6   Copy       Close        Close     Hashes for cchardet-2.1.7-cp39-cp39-win32.whl  Hashes for cchardet-2.1.7-cp39-cp39-win32.whl   Algorithm  Hash digest      SHA256  2309ff8fc652b0fc3c0cff5dbb172530c7abb92fe9ba2417c9c0bcf688463c1c   Copy     MD5  7b12756b7c13370044547e1d69e09d84   Copy     BLAKE2b-256  9d38dcc25b61c506274e1bb5086834da813638ea047d0fdbbf3eafc9e0ea41f2   Copy       Close        Close     Hashes for cchardet-2.1.7-cp39-cp39-manylinux2010_x86_64.whl  Hashes for cchardet-2.1.7-cp39-cp39-manylinux2010_x86_64.whl   Algorithm  Hash digest      SHA256  c96aee9ebd1147400e608a3eff97c44f49811f8904e5a43069d55603ac4d8c97   Copy     MD5  5b121ded28f97aff7a4dc71879cc9f7a   Copy     BLAKE2b-256  bed33f9c005bead891d320ea3e796e5ed76776d2ac0671530188984bb632559b   Copy       Close        Close     Hashes for cchardet-2.1.7-cp39-cp39-manylinux2010_i686.whl  Hashes for cchardet-2.1.7-cp39-cp39-manylinux2010_i686.whl   Algorithm  Hash digest      SHA256  80e6faae75ecb9be04a7b258dc4750d459529debb6b8dee024745b7b5a949a34   Copy     MD5  97287df5b8f68856385b082130631ecf   Copy     BLAKE2b-256  f1456ea939fb941b4699561d24462ac5cff7f6ed1f10162299de83a6d9e11287   Copy       Close        Close     Hashes for cchardet-2.1.7-cp39-cp39-manylinux1_x86_64.whl  Hashes for cchardet-2.1.7-cp39-cp39-manylinux1_x86_64.whl   Algorithm  Hash digest      SHA256  fdac1e4366d0579fff056d1280b8dc6348be964fda8ebb627c0269e097ab37fa   Copy     MD5  c1ab0148e3b07dd934514d1cedd755fa   Copy     BLAKE2b-256  19093ab7094e7c4bc9fd9830c8d1c0c15013b7cd9ba13c04a75e8fea08036f8a   Copy       Close        Close     Hashes for cchardet-2.1.7-cp39-cp39-manylinux1_i686.whl  Hashes for cchardet-2.1.7-cp39-cp39-manylinux1_i686.whl   Algorithm  Hash digest      SHA256  bd7f262f41fd9caf5a5f09207a55861a67af6ad5c66612043ed0f81c58cdf376   Copy     MD5  dc01007b0b1faf171571739ff7485f47   Copy     BLAKE2b-256  5ffdd31308d96a2e3a5aae521a9b0aa12d9e5a282e5b3a15ee083d43137a0e0a   Copy       Close        Close     Hashes for cchardet-2.1.7-cp39-cp39-macosx_10_9_x86_64.whl  Hashes for cchardet-2.1.7-cp39-cp39-macosx_10_9_x86_64.whl   Algorithm  Hash digest      SHA256  48ba829badef61441e08805cfa474ccd2774be2ff44b34898f5854168c596d4d   Copy     MD5  5ef242897305f52e2c57da38f5d64676   Copy     BLAKE2b-256  04937fad4f4711b0d4eee4e917bbd3cd269fad682d42ccb1e43cc3512aa8af4b   Copy       Close        Close     Hashes for cchardet-2.1.7-cp38-cp38-win_amd64.whl  Hashes for cchardet-2.1.7-cp38-cp38-win_amd64.whl   Algorithm  Hash digest      SHA256  273699c4e5cd75377776501b72a7b291a988c6eec259c29505094553ee505597   Copy     MD5  ab36d23c05e4956f181f62e5a6b1a9a1   Copy     BLAKE2b-256  21eb23024490b86c040248fa9eb92156d115288b8f8d194c0590d5550b96782f   Copy       Close        Close     Hashes for cchardet-2.1.7-cp38-cp38-win32.whl  Hashes for cchardet-2.1.7-cp38-cp38-win32.whl   Algorithm  Hash digest      SHA256  0b859069bbb9d27c78a2c9eb997e6f4b738db2d7039a03f8792b4058d61d1109   Copy     MD5  49369d60edbcdea606bc7fbedf0e3298   Copy     BLAKE2b-256  8891b17e4d000037d10f26a0b04a904ebd727f16993857e01f37bc49fef179ab   Copy       Close        Close     Hashes for cchardet-2.1.7-cp38-cp38-manylinux2010_x86_64.whl  Hashes for cchardet-2.1.7-cp38-cp38-manylinux2010_x86_64.whl   Algorithm  Hash digest      SHA256  f16517f3697569822c6d09671217fdeab61dfebc7acb5068634d6b0728b86c0b   Copy     MD5  2ebfc2f339a7953e7f4c4559c65376db   Copy     BLAKE2b-256  bb5fa822d40fec63f9e3caa52cbb61db7502dd904c878344035b52f1d3dc714a   Copy       Close        Close     Hashes for cchardet-2.1.7-cp38-cp38-manylinux2010_i686.whl  Hashes for cchardet-2.1.7-cp38-cp38-manylinux2010_i686.whl   Algorithm  Hash digest      SHA256  45456c59ec349b29628a3c6bfb86d818ec3a6fbb7eb72de4ff3bd4713681c0e3   Copy     MD5  7fc54d63fbfcf40c939b40e6e26f85d0   Copy     BLAKE2b-256  a1bcbbb07486ef8da914f15f7bb1f3e1eaadd3b88a70e4decd64e184364173c5   Copy       Close        Close     Hashes for cchardet-2.1.7-cp38-cp38-manylinux1_x86_64.whl  Hashes for cchardet-2.1.7-cp38-cp38-manylinux1_x86_64.whl   Algorithm  Hash digest      SHA256  90086e5645f8a1801350f4cc6cb5d5bf12d3fa943811bb08667744ec1ecc9ccd   Copy     MD5  e8c3a56427c4e2e42fbef8bd95b7d08e   Copy     BLAKE2b-256  c8e311ead63869139948f61b922a1539f4439554358e6b4d304ccf2f1c836004   Copy       Close        Close     Hashes for cchardet-2.1.7-cp38-cp38-manylinux1_i686.whl  Hashes for cchardet-2.1.7-cp38-cp38-manylinux1_i686.whl   Algorithm  Hash digest      SHA256  27a9ba87c9f99e0618e1d3081189b1217a7d110e5c5597b0b7b7c3fedd1c340a   Copy     MD5  5492936b5051a84ddab13c86d7955f52   Copy     BLAKE2b-256  789da9bd2cfc2d362a40c64c279c31dc5b92c65d5129c9265b589b90df5090d0   Copy       Close        Close     Hashes for cchardet-2.1.7-cp38-cp38-macosx_10_9_x86_64.whl  Hashes for cchardet-2.1.7-cp38-cp38-macosx_10_9_x86_64.whl   Algorithm  Hash digest      SHA256  b59ddc615883835e03c26f81d5fc3671fab2d32035c87f50862de0da7d7db535   Copy     MD5  7d9633c9980c6f5e4aa3055beb09211d   Copy     BLAKE2b-256  308656083d1621aba5f5ff7c06b831d88bdacea8a2bbc2198d34c0b26f05ca62   Copy       Close        Close     Hashes for cchardet-2.1.7-cp37-cp37m-win_amd64.whl  Hashes for cchardet-2.1.7-cp37-cp37m-win_amd64.whl   Algorithm  Hash digest      SHA256  54d0b26fd0cd4099f08fb9c167600f3e83619abefeaa68ad823cc8ac1f7bcc0c   Copy     MD5  c08e2ecaa7bf45ec2f8e01d7cf0dbe21   Copy     BLAKE2b-256  1be6ecd8bb8440ad5c8b7cdb4d0c3fb1e7e653fc9b49c6feca4fce81bbc744a2   Copy       Close        Close     Hashes for cchardet-2.1.7-cp37-cp37m-win32.whl  Hashes for cchardet-2.1.7-cp37-cp37m-win32.whl   Algorithm  Hash digest      SHA256  50ad671e8d6c886496db62c3bd68b8d55060688c655873aa4ce25ca6105409a1   Copy     MD5  205acfc203814b06a55241f29878ba49   Copy     BLAKE2b-256  ea67b6ba47a3e34940557c2c6ad5337ecaa68781168401be1c818bcf74f8e3d7   Copy       Close        Close     Hashes for cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl  Hashes for cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl   Algorithm  Hash digest      SHA256  ec3eb5a9c475208cf52423524dcaf713c394393e18902e861f983c38eeb77f18   Copy     MD5  cffeced1256b56d33c5550367eae6645   Copy     BLAKE2b-256  8072a4fba7559978de00cf44081c548c5d294bf00ac7dcda2db405d2baa8c67a   Copy       Close        Close     Hashes for cchardet-2.1.7-cp37-cp37m-manylinux2010_i686.whl  Hashes for cchardet-2.1.7-cp37-cp37m-manylinux2010_i686.whl   Algorithm  Hash digest      SHA256  b154effa12886e9c18555dfc41a110f601f08d69a71809c8d908be4b1ab7314f   Copy     MD5  6a437f1825b41fb3863e95d436dd1317   Copy     BLAKE2b-256  b4537a295565b7599db90a21a99a7c01c5f931a2d55ecd541209514f3222d37d   Copy       Close        Close     Hashes for cchardet-2.1.7-cp37-cp37m-manylinux1_x86_64.whl  Hashes for cchardet-2.1.7-cp37-cp37m-manylinux1_x86_64.whl   Algorithm  Hash digest      SHA256  a39526c1c526843965cec589a6f6b7c2ab07e3e56dc09a7f77a2be6a6afa4636   Copy     MD5  8a3b8eafe9e70e56c2c034347f228ffa   Copy     BLAKE2b-256  d509da7c6f30cb053f77d79058994064d76b1b789c25caaa9cb20fce9f300370   Copy       Close        Close     Hashes for cchardet-2.1.7-cp37-cp37m-manylinux1_i686.whl  Hashes for cchardet-2.1.7-cp37-cp37m-manylinux1_i686.whl   Algorithm  Hash digest      SHA256  70eeae8aaf61192e9b247cf28969faef00578becd2602526ecd8ae7600d25e0e   Copy     MD5  125acb26ca4982c3a8b3b1650ff357f9   Copy     BLAKE2b-256  f1340c90d57d3fcbe7a05ee8810c003edba710ce0ac809d2cc0460b5f15de76d   Copy       Close        Close     Hashes for cchardet-2.1.7-cp37-cp37m-macosx_10_9_x86_64.whl  Hashes for cchardet-2.1.7-cp37-cp37m-macosx_10_9_x86_64.whl   Algorithm  Hash digest      SHA256  302aa443ae2526755d412c9631136bdcd1374acd08e34f527447f06f3c2ddb98   Copy     MD5  ae319abafbc7af0be6a3a154a79ab29c   Copy     BLAKE2b-256  0cf1c45f4ecb68d741596b570f8d585a7a36b9d39cb15fb5b7066751765325a9   Copy       Close        Close     Hashes for cchardet-2.1.7-cp36-cp36m-win_amd64.whl  Hashes for cchardet-2.1.7-cp36-cp36m-win_amd64.whl   Algorithm  Hash digest      SHA256  f86e0566cb61dc4397297696a4a1b30f6391b50bc52b4f073507a48466b6255a   Copy     MD5  0426e6a8e80b2e264bc9fef77d0ad18f   Copy     BLAKE2b-256  90df66ed9f7b330133bc412975760a67c2f1beaf19058fd58eba573b30f79790   Copy       Close        Close     Hashes for cchardet-2.1.7-cp36-cp36m-win32.whl  Hashes for cchardet-2.1.7-cp36-cp36m-win32.whl   Algorithm  Hash digest      SHA256  eee4f5403dc3a37a1ca9ab87db32b48dc7e190ef84601068f45397144427cc5e   Copy     MD5  b1183e668f62bc9a0b3ef1e2b0ae6410   Copy     BLAKE2b-256  0c4226ef0b6ed6c37ec08b89495b0c999ece88f72aba765603131b31712b6ed3   Copy       Close        Close     Hashes for cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl  Hashes for cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl   Algorithm  Hash digest      SHA256  54341e7e1ba9dc0add4c9d23b48d3a94e2733065c13920e85895f944596f6150   Copy     MD5  aba5dd216be844686157690f34964db3   Copy     BLAKE2b-256  a0e5a0b9edd8664ea3b0d3270c451ebbf86655ed9fc4c3e4c45b9afae9c2e382   Copy       Close        Close     Hashes for cchardet-2.1.7-cp36-cp36m-manylinux2010_i686.whl  Hashes for cchardet-2.1.7-cp36-cp36m-manylinux2010_i686.whl   Algorithm  Hash digest      SHA256  228d2533987c450f39acf7548f474dd6814c446e9d6bd228e8f1d9a2d210f10b   Copy     MD5  fa59d462740e0e699bc98ceafaa6f603   Copy     BLAKE2b-256  7671ffe383995aba6ab7b67e72bf65837ab3f57990964f346dc923c07692859e   Copy       Close        Close     Hashes for cchardet-2.1.7-cp36-cp36m-manylinux1_x86_64.whl  Hashes for cchardet-2.1.7-cp36-cp36m-manylinux1_x86_64.whl   Algorithm  Hash digest      SHA256  6b6397d8a32b976a333bdae060febd39ad5479817fabf489e5596a588ad05133   Copy     MD5  da329d6f0ef9779e1fe902b21553867a   Copy     BLAKE2b-256  b4c641a74560ab45f9cbc602dee51e5e3fad2f487805f7e0e5087999b69745d3   Copy       Close        Close     Hashes for cchardet-2.1.7-cp36-cp36m-manylinux1_i686.whl  Hashes for cchardet-2.1.7-cp36-cp36m-manylinux1_i686.whl   Algorithm  Hash digest      SHA256  5a25f9577e9bebe1a085eec2d6fdd72b7a9dd680811bba652ea6090fb2ff472f   Copy     MD5  b6c6ecb0cf15577f1b9cfe9707bff1cd   Copy     BLAKE2b-256  03f21585c895df465fe183edbb85a6c98f62b4df70f05a47ce772ba25f89a9ce   Copy       Close        Close     Hashes for cchardet-2.1.7-cp36-cp36m-macosx_10_9_x86_64.whl  Hashes for cchardet-2.1.7-cp36-cp36m-macosx_10_9_x86_64.whl   Algorithm  Hash digest      SHA256  c6f70139aaf47ffb94d89db603af849b82efdf756f187cdd3e566e30976c519f   Copy     MD5  0d79f829399f1d207bc1846d7016bfd8   Copy     BLAKE2b-256  a8a65967f5c4095e6952781863422674d6c23eb40737480d3939be264bf1ae20   Copy       Close          Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
Python Enhancement Proposals   Python »  PEP Index »  PEP 8       Toggle light / dark / auto colour theme      PEP 8 – Style Guide for Python Code   Author :  Guido van Rossum <guido at python.org>,
Barry Warsaw <barry at python.org>,
Alyssa Coghlan <ncoghlan at gmail.com>  Status :  Active  Type :  Process  Created :  05-Jul-2001  Post-History :  05-Jul-2001, 01-Aug-2013     Table of Contents  Introduction  A Foolish Consistency is the Hobgoblin of Little Minds  Code Lay-out  Indentation  Tabs or Spaces?
Maximum Line Length  Should a Line Break Before or After a Binary Operator?
Blank Lines  Source File Encoding  Imports  Module Level Dunder Names    String Quotes  Whitespace in Expressions and Statements  Pet Peeves  Other Recommendations    When to Use Trailing Commas  Comments  Block Comments  Inline Comments  Documentation Strings    Naming Conventions  Overriding Principle  Descriptive: Naming Styles  Prescriptive: Naming Conventions  Names to Avoid  ASCII Compatibility  Package and Module Names  Class Names  Type Variable Names  Exception Names  Global Variable Names  Function and Variable Names  Function and Method Arguments  Method Names and Instance Variables  Constants  Designing for Inheritance    Public and Internal Interfaces    Programming Recommendations  Function Annotations  Variable Annotations    References  Copyright     Introduction  This document gives coding conventions for the Python code comprising
the standard library in the main Python distribution.
Please see the
companion informational PEP describing style guidelines for the C code
in the C implementation of Python .
This document and PEP 257 (Docstring Conventions) were adapted from
Guido’s original Python Style Guide essay, with some additions from
Barry’s style guide [2] .
This style guide evolves over time as additional conventions are
identified and past conventions are rendered obsolete by changes in
the language itself.
Many projects have their own coding style guidelines.
In the event of any
conflicts, such project-specific guides take precedence for that project.
A Foolish Consistency is the Hobgoblin of Little Minds  One of Guido’s key insights is that code is read much more often than
it is written.
The guidelines provided here are intended to improve
the readability of code and make it consistent across the wide
spectrum of Python code.
As PEP 20 says, “Readability counts”.
A style guide is about consistency.
Consistency with this style guide
is important.
Consistency within a project is more important.
Consistency within one module or function is the most important.
However, know when to be inconsistent – sometimes style guide
recommendations just aren’t applicable.
When in doubt, use your best
judgment.
Look at other examples and decide what looks best.
And
don’t hesitate to ask!
In particular: do not break backwards compatibility just to comply with
this PEP!
Some other good reasons to ignore a particular guideline:   When applying the guideline would make the code less readable, even
for someone who is used to reading code that follows this PEP.
To be consistent with surrounding code that also breaks it (maybe
for historic reasons) – although this is also an opportunity to
clean up someone else’s mess (in true XP style).
Because the code in question predates the introduction of the
guideline and there is no other reason to be modifying that code.
When the code needs to remain compatible with older versions of
Python that don’t support the feature recommended by the style guide.
Code Lay-out   Indentation  Use 4 spaces per indentation level.
Continuation lines should align wrapped elements either vertically
using Python’s implicit line joining inside parentheses, brackets and
braces, or using a hanging indent  [1] .
When using a hanging
indent the following should be considered; there should be no
arguments on the first line and further indentation should be used to
clearly distinguish itself as a continuation line:  # Correct:  # Aligned with opening delimiter.
foo  =  long_function_name ( var_one ,  var_two ,  var_three ,  var_four )  # Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.
def  long_function_name (  var_one ,  var_two ,  var_three ,  var_four ):  print ( var_one )  # Hanging indents should add a level.
foo  =  long_function_name (  var_one ,  var_two ,  var_three ,  var_four )    # Wrong:  # Arguments on first line forbidden when not using vertical alignment.
foo  =  long_function_name ( var_one ,  var_two ,  var_three ,  var_four )  # Further indentation required as indentation is not distinguishable.
def  long_function_name (  var_one ,  var_two ,  var_three ,  var_four ):  print ( var_one )    The 4-space rule is optional for continuation lines.
Optional:  # Hanging indents *may* be indented to other than 4 spaces.
foo  =  long_function_name (  var_one ,  var_two ,  var_three ,  var_four )    When the conditional part of an if -statement is long enough to require
that it be written across multiple lines, it’s worth noting that the
combination of a two character keyword (i.e.
if ), plus a single space,
plus an opening parenthesis creates a natural 4-space indent for the
subsequent lines of the multiline conditional.
This can produce a visual
conflict with the indented suite of code nested inside the if -statement,
which would also naturally be indented to 4 spaces.
This PEP takes no
explicit position on how (or whether) to further visually distinguish such
conditional lines from the nested suite inside the if -statement.
Acceptable options in this situation include, but are not limited to:  # No extra indentation.
if  ( this_is_one_thing  and  that_is_another_thing ):  do_something ()  # Add a comment, which will provide some distinction in editors  # supporting syntax highlighting.
if  ( this_is_one_thing  and  that_is_another_thing ):  # Since both conditions are true, we can frobnicate.
do_something ()  # Add some extra indentation on the conditional continuation line.
if  ( this_is_one_thing  and  that_is_another_thing ):  do_something ()    (Also see the discussion of whether to break before or after binary
operators below.)
The closing brace/bracket/parenthesis on multiline constructs may
either line up under the first non-whitespace character of the last
line of list, as in:  my_list  =  [  1 ,  2 ,  3 ,  4 ,  5 ,  6 ,  ]  result  =  some_function_that_takes_arguments (  'a' ,  'b' ,  'c' ,  'd' ,  'e' ,  'f' ,  )    or it may be lined up under the first character of the line that
starts the multiline construct, as in:  my_list  =  [  1 ,  2 ,  3 ,  4 ,  5 ,  6 ,  ]  result  =  some_function_that_takes_arguments (  'a' ,  'b' ,  'c' ,  'd' ,  'e' ,  'f' ,  )      Tabs or Spaces?
Spaces are the preferred indentation method.
Tabs should be used solely to remain consistent with code that is
already indented with tabs.
Python disallows mixing tabs and spaces for indentation.
Maximum Line Length  Limit all lines to a maximum of 79 characters.
For flowing long blocks of text with fewer structural restrictions
(docstrings or comments), the line length should be limited to 72
characters.
Limiting the required editor window width makes it possible to have
several files open side by side, and works well when using code
review tools that present the two versions in adjacent columns.
The default wrapping in most tools disrupts the visual structure of the
code, making it more difficult to understand.
The limits are chosen to
avoid wrapping in editors with the window width set to 80, even
if the tool places a marker glyph in the final column when wrapping
lines.
Some web based tools may not offer dynamic line wrapping at all.
Some teams strongly prefer a longer line length.
For code maintained
exclusively or primarily by a team that can reach agreement on this
issue, it is okay to increase the line length limit up to 99 characters,
provided that comments and docstrings are still wrapped at 72
characters.
The Python standard library is conservative and requires limiting
lines to 79 characters (and docstrings/comments to 72).
The preferred way of wrapping long lines is by using Python’s implied
line continuation inside parentheses, brackets and braces.
Long lines
can be broken over multiple lines by wrapping expressions in
parentheses.
These should be used in preference to using a backslash
for line continuation.
Backslashes may still be appropriate at times.
For example, long,
multiple with -statements could not use implicit continuation
before Python 3.10, so backslashes were acceptable for that case:  with  open ( '/path/to/some/file/you/want/to/read' )  as  file_1 , \ open ( '/path/to/some/file/being/written' ,  'w' )  as  file_2 :  file_2 .
write ( file_1 .
read ())    (See the previous discussion on multiline if-statements for further
thoughts on the indentation of such multiline with -statements.)
Another such case is with assert statements.
Make sure to indent the continued line appropriately.
Should a Line Break Before or After a Binary Operator?
For decades the recommended style was to break after binary operators.
But this can hurt readability in two ways: the operators tend to get
scattered across different columns on the screen, and each operator is
moved away from its operand and onto the previous line.
Here, the eye
has to do extra work to tell which items are added and which are
subtracted:  # Wrong:  # operators sit far away from their operands  income  =  ( gross_wages  +  taxable_interest  +  ( dividends  -  qualified_dividends )  -  ira_deduction  -  student_loan_interest )    To solve this readability problem, mathematicians and their publishers
follow the opposite convention.
Donald Knuth explains the traditional
rule in his Computers and Typesetting series: “Although formulas
within a paragraph always break after binary operations and relations,
displayed formulas always break before binary operations” [3] .
Following the tradition from mathematics usually results in more
readable code:  # Correct:  # easy to match operators with operands  income  =  ( gross_wages  +  taxable_interest  +  ( dividends  -  qualified_dividends )  -  ira_deduction  -  student_loan_interest )    In Python code, it is permissible to break before or after a binary
operator, as long as the convention is consistent locally.
For new
code Knuth’s style is suggested.
Blank Lines  Surround top-level function and class definitions with two blank
lines.
Method definitions inside a class are surrounded by a single blank
line.
Extra blank lines may be used (sparingly) to separate groups of
related functions.
Blank lines may be omitted between a bunch of
related one-liners (e.g.
a set of dummy implementations).
Use blank lines in functions, sparingly, to indicate logical sections.
Python accepts the control-L (i.e.
^L) form feed character as
whitespace; many tools treat these characters as page separators, so
you may use them to separate pages of related sections of your file.
Note, some editors and web-based code viewers may not recognize
control-L as a form feed and will show another glyph in its place.
Source File Encoding  Code in the core Python distribution should always use UTF-8, and should not
have an encoding declaration.
In the standard library, non-UTF-8 encodings should be used only for
test purposes.
Use non-ASCII characters sparingly, preferably only to
denote places and human names.
If using non-ASCII characters as data,
avoid noisy Unicode characters like z̯̯͡a̧͎̺l̡͓̫g̹̲o̡̼̘ and byte order
marks.
All identifiers in the Python standard library MUST use ASCII-only
identifiers, and SHOULD use English words wherever feasible (in many
cases, abbreviations and technical terms are used which aren’t
English).
Open source projects with a global audience are encouraged to adopt a
similar policy.
Imports   Imports should usually be on separate lines: # Correct:  import  os  import  sys    # Wrong:  import  sys ,  os    It’s okay to say this though:  # Correct:  from  subprocess  import  Popen ,  PIPE     Imports are always put at the top of the file, just after any module
comments and docstrings, and before module globals and constants.
Imports should be grouped in the following order:   Standard library imports.
Related third party imports.
Local application/library specific imports.
You should put a blank line between each group of imports.
Absolute imports are recommended, as they are usually more readable
and tend to be better behaved (or at least give better error
messages) if the import system is incorrectly configured (such as
when a directory inside a package ends up on sys.path ): import  mypkg.sibling  from  mypkg  import  sibling  from  mypkg.sibling  import  example    However, explicit relative imports are an acceptable alternative to
absolute imports, especially when dealing with complex package layouts
where using absolute imports would be unnecessarily verbose:  from  .
import  sibling  from  .sibling  import  example    Standard library code should avoid complex package layouts and always
use absolute imports.
When importing a class from a class-containing module, it’s usually
okay to spell this: from  myclass  import  MyClass  from  foo.bar.yourclass  import  YourClass    If this spelling causes local name clashes, then spell them explicitly:  import  myclass  import  foo.bar.yourclass    and use myclass.MyClass and foo.bar.yourclass.YourClass .
Wildcard imports ( from  <module>  import  * ) should be avoided, as
they make it unclear which names are present in the namespace,
confusing both readers and many automated tools.
There is one
defensible use case for a wildcard import, which is to republish an
internal interface as part of a public API (for example, overwriting
a pure Python implementation of an interface with the definitions
from an optional accelerator module and exactly which definitions
will be overwritten isn’t known in advance).
When republishing names this way, the guidelines below regarding
public and internal interfaces still apply.
Module Level Dunder Names  Module level “dunders” (i.e.
names with two leading and two trailing
underscores) such as __all__ , __author__ , __version__ ,
etc.
should be placed after the module docstring but before any import
statements except  from  __future__ imports.
Python mandates that
future-imports must appear in the module before any other code except
docstrings:  """This is the example module.
This module does stuff.
 """
from  __future__  import  barry_as_FLUFL  __all__  =  [ 'a' ,  'b' ,  'c' ]  __version__  =  '0.1'  __author__  =  'Cardinal Biggles'  import  os  import  sys       String Quotes  In Python, single-quoted strings and double-quoted strings are the
same.
This PEP does not make a recommendation for this.
Pick a rule
and stick to it.
When a string contains single or double quote
characters, however, use the other one to avoid backslashes in the
string.
It improves readability.
For triple-quoted strings, always use double quote characters to be
consistent with the docstring convention in PEP 257 .
Whitespace in Expressions and Statements   Pet Peeves  Avoid extraneous whitespace in the following situations:   Immediately inside parentheses, brackets or braces: # Correct:  spam ( ham [ 1 ],  { eggs :  2 })    # Wrong:  spam (  ham [  1  ],  {  eggs :  2  }  )     Between a trailing comma and a following close parenthesis: # Correct:  foo  =  ( 0 ,)    # Wrong:  bar  =  ( 0 ,  )     Immediately before a comma, semicolon, or colon: # Correct:  if  x  ==  4 :  print ( x ,  y );  x ,  y  =  y ,  x    # Wrong:  if  x  ==  4  :  print ( x  ,  y )  ;  x  ,  y  =  y  ,  x     However, in a slice the colon acts like a binary operator, and
should have equal amounts on either side (treating it as the
operator with the lowest priority).
In an extended slice, both
colons must have the same amount of spacing applied.
Exception:
when a slice parameter is omitted, the space is omitted: # Correct:  ham [ 1 : 9 ],  ham [ 1 : 9 : 3 ],  ham [: 9 : 3 ],  ham [ 1 :: 3 ],  ham [ 1 : 9 :]  ham [ lower : upper ],  ham [ lower : upper :],  ham [ lower :: step ]  ham [ lower + offset  :  upper + offset ]  ham [:  upper_fn ( x )  :  step_fn ( x )],  ham [::  step_fn ( x )]  ham [ lower  +  offset  :  upper  +  offset ]    # Wrong:  ham [ lower  +  offset : upper  +  offset ]  ham [ 1 :  9 ],  ham [ 1  : 9 ],  ham [ 1 : 9  : 3 ]  ham [ lower  :  :  step ]  ham [  :  upper ]     Immediately before the open parenthesis that starts the argument
list of a function call: # Correct:  spam ( 1 )    # Wrong:  spam  ( 1 )     Immediately before the open parenthesis that starts an indexing or
slicing: # Correct:  dct [ 'key' ]  =  lst [ index ]    # Wrong:  dct  [ 'key' ]  =  lst  [ index ]     More than one space around an assignment (or other) operator to
align it with another: # Correct:  x  =  1  y  =  2  long_variable  =  3    # Wrong:  x  =  1  y  =  2  long_variable  =  3        Other Recommendations   Avoid trailing whitespace anywhere.
Because it’s usually invisible,
it can be confusing: e.g.
a backslash followed by a space and a
newline does not count as a line continuation marker.
Some editors
don’t preserve it and many projects (like CPython itself) have
pre-commit hooks that reject it.
Always surround these binary operators with a single space on either
side: assignment ( = ), augmented assignment ( += , -= etc.
), comparisons ( == , < , > , != , <> , <= , >= , in , not  in , is , is  not ), Booleans ( and , or , not ).
If operators with different priorities are used, consider adding
whitespace around the operators with the lowest priority(ies).
Use
your own judgment; however, never use more than one space, and
always have the same amount of whitespace on both sides of a binary
operator: # Correct:  i  =  i  +  1  submitted  +=  1  x  =  x * 2  -  1  hypot2  =  x * x  +  y * y  c  =  ( a + b )  *  ( a - b )    # Wrong:  i = i + 1  submitted  += 1  x  =  x  *  2  -  1  hypot2  =  x  *  x  +  y  *  y  c  =  ( a  +  b )  *  ( a  -  b )     Function annotations should use the normal rules for colons and
always have spaces around the -> arrow if present.
(See Function Annotations below for more about function annotations.
): # Correct:  def  munge ( input :  AnyStr ):  ...
 def  munge ()  ->  PosInt :  ...
   # Wrong:  def  munge ( input : AnyStr ):  ...
 def  munge () -> PosInt :  ...
    Don’t use spaces around the = sign when used to indicate a
keyword argument, or when used to indicate a default value for an unannotated function parameter: # Correct:  def  complex ( real ,  imag = 0.0 ):  return  magic ( r = real ,  i = imag )    # Wrong:  def  complex ( real ,  imag  =  0.0 ):  return  magic ( r  =  real ,  i  =  imag )    When combining an argument annotation with a default value, however, do use
spaces around the = sign:  # Correct:  def  munge ( sep :  AnyStr  =  None ):  ...
 def  munge ( input :  AnyStr ,  sep :  AnyStr  =  None ,  limit = 1000 ):  ...
   # Wrong:  def  munge ( input :  AnyStr = None ):  ...
 def  munge ( input :  AnyStr ,  limit  =  1000 ):  ...
Compound statements (multiple statements on the same line) are
generally discouraged: # Correct:  if  foo  ==  'blah' :  do_blah_thing ()  do_one ()  do_two ()  do_three ()    Rather not:  # Wrong:  if  foo  ==  'blah' :  do_blah_thing ()  do_one ();  do_two ();  do_three ()     While sometimes it’s okay to put an if/for/while with a small body
on the same line, never do this for multi-clause statements.
Also
avoid folding such long lines!
Rather not:  # Wrong:  if  foo  ==  'blah' :  do_blah_thing ()  for  x  in  lst :  total  +=  x  while  t  <  10 :  t  =  delay ()    Definitely not:  # Wrong:  if  foo  ==  'blah' :  do_blah_thing ()  else :  do_non_blah_thing ()  try :  something ()  finally :  cleanup ()  do_one ();  do_two ();  do_three ( long ,  argument ,  list ,  like ,  this )  if  foo  ==  'blah' :  one ();  two ();  three ()         When to Use Trailing Commas  Trailing commas are usually optional, except they are mandatory when
making a tuple of one element.
For clarity, it is recommended to
surround the latter in (technically redundant) parentheses:  # Correct:  FILES  =  ( 'setup.cfg' ,)    # Wrong:  FILES  =  'setup.cfg' ,    When trailing commas are redundant, they are often helpful when a
version control system is used, when a list of values, arguments or
imported items is expected to be extended over time.
The pattern is
to put each value (etc.)
on a line by itself, always adding a trailing
comma, and add the close parenthesis/bracket/brace on the next line.
However it does not make sense to have a trailing comma on the same
line as the closing delimiter (except in the above case of singleton
tuples):  # Correct:  FILES  =  [  'setup.cfg' ,  'tox.ini' ,  ]  initialize ( FILES ,  error = True ,  )    # Wrong:  FILES  =  [ 'setup.cfg' ,  'tox.ini' ,]  initialize ( FILES ,  error = True ,)      Comments  Comments that contradict the code are worse than no comments.
Always
make a priority of keeping the comments up-to-date when the code
changes!
Comments should be complete sentences.
The first word should be
capitalized, unless it is an identifier that begins with a lower case
letter (never alter the case of identifiers!).
Block comments generally consist of one or more paragraphs built out of
complete sentences, with each sentence ending in a period.
You should use one or two spaces after a sentence-ending period in
multi-sentence comments, except after the final sentence.
Ensure that your comments are clear and easily understandable to other
speakers of the language you are writing in.
Python coders from non-English speaking countries: please write your
comments in English, unless you are 120% sure that the code will never
be read by people who don’t speak your language.
Block Comments  Block comments generally apply to some (or all) code that follows
them, and are indented to the same level as that code.
Each line of a
block comment starts with a # and a single space (unless it is
indented text inside the comment).
Paragraphs inside a block comment are separated by a line containing a
single # .
Inline Comments  Use inline comments sparingly.
An inline comment is a comment on the same line as a statement.
Inline comments should be separated by at least two spaces from the
statement.
They should start with a # and a single space.
Inline comments are unnecessary and in fact distracting if they state
the obvious.
Don’t do this:  x  =  x  +  1  # Increment x    But sometimes, this is useful:  x  =  x  +  1  # Compensate for border      Documentation Strings  Conventions for writing good documentation strings
(a.k.a.
“docstrings”) are immortalized in PEP 257 .
Write docstrings for all public modules, functions, classes, and
methods.
Docstrings are not necessary for non-public methods, but
you should have a comment that describes what the method does.
This
comment should appear after the def line.
PEP 257 describes good docstring conventions.
Note that most
importantly, the """ that ends a multiline docstring should be
on a line by itself: """Return a foobang  Optional plotz says to frobnicate the bizbaz first.
 """
For one liner docstrings, please keep the closing """ on
the same line: """Return an ex-parrot."""
Naming Conventions  The naming conventions of Python’s library are a bit of a mess, so
we’ll never get this completely consistent – nevertheless, here are
the currently recommended naming standards.
New modules and packages
(including third party frameworks) should be written to these
standards, but where an existing library has a different style,
internal consistency is preferred.
Overriding Principle  Names that are visible to the user as public parts of the API should
follow conventions that reflect usage rather than implementation.
Descriptive: Naming Styles  There are a lot of different naming styles.
It helps to be able to
recognize what naming style is being used, independently from what
they are used for.
The following naming styles are commonly distinguished:   b (single lowercase letter)  B (single uppercase letter)  lowercase  lower_case_with_underscores  UPPERCASE  UPPER_CASE_WITH_UNDERSCORES  CapitalizedWords (or CapWords, or CamelCase – so named because
of the bumpy look of its letters [4] ).
This is also sometimes known
as StudlyCaps.
Note: When using acronyms in CapWords, capitalize all the
letters of the acronym.
Thus HTTPServerError is better than
HttpServerError.
mixedCase (differs from CapitalizedWords by initial lowercase
character!)
Capitalized_Words_With_Underscores (ugly!)
There’s also the style of using a short unique prefix to group related
names together.
This is not used much in Python, but it is mentioned
for completeness.
For example, the os.stat() function returns a
tuple whose items traditionally have names like st_mode , st_size , st_mtime and so on.
(This is done to emphasize the
correspondence with the fields of the POSIX system call struct, which
helps programmers familiar with that.)
The X11 library uses a leading X for all its public functions.
In
Python, this style is generally deemed unnecessary because attribute
and method names are prefixed with an object, and function names are
prefixed with a module name.
In addition, the following special forms using leading or trailing
underscores are recognized (these can generally be combined with any
case convention):   _single_leading_underscore : weak “internal use” indicator.
E.g.
from  M  import  * does not import objects whose names start
with an underscore.
single_trailing_underscore_ : used by convention to avoid
conflicts with Python keyword, e.g.
: tkinter .
Toplevel ( master ,  class_ = 'ClassName' )     __double_leading_underscore : when naming a class attribute,
invokes name mangling (inside class FooBar, __boo becomes _FooBar__boo ; see below).
__double_leading_and_trailing_underscore__ : “magic” objects or
attributes that live in user-controlled namespaces.
__init__ , __import__ or __file__ .
Never invent
such names; only use them as documented.
Prescriptive: Naming Conventions   Names to Avoid  Never use the characters ‘l’ (lowercase letter el), ‘O’ (uppercase
letter oh), or ‘I’ (uppercase letter eye) as single character variable
names.
In some fonts, these characters are indistinguishable from the
numerals one and zero.
When tempted to use ‘l’, use ‘L’ instead.
ASCII Compatibility  Identifiers used in the standard library must be ASCII compatible
as described in the policy section of PEP 3131 .
Package and Module Names  Modules should have short, all-lowercase names.
Underscores can be
used in the module name if it improves readability.
Python packages
should also have short, all-lowercase names, although the use of
underscores is discouraged.
When an extension module written in C or C++ has an accompanying
Python module that provides a higher level (e.g.
more object oriented)
interface, the C/C++ module has a leading underscore
(e.g.
_socket ).
Class Names  Class names should normally use the CapWords convention.
The naming convention for functions may be used instead in cases where
the interface is documented and used primarily as a callable.
Note that there is a separate convention for builtin names: most builtin
names are single words (or two words run together), with the CapWords
convention used only for exception names and builtin constants.
Type Variable Names  Names of type variables introduced in PEP 484 should normally use CapWords
preferring short names: T , AnyStr , Num .
It is recommended to add
suffixes _co or _contra to the variables used to declare covariant
or contravariant behavior correspondingly:  from  typing  import  TypeVar  VT_co  =  TypeVar ( 'VT_co' ,  covariant = True )  KT_contra  =  TypeVar ( 'KT_contra' ,  contravariant = True )      Exception Names  Because exceptions should be classes, the class naming convention
applies here.
However, you should use the suffix “Error” on your
exception names (if the exception actually is an error).
Global Variable Names  (Let’s hope that these variables are meant for use inside one module
only.)
The conventions are about the same as those for functions.
Modules that are designed for use via from  M  import  * should use
the __all__ mechanism to prevent exporting globals, or use the
older convention of prefixing such globals with an underscore (which
you might want to do to indicate these globals are “module
non-public”).
Function and Variable Names  Function names should be lowercase, with words separated by
underscores as necessary to improve readability.
Variable names follow the same convention as function names.
mixedCase is allowed only in contexts where that’s already the
prevailing style (e.g.
threading.py), to retain backwards
compatibility.
Function and Method Arguments  Always use self for the first argument to instance methods.
Always use cls for the first argument to class methods.
If a function argument’s name clashes with a reserved keyword, it is
generally better to append a single trailing underscore rather than
use an abbreviation or spelling corruption.
Thus class_ is better
than clss .
(Perhaps better is to avoid such clashes by using a
synonym.)
Method Names and Instance Variables  Use the function naming rules: lowercase with words separated by
underscores as necessary to improve readability.
Use one leading underscore only for non-public methods and instance
variables.
To avoid name clashes with subclasses, use two leading underscores to
invoke Python’s name mangling rules.
Python mangles these names with the class name: if class Foo has an
attribute named __a , it cannot be accessed by Foo.__a .
(An
insistent user could still gain access by calling Foo._Foo__a .)
Generally, double leading underscores should be used only to avoid
name conflicts with attributes in classes designed to be subclassed.
Note: there is some controversy about the use of __names (see below).
Constants  Constants are usually defined on a module level and written in all
capital letters with underscores separating words.
Examples include MAX_OVERFLOW and TOTAL .
Designing for Inheritance  Always decide whether a class’s methods and instance variables
(collectively: “attributes”) should be public or non-public.
If in
doubt, choose non-public; it’s easier to make it public later than to
make a public attribute non-public.
Public attributes are those that you expect unrelated clients of your
class to use, with your commitment to avoid backwards incompatible
changes.
Non-public attributes are those that are not intended to be
used by third parties; you make no guarantees that non-public
attributes won’t change or even be removed.
We don’t use the term “private” here, since no attribute is really
private in Python (without a generally unnecessary amount of work).
Another category of attributes are those that are part of the
“subclass API” (often called “protected” in other languages).
Some
classes are designed to be inherited from, either to extend or modify
aspects of the class’s behavior.
When designing such a class, take
care to make explicit decisions about which attributes are public,
which are part of the subclass API, and which are truly only to be
used by your base class.
With this in mind, here are the Pythonic guidelines:   Public attributes should have no leading underscores.
If your public attribute name collides with a reserved keyword,
append a single trailing underscore to your attribute name.
This is
preferable to an abbreviation or corrupted spelling.
(However,
notwithstanding this rule, ‘cls’ is the preferred spelling for any
variable or argument which is known to be a class, especially the
first argument to a class method.)
Note 1: See the argument name recommendation above for class methods.
For simple public data attributes, it is best to expose just the
attribute name, without complicated accessor/mutator methods.
Keep
in mind that Python provides an easy path to future enhancement,
should you find that a simple data attribute needs to grow
functional behavior.
In that case, use properties to hide
functional implementation behind simple data attribute access
syntax.
Note 1: Try to keep the functional behavior side-effect free,
although side-effects such as caching are generally fine.
Note 2: Avoid using properties for computationally expensive
operations; the attribute notation makes the caller believe that
access is (relatively) cheap.
If your class is intended to be subclassed, and you have attributes
that you do not want subclasses to use, consider naming them with
double leading underscores and no trailing underscores.
This
invokes Python’s name mangling algorithm, where the name of the
class is mangled into the attribute name.
This helps avoid
attribute name collisions should subclasses inadvertently contain
attributes with the same name.
Note 1: Note that only the simple class name is used in the mangled
name, so if a subclass chooses both the same class name and attribute
name, you can still get name collisions.
Note 2: Name mangling can make certain uses, such as debugging and __getattr__() , less convenient.
However the name mangling
algorithm is well documented and easy to perform manually.
Note 3: Not everyone likes name mangling.
Try to balance the
need to avoid accidental name clashes with potential use by
advanced callers.
Public and Internal Interfaces  Any backwards compatibility guarantees apply only to public interfaces.
Accordingly, it is important that users be able to clearly distinguish
between public and internal interfaces.
Documented interfaces are considered public, unless the documentation
explicitly declares them to be provisional or internal interfaces exempt
from the usual backwards compatibility guarantees.
All undocumented
interfaces should be assumed to be internal.
To better support introspection, modules should explicitly declare the
names in their public API using the __all__ attribute.
Setting __all__ to an empty list indicates that the module has no public API.
Even with __all__ set appropriately, internal interfaces (packages,
modules, classes, functions, attributes or other names) should still be
prefixed with a single leading underscore.
An interface is also considered internal if any containing namespace
(package, module or class) is considered internal.
Imported names should always be considered an implementation detail.
Other modules must not rely on indirect access to such imported names
unless they are an explicitly documented part of the containing module’s
API, such as os.path or a package’s __init__ module that exposes
functionality from submodules.
Programming Recommendations   Code should be written in a way that does not disadvantage other
implementations of Python (PyPy, Jython, IronPython, Cython, Psyco,
and such).
For example, do not rely on CPython’s efficient implementation of
in-place string concatenation for statements in the form a  +=  b or a  =  a  +  b .
This optimization is fragile even in CPython (it
only works for some types) and isn’t present at all in implementations
that don’t use refcounting.
In performance sensitive parts of the
library, the ''.join() form should be used instead.
This will
ensure that concatenation occurs in linear time across various
implementations.
Comparisons to singletons like None should always be done with is or is  not , never the equality operators.
Also, beware of writing if  x when you really mean if  x  is  not  None – e.g.
when testing whether a variable or argument that
defaults to None was set to some other value.
The other value might
have a type (such as a container) that could be false in a boolean
context!
Use is  not operator rather than not  ...
 is .
While both
expressions are functionally identical, the former is more readable
and preferred: # Correct:  if  foo  is  not  None :    # Wrong:  if  not  foo  is  None :     When implementing ordering operations with rich comparisons, it is
best to implement all six operations ( __eq__ , __ne__ , __lt__ , __le__ , __gt__ , __ge__ ) rather than relying
on other code to only exercise a particular comparison.
To minimize the effort involved, the functools.total_ordering() decorator provides a tool to generate missing comparison methods.
PEP 207 indicates that reflexivity rules are assumed by Python.
Thus, the interpreter may swap y  >  x with x  <  y , y  >=  x with x  <=  y , and may swap the arguments of x  ==  y and x  !=  y .
The sort() and min() operations are guaranteed to use
the < operator and the max() function uses the > operator.
However, it is best to implement all six operations so
that confusion doesn’t arise in other contexts.
Always use a def statement instead of an assignment statement that binds
a lambda expression directly to an identifier: # Correct:  def  f ( x ):  return  2 * x    # Wrong:  f  =  lambda  x :  2 * x    The first form means that the name of the resulting function object is
specifically ‘f’ instead of the generic ‘<lambda>’.
This is more
useful for tracebacks and string representations in general.
The use
of the assignment statement eliminates the sole benefit a lambda
expression can offer over an explicit def statement (i.e.
that it can
be embedded inside a larger expression)   Derive exceptions from Exception rather than BaseException .
Direct inheritance from BaseException is reserved for exceptions
where catching them is almost always the wrong thing to do.
Design exception hierarchies based on the distinctions that code catching the exceptions is likely to need, rather than the locations
where the exceptions are raised.
Aim to answer the question
“What went wrong?” programmatically, rather than only stating that
“A problem occurred” (see PEP 3151 for an example of this lesson being
learned for the builtin exception hierarchy)  Class naming conventions apply here, although you should add the
suffix “Error” to your exception classes if the exception is an
error.
Non-error exceptions that are used for non-local flow control
or other forms of signaling need no special suffix.
Use exception chaining appropriately.
raise  X  from  Y should be used to indicate explicit replacement without losing the
original traceback.
When deliberately replacing an inner exception (using raise  X  from  None ), ensure that relevant details are transferred to the new
exception (such as preserving the attribute name when converting
KeyError to AttributeError, or embedding the text of the original
exception in the new exception message).
When catching exceptions, mention specific exceptions whenever
possible instead of using a bare except: clause: try :  import  platform_specific_module  except  ImportError :  platform_specific_module  =  None    A bare except: clause will catch SystemExit and
KeyboardInterrupt exceptions, making it harder to interrupt a
program with Control-C, and can disguise other problems.
If you
want to catch all exceptions that signal program errors, use except  Exception: (bare except is equivalent to except  BaseException: ).
A good rule of thumb is to limit use of bare ‘except’ clauses to two
cases:   If the exception handler will be printing out or logging the
traceback; at least the user will be aware that an error has
occurred.
If the code needs to do some cleanup work, but then lets the
exception propagate upwards with raise .
try...finally can be a better way to handle this case.
When catching operating system errors, prefer the explicit exception
hierarchy introduced in Python 3.3 over introspection of errno values.
Additionally, for all try/except clauses, limit the try clause
to the absolute minimum amount of code necessary.
Again, this
avoids masking bugs: # Correct:  try :  value  =  collection [ key ]  except  KeyError :  return  key_not_found ( key )  else :  return  handle_value ( value )    # Wrong:  try :  # Too broad!
return  handle_value ( collection [ key ])  except  KeyError :  # Will also catch KeyError raised by handle_value()  return  key_not_found ( key )     When a resource is local to a particular section of code, use a with statement to ensure it is cleaned up promptly and reliably
after use.
A try/finally statement is also acceptable.
Context managers should be invoked through separate functions or methods
whenever they do something other than acquire and release resources: # Correct:  with  conn .
begin_transaction ():  do_stuff_in_transaction ( conn )    # Wrong:  with  conn :  do_stuff_in_transaction ( conn )    The latter example doesn’t provide any information to indicate that
the __enter__ and __exit__ methods are doing something other
than closing the connection after a transaction.
Being explicit is
important in this case.
Be consistent in return statements.
Either all return statements in
a function should return an expression, or none of them should.
If
any return statement returns an expression, any return statements
where no value is returned should explicitly state this as return  None , and an explicit return statement should be present at the
end of the function (if reachable): # Correct:  def  foo ( x ):  if  x  >=  0 :  return  math .
sqrt ( x )  else :  return  None  def  bar ( x ):  if  x  <  0 :  return  None  return  math .
sqrt ( x )    # Wrong:  def  foo ( x ):  if  x  >=  0 :  return  math .
sqrt ( x )  def  bar ( x ):  if  x  <  0 :  return  return  math .
sqrt ( x )     Use ''.startswith() and ''.endswith() instead of string
slicing to check for prefixes or suffixes.
startswith() and endswith() are cleaner and less error prone:  # Correct:  if  foo .
startswith ( 'bar' ):    # Wrong:  if  foo [: 3 ]  ==  'bar' :     Object type comparisons should always use isinstance() instead of
comparing types directly: # Correct:  if  isinstance ( obj ,  int ):    # Wrong:  if  type ( obj )  is  type ( 1 ):     For sequences, (strings, lists, tuples), use the fact that empty
sequences are false: # Correct:  if  not  seq :  if  seq :    # Wrong:  if  len ( seq ):  if  not  len ( seq ):     Don’t write string literals that rely on significant trailing
whitespace.
Such trailing whitespace is visually indistinguishable
and some editors (or more recently, reindent.py) will trim them.
Don’t compare boolean values to True or False using == : # Correct:  if  greeting :    # Wrong:  if  greeting  ==  True :    Worse:  # Wrong:  if  greeting  is  True :     Use of the flow control statements return / break / continue within the finally suite of a try...finally , where the flow control
statement would jump outside the finally suite, is discouraged.
This
is because such statements will implicitly cancel any active exception
that is propagating through the finally suite: # Wrong:  def  foo ():  try :  1  /  0  finally :  return  42       Function Annotations  With the acceptance of PEP 484 , the style rules for function
annotations have changed.
Function annotations should use PEP 484 syntax (there are some
formatting recommendations for annotations in the previous section).
The experimentation with annotation styles that was recommended
previously in this PEP is no longer encouraged.
However, outside the stdlib, experiments within the rules of PEP 484 are now encouraged.
For example, marking up a large third party
library or application with PEP 484 style type annotations,
reviewing how easy it was to add those annotations, and observing
whether their presence increases code understandability.
The Python standard library should be conservative in adopting such
annotations, but their use is allowed for new code and for big
refactorings.
For code that wants to make a different use of function annotations
it is recommended to put a comment of the form: # type: ignore    near the top of the file; this tells type checkers to ignore all
annotations.
(More fine-grained ways of disabling complaints from
type checkers can be found in PEP 484 .)
Like linters, type checkers are optional, separate tools.
Python
interpreters by default should not issue any messages due to type
checking and should not alter their behavior based on annotations.
Users who don’t want to use type checkers are free to ignore them.
However, it is expected that users of third party library packages
may want to run type checkers over those packages.
For this purpose PEP 484 recommends the use of stub files: .pyi files that are read
by the type checker in preference of the corresponding .py files.
Stub files can be distributed with a library, or separately (with
the library author’s permission) through the typeshed repo [5] .
Variable Annotations  PEP 526 introduced variable annotations.
The style recommendations for them are
similar to those on function annotations described above:   Annotations for module level variables, class and instance variables,
and local variables should have a single space after the colon.
There should be no space before the colon.
If an assignment has a right hand side, then the equality sign should have
exactly one space on both sides: # Correct:  code :  int  class  Point :  coords :  Tuple [ int ,  int ]  label :  str  =  '<unknown>'    # Wrong:  code : int  # No space after colon  code  :  int  # Space before colon  class  Test :  result :  int = 0  # No spaces around equality sign     Although the PEP 526 is accepted for Python 3.6, the variable annotation
syntax is the preferred syntax for stub files on all versions of Python
(see PEP 484 for details).
Footnotes    [ 1 ]  Hanging indentation is a type-setting style where all
the lines in a paragraph are indented except the first line.
In
the context of Python, the term is used to describe a style where
the opening parenthesis of a parenthesized statement is the last
non-whitespace character of the line, with subsequent lines being
indented until the closing parenthesis.
References    [ 2 ]  Barry’s GNU Mailman style guide http://barry.warsaw.us/software/STYLEGUIDE.txt   [ 3 ]  Donald Knuth’s The TeXBook , pages 195 and 196.
[ 4 ]  http://www.wikipedia.com/wiki/CamelCase   [ 5 ]  Typeshed repo https://github.com/python/typeshed     Copyright  This document has been placed in the public domain.
Source: https://github.com/python/peps/blob/main/peps/pep-0008.rst  Last modified: 2023-12-09 16:19:37 GMT    Contents   Introduction  A Foolish Consistency is the Hobgoblin of Little Minds  Code Lay-out  Indentation  Tabs or Spaces?
Blank Lines  Source File Encoding  Imports  Module Level Dunder Names    String Quotes  Whitespace in Expressions and Statements  Pet Peeves  Other Recommendations    When to Use Trailing Commas  Comments  Block Comments  Inline Comments  Documentation Strings    Naming Conventions  Overriding Principle  Descriptive: Naming Styles  Prescriptive: Naming Conventions  Names to Avoid  ASCII Compatibility  Package and Module Names  Class Names  Type Variable Names  Exception Names  Global Variable Names  Function and Variable Names  Function and Method Arguments  Method Names and Instance Variables  Constants  Designing for Inheritance    Public and Internal Interfaces    Programming Recommendations  Function Annotations  Variable Annotations    References  Copyright    Page Source (GitHub)        Navigation    index   modules |  Beautiful Soup 4.12.0 documentation »  Beautiful Soup Documentation         Beautiful Soup Documentation ¶   Beautiful Soup is a
Python library for pulling data out of HTML and XML files.
The Dormouse's story  Once upon a time there were three little sisters; and their names were Elsie , Lacie and Tillie ;
 and they lived at the bottom of a well.
...
"""

Running the "three sisters" document through Beautiful Soup gives us a
:py:class:`BeautifulSoup` object, which represents the document as a nested
data structure::

 from bs4 import BeautifulSoup
 soup = BeautifulSoup(html_doc, 'html.parser')

 print(soup.prettify())
 # # # # # #     The Dormouse's story
 # # # #    Once upon a time there were three little sisters; and their names were
 # #     Elsie
 # #    ,
 # #     Lacie
 # #    and
 # #     Tillie
 # #    ; and they lived at the bottom of a well.
# # #    ...
 # # # Here are some simple ways to navigate that data structure::

 soup.title
 # soup.title.name
 # u'title'

 soup.title.string
 # u'The Dormouse's story'

 soup.title.parent.name
 # u'head'

 soup.p
 # The Dormouse's story soup.p['class']
 # u'title'

 soup.a
 # Elsie soup.find_all('a')
 # [ Elsie ,
 # Lacie ,
 # Tillie ]

 soup.find(id="link3")
 # Tillie One common task is extracting all the URLs found within a page's tags::

 for link in soup.find_all('a'):
     print(link.get('href'))
 # http://example.com/elsie
 # http://example.com/lacie
 # http://example.com/tillie

Another common task is extracting all the text from a page::

 print(soup.get_text())
 # The Dormouse's story
 #
 # The Dormouse's story
 #
 # Once upon a time there were three little sisters; and their names were
 # Elsie,
 # Lacie and
 # Tillie;
 # and they lived at the bottom of a well.
#
 # ...
Installing Beautiful Soup
=========================

If you're using a recent version of Debian or Ubuntu Linux, you can
install Beautiful Soup with the system package manager:

:kbd:`$ apt-get install python3-bs4`

Beautiful Soup 4 is published through PyPi, so if you can't install it
with the system packager, you can install it with ``easy_install`` or
``pip``.
The package name is ``beautifulsoup4``.
Make sure you use the
right version of ``pip`` or ``easy_install`` for your Python version
(these may be named ``pip3`` and ``easy_install3`` respectively).
:kbd:`$ easy_install beautifulsoup4`

:kbd:`$ pip install beautifulsoup4`

(The :py:class:`BeautifulSoup` package is `not` what you want.
That's
the previous major release, `Beautiful Soup 3`_.
Lots of software uses
BS3, so it's still available, but if you're writing new code you
should install ``beautifulsoup4``.)
If you don't have ``easy_install`` or ``pip`` installed, you can
`download the Beautiful Soup 4 source tarball `_ and
install it with ``setup.py``.
:kbd:`$ python setup.py install`

If all else fails, the license for Beautiful Soup allows you to
package the entire library with your application.
You can download the
tarball, copy its ``bs4`` directory into your application's codebase,
and use Beautiful Soup without installing it at all.
..
_parser-installation:


Installing a parser
-------------------

Beautiful Soup supports the HTML parser included in Python's standard
library, but it also supports a number of third-party Python parsers.
One is the `lxml parser `_.
Depending on your setup,
you might install lxml with one of these commands:

:kbd:`$ apt-get install python-lxml`

:kbd:`$ easy_install lxml`

:kbd:`$ pip install lxml`

Another alternative is the pure-Python `html5lib parser `_, which parses HTML the way a
web browser does.
Depending on your setup, you might install html5lib
with one of these commands:

:kbd:`$ apt-get install python-html5lib`

:kbd:`$ easy_install html5lib`

:kbd:`$ pip install html5lib`

This table summarizes the advantages and disadvantages of each parser library:

+----------------------+--------------------------------------------+--------------------------------+--------------------------+
| Parser               | Typical usage                              | Advantages                     | Disadvantages            |
+----------------------+--------------------------------------------+--------------------------------+--------------------------+
| Python's html.parser | ``BeautifulSoup(markup, "html.parser")``   | * Batteries included           | * Not as fast as lxml,   |
|                      |                                            | * Decent speed                 |   less lenient than      |
|                      |                                            | * Lenient (As of Python 3.2)   |   html5lib.
|
+----------------------+--------------------------------------------+--------------------------------+--------------------------+
| lxml's HTML parser   | ``BeautifulSoup(markup, "lxml")``          | * Very fast                    | * External C dependency  |
|                      |                                            | * Lenient                      |                          |
+----------------------+--------------------------------------------+--------------------------------+--------------------------+
| lxml's XML parser    | ``BeautifulSoup(markup, "lxml-xml")``      | * Very fast                    | * External C dependency  |
|                      | ``BeautifulSoup(markup, "xml")``           | * The only currently supported |                          |
|                      |                                            |   XML parser                   |                          |
+----------------------+--------------------------------------------+--------------------------------+--------------------------+
| html5lib             | ``BeautifulSoup(markup, "html5lib")``      | * Extremely lenient            | * Very slow              |
|                      |                                            | * Parses pages the same way a  | * External Python        |
|                      |                                            |   web browser does             |   dependency             |
|                      |                                            | * Creates valid HTML5          |                          |
+----------------------+--------------------------------------------+--------------------------------+--------------------------+

If you can, I recommend you install and use lxml for speed.
See `Differences
between parsers`_ for details.
Making the soup
===============

To parse a document, pass it into the :py:class:`BeautifulSoup`
constructor.
You can pass in a string or an open filehandle::

 from bs4 import BeautifulSoup

 with open("index.html") as fp:
     soup = BeautifulSoup(fp, 'html.parser')

 soup = BeautifulSoup(" a web page ", 'html.parser')

First, the document is converted to Unicode, and HTML entities are
converted to Unicode characters::

 print(BeautifulSoup(" Sacré bleu!
", "html.parser"))
 # Sacré bleu!
Beautiful Soup then parses the document using the best available
parser.
(See `Parsing XML`_.)
Kinds of objects
================

Beautiful Soup transforms a complex HTML document into a complex tree
of Python objects.
But you'll only ever have to deal with about four
`kinds` of objects: :py:class:`Tag`, :py:class:`NavigableString`, :py:class:`BeautifulSoup`,
and :py:class:`Comment`.
..
py:class:: Tag

 A :py:class:`Tag` object corresponds to an XML or HTML tag in the original document.
::

  soup = BeautifulSoup(' Extremely bold ', 'html.parser')
  tag = soup.b
  type(tag)
  # Tags have a lot of attributes and methods, and I'll cover most of them
 in `Navigating the tree`_ and `Searching the tree`_.
For now, the most
 important features of a tag are its name and attributes.
..
py:attribute:: name

  Every tag has a name::

   tag.name
   # 'b'

  If you change a tag's name, the change will be reflected in any
  markup generated by Beautiful Soup down the line::

   tag.name = "blockquote"
   tag
   # Extremely bold ..
py:attribute:: attrs

  An HTML or XML tag may have any number of attributes.
The tag `` `` has an attribute "id" whose value is
  "boldest".
You can access a tag's attributes by treating the tag like
  a dictionary::

   tag = BeautifulSoup(' bold ', 'html.parser').b
   tag['id']
   # 'boldest'

  You can access the dictionary of attributes directly as ``.attrs``::

   tag.attrs
   # {'id': 'boldest'}

  You can add, remove, and modify a tag's attributes.
Again, this is
  done by treating the tag as a dictionary::

   tag['id'] = 'verybold'
   tag['another-attribute'] = 1
   tag
   # del tag['id']
   del tag['another-attribute']
   tag
   # bold tag['id']
   # KeyError: 'id'
   tag.get('id')
   # None

  ..
_multivalue:

  Multi-valued attributes
  -----------------------

  HTML 4 defines a few attributes that can have multiple values.
HTML 5
  removes a couple of them, but defines a few more.
The most common
  multi-valued attribute is ``class`` (that is, a tag can have more than
  one CSS class).
Others include ``rel``, ``rev``, ``accept-charset``,
  ``headers``, and ``accesskey``.
They implement the rules described in the HTML specification::

   from bs4.builder import builder_registry
   builder_registry.lookup('html').DEFAULT_CDATA_LIST_ATTRIBUTES
  
..
py:class:: NavigableString

-----------------------------

A string corresponds to a bit of text within a tag.
Beautiful Soup
uses the :py:class:`NavigableString` class to contain these bits of text::

 soup = BeautifulSoup(' Extremely bold ', 'html.parser')
 tag = soup.b
 tag.string
 # 'Extremely bold'
 type(tag.string)
 # A :py:class:`NavigableString` is just like a Python Unicode string, except
that it also supports some of the features described in `Navigating
the tree`_ and `Searching the tree`_.
You can convert a
:py:class:`NavigableString` to a Unicode string with ``str``::

 unicode_string = str(tag.string)
 unicode_string
 # 'Extremely bold'
 type(unicode_string)
 # You can't edit a string in place, but you can replace one string with
another, using :ref:`replace_with()`::

 tag.string.replace_with("No longer bold")
 tag
 # No longer bold :py:class:`NavigableString` supports most of the features described in
`Navigating the tree`_ and `Searching the tree`_, but not all of
them.
In particular, since a string can't contain anything (the way a
tag may contain a string or another tag), strings don't support the
``.contents`` or ``.string`` attributes, or the ``find()`` method.
If you want to use a :py:class:`NavigableString` outside of Beautiful Soup,
you should call ``unicode()`` on it to turn it into a normal Python
Unicode string.
If you don't, your string will carry around a
reference to the entire Beautiful Soup parse tree, even when you're
done using Beautiful Soup.
..
py:class:: BeautifulSoup

---------------------------

The :py:class:`BeautifulSoup` object represents the parsed document as a
whole.
For most purposes, you can treat it as a :py:class:`Tag`
object.
This means it supports most of the methods described in
`Navigating the tree`_ and `Searching the tree`_.
You can also pass a :py:class:`BeautifulSoup` object into one of the methods
defined in `Modifying the tree`_, just as you would a :py:class:`Tag`.
This
lets you do things like combine two parsed documents::

 doc = BeautifulSoup(" INSERT FOOTER HERE Here's the footer ", "xml")
 doc.find(text="INSERT FOOTER HERE").replace_with(footer)
 # 'INSERT FOOTER HERE'
 print(doc)
 # xml version="1.0" encoding="utf-8"?
# Here's the footer Since the :py:class:`BeautifulSoup` object doesn't correspond to an actual
HTML or XML tag, it has no name and no attributes.
But sometimes it's
useful to look at its ``.name``, so it's been given the special
``.name`` "[document]"::

 soup.name
 # '[document]'

Special strings
---------------

:py:class:`Tag`, :py:class:`NavigableString`, and
:py:class:`BeautifulSoup` cover almost everything you'll see in an
HTML or XML file, but there are a few leftover bits.
The main one
you'll probably encounter is the :py:class:`Comment`.
..
py:class:: Comment

::

 markup = " "
 soup = BeautifulSoup(markup, 'html.parser')
 comment = soup.b.string
 type(comment)
 # The :py:class:`Comment` object is just a special type of :py:class:`NavigableString`::

 comment
 # 'Hey, buddy.
Want to buy a used parser'

But when it appears as part of an HTML document, a :py:class:`Comment` is
displayed with special formatting::

 print(soup.b.prettify())
 # # # For HTML documents
^^^^^^^^^^^^^^^^^^
 
Beautiful Soup defines a few :py:class:`NavigableString` subclasses to
contain strings found inside specific HTML tags.
`(These classes are new in Beautiful Soup 4.9.0, and the
html5lib parser doesn't use them.
Here are some of Sphinx’s major features:   Output formats: HTML (including Windows HTML Help), LaTeX (for printable
PDF versions), ePub, Texinfo, manual pages, plain text  Extensive cross-references: semantic markup and automatic links for
functions, classes, citations, glossary terms and similar pieces of
information  Hierarchical structure: easy definition of a document tree, with automatic
links to siblings, parents and children  Automatic indices: general index as well as a language-specific module
indices  Code handling: automatic highlighting using the Pygments highlighter  Extensions: automatic testing of code snippets, inclusion of docstrings
from Python modules (API docs) via built-in extensions , and much more functionality via third-party
extensions .
Themes: modify the look and feel of outputs via creating themes , and reuse many third-party themes .
Contributed extensions: dozens of extensions contributed by users ; most of them installable from PyPI.
Sphinx uses the reStructuredText markup language by default, and can read MyST markdown via third-party extensions.
Both of these
are powerful and straightforward to use, and have functionality
for complex documentation and publishing workflows.
They both build upon Docutils to parse and write documents.
See below for how to navigate Sphinx’s documentation.
See also  The Sphinx documentation Table of Contents has
a full list of this site’s pages.
Get started ¶  These sections cover the basics of getting started with Sphinx, including
creating and building your own documentation from scratch.
Get started   Getting Started  Setting up the documentation sources  Defining document structure  Adding content  Running the build  Documenting objects  Basic configuration  Autodoc  Intersphinx  More topics to be covered    Installing Sphinx  Overview  Linux  macOS  Windows  Installation from PyPI  Docker  Installation from source    Tutorial: Build your first project  Getting started  First steps to document your project using Sphinx  More Sphinx customization  Narrative documentation in Sphinx  Describing code in Sphinx  Automatic documentation generation from code  Appendix: Deploying a Sphinx project online  Where to go from here        User Guides ¶  These sections cover various topics in using and extending Sphinx for various
use-cases.
They are a comprehensive guide to using Sphinx in many contexts and
assume more knowledge of Sphinx.
If you are new to Sphinx, we recommend
starting with Get started .
User Guides   Using Sphinx  reStructuredText  Markdown  Cross-referencing syntax  Configuration  Builders  Domains  Extensions  HTML Theming  Internationalization  Sphinx Web Support    Writing Sphinx Extensions  Developing extensions overview  Extension tutorials  Configuring builders  Templating  HTML theme development    LaTeX customization  The latex_elements configuration setting  The sphinxsetup configuration setting  Additional  CSS-like 'sphinxsetup' keys  LaTeX macros and environments    Sphinx Extensions API  Important objects  Build Phases  Extension metadata  APIs used for writing extensions        Community guide ¶  Sphinx is community supported and welcomes contributions from anybody.
The sections below should help you get started joining the Sphinx community
as well as contributing.
See the Sphinx contributors’ guide if you would
like to contribute to the project.
If you would like
usecase-driven documentation, see Get started or User Guides .
Created using Sphinx 7.3.6.
